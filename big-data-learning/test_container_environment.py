#!/usr/bin/env python3
"""
å®¹å™¨å¼€å‘ç¯å¢ƒå¿«é€Ÿæµ‹è¯•è„šæœ¬
"""

import sys
import os
from datetime import datetime

def print_section(title):
    print(f"\nğŸ§ª {title}")
    print('-' * 40)

def test_spark_connection():
    """æµ‹è¯• Spark é›†ç¾¤è¿æ¥"""
    print_section("Spark é›†ç¾¤è¿æ¥æµ‹è¯•")
    
    try:
        from pyspark.sql import SparkSession
        
        spark = SparkSession.builder \
            .appName("ContainerTest") \
            .master("spark://spark-master:7077") \
            .getOrCreate()
        
        spark.sparkContext.setLogLevel("WARN")
        
        print(f"âœ… Spark ç‰ˆæœ¬: {spark.version}")
        print(f"âœ… é›†ç¾¤è¿æ¥æˆåŠŸ")
        
        # ç®€å•æµ‹è¯•
        data = [(1, "æµ‹è¯•"), (2, "æ•°æ®")]
        df = spark.createDataFrame(data, ["id", "name"])
        count = df.count()
        print(f"âœ… æ•°æ®å¤„ç†æµ‹è¯•é€šè¿‡ (è®°å½•æ•°: {count})")
        
        spark.stop()
        return True
        
    except Exception as e:
        print(f"âŒ Spark è¿æ¥å¤±è´¥: {str(e)}")
        return False

def test_spark_connection():
    """æµ‹è¯• Spark é›†ç¾¤è¿æ¥"""
    print_section("Spark é›†ç¾¤è¿æ¥æµ‹è¯•")
    
    try:
        from pyspark.sql import SparkSession
        
        # åˆ›å»º Spark ä¼šè¯
        print("ğŸ”— è¿æ¥åˆ° Spark é›†ç¾¤...")
        spark = SparkSession.builder \
            .appName("ContainerEnvironmentTest") \
            .master("spark://spark-master:7077") \
            .config("spark.executor.memory", "1g") \
            .config("spark.driver.memory", "512m") \
            .getOrCreate()
        
        # è®¾ç½®æ—¥å¿—çº§åˆ«
        spark.sparkContext.setLogLevel("WARN")
        
        print(f"âœ… Spark ç‰ˆæœ¬: {spark.version}")
        print(f"âœ… Master URL: {spark.sparkContext.master}")
        print(f"âœ… åº”ç”¨åç§°: {spark.sparkContext.appName}")
        print(f"âœ… åº”ç”¨ ID: {spark.sparkContext.applicationId}")
        
        # æµ‹è¯•åŸºæœ¬æ“ä½œ
        print("\nğŸ§® æµ‹è¯•åŸºæœ¬ Spark æ“ä½œ...")
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        data = [(1, "Alice", 25), (2, "Bob", 30), (3, "Charlie", 35)]
        columns = ["id", "name", "age"]
        df = spark.createDataFrame(data, columns)
        
        print("ğŸ“Š åˆ›å»ºæµ‹è¯• DataFrame:")
        df.show()
        
        # æµ‹è¯• SQL æŸ¥è¯¢
        df.createOrReplaceTempView("people")
        result = spark.sql("SELECT name, age FROM people WHERE age > 25")
        print("ğŸ” SQL æŸ¥è¯¢ç»“æœ (age > 25):")
        result.show()
        
        # æµ‹è¯•èšåˆæ“ä½œ
        avg_age = df.agg({"age": "avg"}).collect()[0][0]
        print(f"ğŸ“ˆ å¹³å‡å¹´é¾„: {avg_age:.1f}")
        
        # è·å–é›†ç¾¤ä¿¡æ¯
        print("\nğŸ–¥ï¸  é›†ç¾¤ä¿¡æ¯:")
        print(f"  é»˜è®¤å¹¶è¡Œåº¦: {spark.sparkContext.defaultParallelism}")
        print(f"  Executor æ•°é‡: {len(spark.sparkContext.statusTracker().getExecutorInfos()) - 1}")
        
        spark.stop()
        print("\nâœ… Spark é›†ç¾¤æµ‹è¯•é€šè¿‡")
        return True
        
    except Exception as e:
        print(f"âŒ Spark è¿æ¥å¤±è´¥: {str(e)}")
        print("\nğŸ”§ æ•…éšœæ’é™¤å»ºè®®:")
        print("1. æ£€æŸ¥ Spark é›†ç¾¤æ˜¯å¦å¯åŠ¨: docker-compose ps")
        print("2. æ£€æŸ¥ç½‘ç»œè¿æ¥: docker exec -it spark-dev ping spark-master")
        print("3. æŸ¥çœ‹ Master æ—¥å¿—: docker-compose logs spark-master")
        return False

def test_data_access():
    """æµ‹è¯•æ•°æ®ç›®å½•è®¿é—®"""
    print_section("æ•°æ®ç›®å½•è®¿é—®æµ‹è¯•")
    
    data_dir = "/opt/bitnami/spark/data"
    notebooks_dir = "/opt/bitnami/spark/notebooks"
    
    # æ£€æŸ¥ç›®å½•æ˜¯å¦å­˜åœ¨
    directories = [
        (data_dir, "æ•°æ®ç›®å½•"),
        (notebooks_dir, "Notebooks ç›®å½•"),
        ("/opt/bitnami/spark/scripts", "è„šæœ¬ç›®å½•")
    ]
    
    for dir_path, dir_name in directories:
        if os.path.exists(dir_path):
            files = os.listdir(dir_path)
            print(f"âœ… {dir_name}: {dir_path} ({len(files)} ä¸ªæ–‡ä»¶/ç›®å½•)")
            if files:
                print(f"   å†…å®¹ç¤ºä¾‹: {', '.join(files[:5])}")
        else:
            print(f"âŒ {dir_name}: {dir_path} ä¸å­˜åœ¨")
    
    # æµ‹è¯•æ–‡ä»¶è¯»å†™
    test_file = os.path.join(data_dir, "test_write.txt")
    try:
        with open(test_file, 'w') as f:
            f.write(f"æµ‹è¯•æ—¶é—´: {datetime.now()}")
        
        with open(test_file, 'r') as f:
            content = f.read()
        
        os.remove(test_file)
        print("âœ… æ–‡ä»¶è¯»å†™æµ‹è¯•é€šè¿‡")
        return True
        
    except Exception as e:
        print(f"âŒ æ–‡ä»¶è¯»å†™æµ‹è¯•å¤±è´¥: {str(e)}")
        return False

def test_jupyter_config():
    """æµ‹è¯• Jupyter é…ç½®"""
    print_section("Jupyter é…ç½®æµ‹è¯•")
    
    try:
        import jupyter_core
        import jupyterlab
        
        print(f"âœ… Jupyter Core ç‰ˆæœ¬: {jupyter_core.__version__}")
        print(f"âœ… JupyterLab ç‰ˆæœ¬: {jupyterlab.__version__}")
        
        # æ£€æŸ¥é…ç½®æ–‡ä»¶
        config_file = os.path.expanduser("~/.jupyter/jupyter_lab_config.py")
        if os.path.exists(config_file):
            print(f"âœ… é…ç½®æ–‡ä»¶å­˜åœ¨: {config_file}")
        else:
            print(f"âš ï¸  é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_file}")
        
        # æ£€æŸ¥ PySpark åˆå§‹åŒ–æ–‡ä»¶
        pyspark_init = "/opt/bitnami/spark/notebooks/pyspark_init.py"
        if os.path.exists(pyspark_init):
            print(f"âœ… PySpark åˆå§‹åŒ–æ–‡ä»¶å­˜åœ¨: {pyspark_init}")
        else:
            print(f"âš ï¸  PySpark åˆå§‹åŒ–æ–‡ä»¶ä¸å­˜åœ¨: {pyspark_init}")
        
        return True
        
    except ImportError as e:
        print(f"âŒ Jupyter å¯¼å…¥å¤±è´¥: {str(e)}")
        return False

def test_network_connectivity():
    """æµ‹è¯•ç½‘ç»œè¿é€šæ€§"""
    print_section("ç½‘ç»œè¿é€šæ€§æµ‹è¯•")
    
    import socket
    
    # æµ‹è¯•æœåŠ¡è¿æ¥
    services = [
        ("spark-master", 7077, "Spark Master"),
        ("spark-master", 8080, "Spark Master UI"),
        ("spark-worker-1", 8881, "Spark Worker 1"),
        ("spark-worker-2", 8882, "Spark Worker 2")
    ]
    
    for host, port, service in services:
        try:
            sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
            sock.settimeout(5)
            result = sock.connect_ex((host, port))
            sock.close()
            
            if result == 0:
                print(f"âœ… {service}: {host}:{port} - è¿æ¥æˆåŠŸ")
            else:
                print(f"âŒ {service}: {host}:{port} - è¿æ¥å¤±è´¥")
                
        except Exception as e:
            print(f"âŒ {service}: {host}:{port} - æµ‹è¯•å¤±è´¥: {str(e)}")

def test_basic_network():
    """æµ‹è¯•åŸºæœ¬ç½‘ç»œè¿é€šæ€§"""
    print_section("ç½‘ç»œè¿é€šæ€§æµ‹è¯•")
    
    import socket
    
    # æµ‹è¯•å…³é”®æœåŠ¡è¿æ¥
    services = [
        ("spark-master", 7077, "Spark Master"),
        ("spark-master", 8080, "Spark Master UI"),
        ("spark-worker-1", 8881, "Spark Worker 1"),
        ("spark-worker-2", 8882, "Spark Worker 2")
    ]
    
    network_ok = True
    for host, port, service in services:
        try:
            sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
            sock.settimeout(3)
            result = sock.connect_ex((host, port))
            sock.close()
            
            if result == 0:
                print(f"âœ… {service}: {host}:{port}")
            else:
                print(f"âŒ {service}: {host}:{port} - è¿æ¥å¤±è´¥")
                network_ok = False
                
        except Exception as e:
            print(f"âŒ {service}: {host}:{port} - å¼‚å¸¸: {str(e)}")
            network_ok = False
    
    return network_ok

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ å®¹å™¨å¼€å‘ç¯å¢ƒå¿«é€Ÿæµ‹è¯•")
    print(f"â° æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    # æ£€æŸ¥åŸºæœ¬ç¯å¢ƒ
    print(f"\nğŸ“‹ ç¯å¢ƒä¿¡æ¯:")
    print(f"  Python: {sys.version.split()[0]}")
    print(f"  å·¥ä½œç›®å½•: {os.getcwd()}")
    
    # æµ‹è¯•ç½‘ç»œè¿é€šæ€§
    network_ok = test_basic_network()
    
    # æµ‹è¯• Spark è¿æ¥
    spark_ok = test_spark_connection()
    
    if network_ok and spark_ok:
        print("\nğŸ‰ ç¯å¢ƒæµ‹è¯•é€šè¿‡ï¼")
        print("\nğŸ’¡ å¿«é€Ÿå¼€å§‹:")
        print("  â€¢ Jupyter Lab: http://localhost:8888 (token: spark-learning)")
        print("  â€¢ ç¤ºä¾‹ Notebook: container-spark-demo.ipynb")
        print("  â€¢ Spark UI: http://localhost:8080")
        print("\nğŸ” è¯¦ç»†ç½‘ç»œæµ‹è¯•:")
        print("  â€¢ è¿è¡Œ: python3 test_network_connectivity.py")
    else:
        print("\nâŒ ç¯å¢ƒæµ‹è¯•å¤±è´¥")
        if not network_ok:
            print("  âš ï¸  ç½‘ç»œè¿é€šæ€§é—®é¢˜")
        if not spark_ok:
            print("  âš ï¸  Spark è¿æ¥é—®é¢˜")
        print("\nğŸ”§ æ•…éšœæ’é™¤:")
        print("  â€¢ æ£€æŸ¥å®¹å™¨çŠ¶æ€: docker-compose ps")
        print("  â€¢ ç½‘ç»œè¯Šæ–­: python3 test_network_connectivity.py")
        print("  â€¢ é‡å¯ç¯å¢ƒ: docker-compose restart spark-dev")
        print("  â€¢ æŸ¥çœ‹æ—¥å¿—: docker-compose logs spark-dev")

if __name__ == "__main__":
    main()