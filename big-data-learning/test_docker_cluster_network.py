"""
é€šè¿‡æ­£ç¡®ç½‘ç»œé…ç½®è¿æ¥åˆ°Dockerä¸­çš„Sparké›†ç¾¤
"""
import os
import sys
import socket
from pyspark.sql import SparkSession
from pyspark import SparkConf

def get_local_ip():
    """
    è·å–æœ¬åœ°IPåœ°å€
    """
    try:
        # åˆ›å»ºä¸€ä¸ªUDP socketæ¥è·å–æœ¬åœ°IP
        s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
        # è¿æ¥åˆ°ä¸€ä¸ªè¿œç¨‹åœ°å€ï¼ˆä¸ä¼šçœŸæ­£å‘é€æ•°æ®ï¼‰
        s.connect(("8.8.8.8", 80))
        ip = s.getsockname()[0]
        s.close()
        return ip
    except Exception:
        return "127.0.0.1"

def test_docker_cluster_connection():
    """
    æµ‹è¯•è¿æ¥åˆ°Dockerä¸­çš„Sparké›†ç¾¤ï¼Œä½¿ç”¨æ­£ç¡®çš„ç½‘ç»œé…ç½®
    """
    print("=== æµ‹è¯•è¿æ¥åˆ°Dockerä¸­çš„Sparké›†ç¾¤ï¼ˆç½‘ç»œä¼˜åŒ–ï¼‰ ===")
    
    # è·å–æœ¬åœ°IPåœ°å€
    local_ip = get_local_ip()
    print(f"æœ¬åœ°IPåœ°å€: {local_ip}")
    
    # è®¾ç½®ç¯å¢ƒå˜é‡
    os.environ['PYSPARK_PYTHON'] = sys.executable
    os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable
    
    try:
        # åˆ›å»ºè¿æ¥åˆ°Dockeré›†ç¾¤çš„é…ç½®
        # ä½¿ç”¨ä¼˜åŒ–çš„ç½‘ç»œå’Œèµ„æºé…ç½®
        conf = SparkConf() \
            .setAppName("DockerClusterTestNetwork") \
            .setMaster("spark://localhost:7077") \
            .set("spark.driver.memory", "512m") \
            .set("spark.executor.memory", "512m") \
            .set("spark.executor.cores", "1") \
            .set("spark.cores.max", "2") \
            .set("spark.driver.host", local_ip) \
            .set("spark.driver.bindAddress", "0.0.0.0") \
            .set("spark.network.timeout", "240s") \
            .set("spark.executor.heartbeatInterval", "60s") \
            .set("spark.driver.port", "10000") \
            .set("spark.driver.blockManager.port", "10001") \
            .set("spark.ui.port", "4040")
        
        print("æ­£åœ¨åˆ›å»ºSparkä¼šè¯...")
        print("é…ç½®ä¿¡æ¯:")
        for item in conf.getAll():
            print(f"  {item[0]}: {item[1]}")
        
        spark = SparkSession.builder.config(conf=conf).getOrCreate()
        print("âœ… Sparkä¼šè¯åˆ›å»ºæˆåŠŸ!")
        
        # æµ‹è¯•åŸºæœ¬æ“ä½œ
        print("\n--- æµ‹è¯•åŸºæœ¬æ“ä½œ ---")
        print(f"Sparkç‰ˆæœ¬: {spark.version}")
        print(f"åº”ç”¨ID: {spark.sparkContext.applicationId}")
        
        # åˆ›å»ºä¸€ä¸ªç®€å•çš„DataFrameå¹¶è¿›è¡Œéæ”¶é›†æ“ä½œ
        print("\n--- æµ‹è¯•éæ”¶é›†æ“ä½œ ---")
        df = spark.range(100)
        count = df.count()  # countæ“ä½œä¼šåœ¨é›†ç¾¤ä¸Šæ‰§è¡Œä½†åªè¿”å›ä¸€ä¸ªæ•°å­—
        print(f"DataFrameè®¡æ•°: {count}")
        
        # è¿›è¡Œè¿‡æ»¤æ“ä½œ
        filtered_df = df.filter("id > 90")
        filtered_count = filtered_df.count()
        print(f"è¿‡æ»¤åçš„è®¡æ•°: {filtered_count}")
        
        spark.stop()
        print("\nâœ… æˆåŠŸè¿æ¥åˆ°Dockeré›†ç¾¤å¹¶å®Œæˆæµ‹è¯•!")
        return True
        
    except Exception as e:
        print(f"âŒ è¿æ¥å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def main():
    """
    ä¸»å‡½æ•°
    """
    print("å¼€å§‹æµ‹è¯•è¿æ¥åˆ°Dockerä¸­çš„Sparké›†ç¾¤ï¼ˆç½‘ç»œä¼˜åŒ–ï¼‰...")
    success = test_docker_cluster_connection()
    
    if success:
        print("\nğŸ‰ é›†ç¾¤è¿æ¥æµ‹è¯•æˆåŠŸ!")
        print("æ‚¨ç°åœ¨å¯ä»¥ä½¿ç”¨æœ¬åœ°PySparkè¿æ¥åˆ°Dockerä¸­çš„Sparké›†ç¾¤äº†ã€‚")
    else:
        print("\nâŒ é›†ç¾¤è¿æ¥æµ‹è¯•å¤±è´¥ã€‚")
        print("å¯èƒ½çš„é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆ:")
        print("1. ç½‘ç»œè¿æ¥é—®é¢˜:")
        print("   - ç¡®ä¿Dockerå®¹å™¨å¯ä»¥è®¿é—®æ‚¨çš„æœ¬åœ°æœºå™¨")
        print("   - æ£€æŸ¥é˜²ç«å¢™è®¾ç½®ï¼Œç¡®ä¿ç«¯å£10000å’Œ10001æ˜¯å¼€æ”¾çš„")
        print("2. èµ„æºé…ç½®é—®é¢˜:")
        print("   - æ£€æŸ¥Spark Master UI (http://localhost:8080) ç¡®ä¿æœ‰è¶³å¤Ÿçš„èµ„æº")
        print("3. ç‰ˆæœ¬å…¼å®¹æ€§é—®é¢˜:")
        print("   - ç¡®ä¿æœ¬åœ°PySparkç‰ˆæœ¬ä¸Dockerä¸­çš„Sparkç‰ˆæœ¬å…¼å®¹")

if __name__ == "__main__":
    main()