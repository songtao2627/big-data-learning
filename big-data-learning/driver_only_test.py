"""
ä»…ä½¿ç”¨é©±åŠ¨ç¨‹åºçš„PySparkæµ‹è¯•è„šæœ¬
"""
import os
import sys

def test_driver_only():
    """
    æµ‹è¯•ä»…ä½¿ç”¨é©±åŠ¨ç¨‹åºçš„PySparkåŠŸèƒ½
    """
    print("=== ä»…é©±åŠ¨ç¨‹åºæ¨¡å¼æµ‹è¯• ===")
    
    # æ£€æŸ¥PySparkç‰ˆæœ¬
    try:
        import pyspark
        print(f"PySparkç‰ˆæœ¬: {pyspark.__version__}")
    except ImportError:
        print("âŒ æ— æ³•å¯¼å…¥PySpark")
        return False
    
    # è®¾ç½®ç¯å¢ƒå˜é‡
    os.environ['PYSPARK_PYTHON'] = sys.executable
    os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable
    
    print("\n=== æµ‹è¯•SparkContextåŠŸèƒ½ ===")
    try:
        # å¯¼å…¥å¿…è¦çš„æ¨¡å—
        from pyspark.sql import SparkSession
        from pyspark import SparkContext, SparkConf
        
        # åˆ›å»ºé…ç½®ï¼Œå°½é‡å‡å°‘éœ€è¦æ‰§è¡Œå™¨çš„æ“ä½œ
        conf = SparkConf() \
            .setAppName("DriverOnlyTest") \
            .setMaster("local[1]") \
            .set("spark.driver.memory", "512m") \
            .set("spark.executor.memory", "512m") \
            .set("spark.sql.adaptive.enabled", "false") \
            .set("spark.sql.adaptive.coalescePartitions.enabled", "false")
        
        spark = SparkSession.builder.config(conf=conf).getOrCreate()
        sc = spark.sparkContext
        
        print("âœ… Sparkä¼šè¯å’Œä¸Šä¸‹æ–‡åˆ›å»ºæˆåŠŸ!")
        
        # æµ‹è¯•æœ¬åœ°æ•°æ®ç»“æ„
        print("\n--- æµ‹è¯•æœ¬åœ°æ•°æ®ç»“æ„ ---")
        # ä½¿ç”¨parallelizeä¼šè§¦å‘æ‰§è¡Œå™¨ï¼Œæˆ‘ä»¬é¿å…è¿™æ ·åš
        # ç›¸åï¼Œæˆ‘ä»¬æµ‹è¯•å¯ä»¥ç›´æ¥åœ¨é©±åŠ¨ç¨‹åºä¸­å®Œæˆçš„æ“ä½œ
        
        # æµ‹è¯•ç³»ç»Ÿä¿¡æ¯
        print(f"Sparkç‰ˆæœ¬: {sc.version}")
        print(f"åº”ç”¨åç§°: {sc.appName}")
        print(f"ä¸»èŠ‚ç‚¹: {sc.master}")
        
        # æµ‹è¯•é…ç½®ä¿¡æ¯
        print("\n--- æµ‹è¯•é…ç½®ä¿¡æ¯ ---")
        print(f"é©±åŠ¨ç¨‹åºå†…å­˜: {sc.getConf().get('spark.driver.memory', 'æœªè®¾ç½®')}")
        print(f"æ‰§è¡Œå™¨å†…å­˜: {sc.getConf().get('spark.executor.memory', 'æœªè®¾ç½®')}")
        
        # æµ‹è¯•ç®€å•çš„æœ¬åœ°æ“ä½œ
        print("\n--- æµ‹è¯•æœ¬åœ°æ“ä½œ ---")
        # åˆ›å»ºä¸€ä¸ªç®€å•çš„æœ¬åœ°åˆ—è¡¨
        local_data = [1, 2, 3, 4, 5]
        print(f"æœ¬åœ°æ•°æ®: {local_data}")
        
        # å¯¹æœ¬åœ°æ•°æ®è¿›è¡Œæ“ä½œ
        squared_data = [x ** 2 for x in local_data]
        print(f"å¹³æ–¹åæ•°æ®: {squared_data}")
        
        # æµ‹è¯•å­—ç¬¦ä¸²æ“ä½œ
        test_string = "Hello PySpark"
        print(f"æµ‹è¯•å­—ç¬¦ä¸²: {test_string}")
        print(f"å­—ç¬¦ä¸²é•¿åº¦: {len(test_string)}")
        print(f"å¤§å†™è½¬æ¢: {test_string.upper()}")
        
        spark.stop()
        print("\nâœ… ä»…é©±åŠ¨ç¨‹åºæµ‹è¯•é€šè¿‡!")
        return True
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def main():
    """
    ä¸»å‡½æ•°
    """
    print("å¼€å§‹ä»…é©±åŠ¨ç¨‹åºæ¨¡å¼æµ‹è¯•...")
    success = test_driver_only()
    
    if success:
        print("\nğŸ‰ ä»…é©±åŠ¨ç¨‹åºæµ‹è¯•æˆåŠŸ!")
        print("æ‚¨çš„PySparkç¯å¢ƒå·²æ­£ç¡®å®‰è£…ï¼Œå¯ä»¥è®¿é—®Spark APIã€‚")
        print("\næ³¨æ„:")
        print("- æ‚¨å¯ä»¥ä½¿ç”¨Sparké…ç½®å’Œä¸Šä¸‹æ–‡å¯¹è±¡")
        print("- æŸäº›åˆ†å¸ƒå¼æ“ä½œå¯èƒ½éœ€è¦é¢å¤–é…ç½®æ‰èƒ½æ­£å¸¸å·¥ä½œ")
        print("- è¿™æ˜¯è§£å†³Windowsä¸ŠPySparké—®é¢˜çš„ç¬¬ä¸€æ­¥")
    else:
        print("\nâŒ ä»…é©±åŠ¨ç¨‹åºæµ‹è¯•å¤±è´¥ã€‚")

if __name__ == "__main__":
    main()