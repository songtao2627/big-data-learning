"""
æµ‹è¯•ä¿®å¤åçš„PySparkç¯å¢ƒ
"""
import os
import sys
from pyspark.sql import SparkSession
from pyspark import SparkConf

def test_pyspark_environment():
    """
    æµ‹è¯•PySparkç¯å¢ƒæ˜¯å¦æ­£å¸¸å·¥ä½œ
    """
    print("=== æµ‹è¯•ä¿®å¤åçš„PySparkç¯å¢ƒ ===")
    
    # æ£€æŸ¥PySparkç‰ˆæœ¬
    try:
        import pyspark
        print(f"PySparkç‰ˆæœ¬: {pyspark.__version__}")
    except ImportError:
        print("âŒ æ— æ³•å¯¼å…¥PySpark")
        return False
    
    # æ£€æŸ¥Javaç‰ˆæœ¬
    try:
        import subprocess
        result = subprocess.run(["java", "-version"], capture_output=True, text=True)
        print("Javaç‰ˆæœ¬ä¿¡æ¯:")
        print(result.stderr.strip())
    except Exception as e:
        print(f"æ— æ³•æ£€æŸ¥Javaç‰ˆæœ¬: {e}")
    
    # è®¾ç½®ç¯å¢ƒå˜é‡
    os.environ['PYSPARK_PYTHON'] = sys.executable
    os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable
    
    print("\n=== æµ‹è¯•æœ¬åœ°Sparkä¼šè¯ ===")
    try:
        # åˆ›å»ºæœ¬åœ°Sparkä¼šè¯
        conf = SparkConf() \
            .setAppName("TestFixedEnvironment") \
            .setMaster("local[2]") \
            .set("spark.driver.memory", "512m") \
            .set("spark.executor.memory", "512m")
        
        spark = SparkSession.builder.config(conf=conf).getOrCreate()
        print("âœ… Sparkä¼šè¯åˆ›å»ºæˆåŠŸ!")
        
        # æµ‹è¯•åŸºæœ¬æ“ä½œ
        print("\n--- æµ‹è¯•åŸºæœ¬æ“ä½œ ---")
        df = spark.range(10)
        result = df.filter("id > 5").collect()
        print(f"è¿‡æ»¤ç»“æœ: {[r.id for r in result]}")
        
        # æµ‹è¯•DataFrameæ“ä½œ
        print("\n--- æµ‹è¯•DataFrameæ“ä½œ ---")
        data = [("Alice", 25), ("Bob", 30), ("Charlie", 35)]
        columns = ["Name", "Age"]
        df = spark.createDataFrame(data, columns)
        df.show()
        
        # æµ‹è¯•SQLæ“ä½œ
        print("\n--- æµ‹è¯•SQLæ“ä½œ ---")
        df.createOrReplaceTempView("people")
        sql_result = spark.sql("SELECT * FROM people WHERE Age > 25").collect()
        print(f"å¹´é¾„å¤§äº25çš„äºº: {[r.Name for r in sql_result]}")
        
        spark.stop()
        print("\nâœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡! æœ¬åœ°PySparkç¯å¢ƒå·²æˆåŠŸä¿®å¤ã€‚")
        return True
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def main():
    """
    ä¸»å‡½æ•°
    """
    print("å¼€å§‹æµ‹è¯•ä¿®å¤åçš„PySparkç¯å¢ƒ...")
    success = test_pyspark_environment()
    
    if success:
        print("\nğŸ‰ ç¯å¢ƒä¿®å¤æˆåŠŸ!")
        print("\nç°åœ¨æ‚¨å¯ä»¥:")
        print("1. ä½¿ç”¨æœ¬åœ°æ¨¡å¼è¿›è¡ŒSparkå¼€å‘å’Œæµ‹è¯•")
        print("2. è¿æ¥åˆ°Dockerä¸­çš„Sparké›†ç¾¤ (éœ€è¦ç½‘ç»œé…ç½®)")
        print("3. åœ¨Jupyter Notebookä¸­ä½¿ç”¨PySpark")
    else:
        print("\nâŒ ç¯å¢ƒä¿®å¤å¤±è´¥ï¼Œè¯·æ£€æŸ¥é…ç½®ã€‚")

if __name__ == "__main__":
    main()