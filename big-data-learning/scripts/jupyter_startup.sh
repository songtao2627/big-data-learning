#!/bin/bash
# Jupyterå¯åŠ¨è„šæœ¬ - å®Œæ•´ç‰ˆæœ¬

echo "å¼€å§‹é…ç½®Jupyterå’ŒPySparkçŽ¯å¢ƒ..."

# æ³¨æ„ï¼šPySpark éœ€è¦æ‰‹åŠ¨å®‰è£…ä»¥ä¾¿å­¦ä¹ è¿‡ç¨‹å¯è§
echo "è·³è¿‡è‡ªåŠ¨å®‰è£…ï¼Œç­‰å¾…æ‰‹åŠ¨å®‰è£… PySpark..."

# é…ç½®PySparkçŽ¯å¢ƒå˜é‡
echo "é…ç½®PySparkçŽ¯å¢ƒå˜é‡..."
export SPARK_HOME=/usr/local/spark
export PYTHONPATH=$SPARK_HOME/python:$PYTHONPATH
export PYSPARK_PYTHON=/opt/conda/bin/python
export PYSPARK_DRIVER_PYTHON=/opt/conda/bin/python
export PYSPARK_DRIVER_PYTHON_OPTS="notebook"

# åˆ›å»ºå·¥ä½œç›®å½•
mkdir -p /home/jovyan/work/custom
mkdir -p /home/jovyan/work/examples

# åˆ›å»ºSparkè¿žæŽ¥æµ‹è¯•è„šæœ¬
echo "åˆ›å»ºæµ‹è¯•è„šæœ¬..."
cat > /home/jovyan/work/test_spark_connection.py << 'EOL'
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Sparkè¿žæŽ¥æµ‹è¯•è„šæœ¬
ç”¨äºŽéªŒè¯Jupyterä¸ŽSparké›†ç¾¤çš„è¿žæŽ¥
"""

from pyspark.sql import SparkSession
import sys

def test_spark_connection():
    """æµ‹è¯•Sparkè¿žæŽ¥"""
    try:
        # åˆ›å»ºSparkSession
        print("æ­£åœ¨è¿žæŽ¥Sparké›†ç¾¤...")
        spark = SparkSession.builder \
            .appName("SparkConnectionTest") \
            .master("spark://spark-master:7077") \
            .config("spark.sql.adaptive.enabled", "true") \
            .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
            .getOrCreate()

        # æµ‹è¯•è¿žæŽ¥
        print(f"âœ… Sparkç‰ˆæœ¬: {spark.version}")
        print(f"âœ… Sparkåº”ç”¨ID: {spark.sparkContext.applicationId}")
        print(f"âœ… Spark UIåœ°å€: {spark.sparkContext.uiWebUrl}")
        
        # åˆ›å»ºæµ‹è¯•DataFrame
        print("\nåˆ›å»ºæµ‹è¯•DataFrame...")
        test_data = [("å¼ ä¸‰", 25, "åŒ—äº¬"), ("æŽå››", 30, "ä¸Šæµ·"), ("çŽ‹äº”", 35, "å¹¿å·ž")]
        columns = ["å§“å", "å¹´é¾„", "åŸŽå¸‚"]
        df = spark.createDataFrame(test_data, columns)
        
        print("æµ‹è¯•DataFrameå†…å®¹:")
        df.show()
        
        print(f"DataFrameè¡Œæ•°: {df.count()}")
        print(f"DataFrameåˆ—æ•°: {len(df.columns)}")
        
        # ç®€å•çš„æ•°æ®æ“ä½œæµ‹è¯•
        print("\næ‰§è¡Œç®€å•çš„æ•°æ®æ“ä½œ...")
        avg_age = df.agg({"å¹´é¾„": "avg"}).collect()[0][0]
        print(f"å¹³å‡å¹´é¾„: {avg_age:.1f}")
        
        # åœæ­¢SparkSession
        spark.stop()
        print("\nðŸŽ‰ Sparkè¿žæŽ¥æµ‹è¯•æˆåŠŸï¼")
        return True
        
    except Exception as e:
        print(f"âŒ Sparkè¿žæŽ¥æµ‹è¯•å¤±è´¥: {str(e)}")
        return False

if __name__ == "__main__":
    test_spark_connection()
EOL

# åˆ›å»ºæ¬¢è¿Žç¬”è®°æœ¬
cat > /home/jovyan/work/00-welcome.ipynb << 'EOL'
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸŽ‰ æ¬¢è¿Žä½¿ç”¨å¤§æ•°æ®å­¦ä¹ å¹³å°ï¼\n",
    "\n",
    "è¿™æ˜¯ä¸€ä¸ªåŸºäºŽDockerçš„Apache Sparkå­¦ä¹ çŽ¯å¢ƒï¼ŒåŒ…å«ï¼š\n",
    "- Apache Spark 3.4 é›†ç¾¤\n",
    "- Jupyter Notebook with PySpark\n",
    "- ä¸°å¯Œçš„å­¦ä¹ ææ–™å’Œå®žè·µé¡¹ç›®\n",
    "\n",
    "## ðŸš€ å¿«é€Ÿå¼€å§‹\n",
    "\n",
    "### 1. æµ‹è¯•Sparkè¿žæŽ¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¿è¡ŒSparkè¿žæŽ¥æµ‹è¯•\n",
    "exec(open('test_spark_connection.py').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. ç®€å•çš„PySparkç¤ºä¾‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# åˆ›å»ºSparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Welcome\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# åˆ›å»ºç®€å•çš„DataFrame\n",
    "data = [(\"Alice\", 25), (\"Bob\", 30), (\"Charlie\", 35)]\n",
    "df = spark.createDataFrame(data, [\"name\", \"age\"])\n",
    "\n",
    "print(\"DataFrameå†…å®¹:\")\n",
    "df.show()\n",
    "\n",
    "print(f\"æ€»è¡Œæ•°: {df.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. å­¦ä¹ è·¯å¾„\n",
    "\n",
    "å»ºè®®æŒ‰ä»¥ä¸‹é¡ºåºå­¦ä¹ ï¼š\n",
    "\n",
    "1. **SparkåŸºç¡€** (`01-spark-basics/`)\n",
    "   - RDDåŸºç¡€æ“ä½œ\n",
    "   - DataFrame API\n",
    "   - Dataset API\n",
    "\n",
    "2. **Spark SQL** (`02-spark-sql/`)\n",
    "   - SQLåŸºç¡€æŸ¥è¯¢\n",
    "   - é«˜çº§æŸ¥è¯¢æŠ€å·§\n",
    "   - æ€§èƒ½è°ƒä¼˜\n",
    "\n",
    "3. **Spark Streaming** (`03-spark-streaming/`)\n",
    "   - æµå¤„ç†åŸºç¡€\n",
    "   - ç»“æž„åŒ–æµ\n",
    "   - Kafkaé›†æˆ\n",
    "\n",
    "4. **å®žè·µé¡¹ç›®** (`04-projects/`)\n",
    "   - æ—¥å¿—åˆ†æž\n",
    "   - æŽ¨èç³»ç»Ÿ\n",
    "   - å®žæ—¶ä»ªè¡¨ç›˜\n",
    "\n",
    "### 4. æœ‰ç”¨çš„é“¾æŽ¥\n",
    "\n",
    "- **Spark Master UI**: http://localhost:8080\n",
    "- **Sparkåº”ç”¨UI**: http://localhost:4040 (è¿è¡Œä½œä¸šæ—¶)\n",
    "- **å­¦ä¹ è·¯å¾„**: [learning_path.md](learning_path.md)\n",
    "\n",
    "ç¥å­¦ä¹ æ„‰å¿«ï¼ðŸŽ“"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
EOL

echo "âœ… JupyterçŽ¯å¢ƒé…ç½®å®Œæˆï¼"
echo "ðŸ“š å·²åˆ›å»ºæ¬¢è¿Žç¬”è®°æœ¬å’Œæµ‹è¯•è„šæœ¬"
echo "ðŸš€ æ­£åœ¨å¯åŠ¨Jupyter Lab..."

# å¯åŠ¨Jupyter Lab
# ç¡®ä¿çŽ¯å¢ƒå˜é‡åœ¨ Jupyter è¿›ç¨‹ä¸­å¯ç”¨
export SPARK_HOME=/usr/local/spark
export PYTHONPATH=$SPARK_HOME/python:$PYTHONPATH

exec /opt/conda/bin/jupyter lab \
    --NotebookApp.token='' \
    --NotebookApp.password='' \
    --NotebookApp.allow_root=True \
    --ip='0.0.0.0' \
    --port=8888 \
    --no-browser