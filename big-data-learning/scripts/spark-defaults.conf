# Spark配置文件 - 大数据学习平台
# ===========================================

# 基本配置
spark.master                     spark://spark-master:7077
spark.app.name                   BigDataLearning
spark.driver.memory              1g
spark.executor.memory            2g
spark.executor.cores             2
spark.driver.cores               1
spark.driver.maxResultSize       1g

# 自适应查询执行 (AQE) - Spark 3.x 新特性
spark.sql.adaptive.enabled                    true
spark.sql.adaptive.coalescePartitions.enabled true
spark.sql.adaptive.skewJoin.enabled           true
spark.sql.adaptive.localShuffleReader.enabled true

# 动态分区裁剪 - 性能优化
spark.sql.optimizer.dynamicPartitionPruning.enabled true

# 序列化配置
spark.serializer                 org.apache.spark.serializer.KryoSerializer
spark.kryo.unsafe                true
spark.kryo.registrationRequired  false

# 网络配置
spark.network.timeout            800s
spark.executor.heartbeatInterval 60s
spark.rpc.askTimeout             600s
spark.rpc.lookupTimeout          120s

# 内存管理
spark.memory.fraction            0.6
spark.memory.storageFraction     0.5
spark.executor.memoryFraction    0.6

# UI配置
spark.ui.enabled                 true
spark.ui.port                    4040
spark.ui.retainedJobs            1000
spark.ui.retainedStages          1000

# 历史服务器配置
spark.eventLog.enabled           true
spark.eventLog.dir               /opt/bitnami/spark/logs
spark.eventLog.compress          true

# SQL配置
spark.sql.warehouse.dir          /opt/bitnami/spark/warehouse
spark.sql.shuffle.partitions     200
spark.sql.adaptive.advisoryPartitionSizeInBytes 64MB
spark.sql.adaptive.coalescePartitions.minPartitionNum 1

# 广播连接配置
spark.sql.autoBroadcastJoinThreshold 10MB

# 缓存配置
spark.sql.inMemoryColumnarStorage.compressed true
spark.sql.inMemoryColumnarStorage.batchSize  10000

# 代码生成配置
spark.sql.codegen.wholeStage     true
spark.sql.codegen.fallback       true

# 检查点配置
spark.sql.streaming.checkpointLocation /opt/bitnami/spark/checkpoints

# 动态资源分配（可选）
spark.dynamicAllocation.enabled false
spark.dynamicAllocation.minExecutors 1
spark.dynamicAllocation.maxExecutors 3

# 清理配置
spark.cleaner.ttl                3600
spark.worker.cleanup.enabled     true
spark.worker.cleanup.interval    1800

# Python配置
spark.pyspark.python             python3
spark.pyspark.driver.python      python3

# 日志配置
spark.log.level                  WARN