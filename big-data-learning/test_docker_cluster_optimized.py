"""
ä¼˜åŒ–èµ„æºé…ç½®ä»¥è¿æ¥åˆ°Dockerä¸­çš„Sparké›†ç¾¤
"""
import os
import sys
from pyspark.sql import SparkSession
from pyspark import SparkConf

def test_docker_cluster_connection():
    """
    æµ‹è¯•è¿æ¥åˆ°Dockerä¸­çš„Sparké›†ç¾¤ï¼Œä½¿ç”¨ä¼˜åŒ–çš„èµ„æºé…ç½®
    """
    print("=== æµ‹è¯•è¿æ¥åˆ°Dockerä¸­çš„Sparké›†ç¾¤ï¼ˆä¼˜åŒ–é…ç½®ï¼‰ ===")
    
    # è®¾ç½®ç¯å¢ƒå˜é‡
    os.environ['PYSPARK_PYTHON'] = sys.executable
    os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable
    
    try:
        # åˆ›å»ºè¿æ¥åˆ°Dockeré›†ç¾¤çš„é…ç½®
        # ä¼˜åŒ–èµ„æºé…ç½®ä»¥é€‚åº”é›†ç¾¤æƒ…å†µ
        conf = SparkConf() \
            .setAppName("DockerClusterTestOptimized") \
            .setMaster("spark://localhost:7077") \
            .set("spark.driver.memory", "512m") \
            .set("spark.executor.memory", "512m") \
            .set("spark.executor.cores", "1") \
            .set("spark.cores.max", "2") \
            .set("spark.driver.host", "127.0.0.1") \
            .set("spark.driver.bindAddress", "127.0.0.1") \
            .set("spark.network.timeout", "240s") \
            .set("spark.executor.heartbeatInterval", "60s")
        
        print("æ­£åœ¨åˆ›å»ºSparkä¼šè¯...")
        spark = SparkSession.builder.config(conf=conf).getOrCreate()
        print("âœ… Sparkä¼šè¯åˆ›å»ºæˆåŠŸ!")
        
        # æµ‹è¯•åŸºæœ¬æ“ä½œ
        print("\n--- æµ‹è¯•åŸºæœ¬æ“ä½œ ---")
        print(f"Sparkç‰ˆæœ¬: {spark.version}")
        print(f"åº”ç”¨ID: {spark.sparkContext.applicationId}")
        
        # åˆ›å»ºä¸€ä¸ªç®€å•çš„DataFrameå¹¶è¿›è¡Œéæ”¶é›†æ“ä½œ
        print("\n--- æµ‹è¯•éæ”¶é›†æ“ä½œ ---")
        df = spark.range(1000)
        count = df.count()  # countæ“ä½œä¼šåœ¨é›†ç¾¤ä¸Šæ‰§è¡Œä½†åªè¿”å›ä¸€ä¸ªæ•°å­—
        print(f"DataFrameè®¡æ•°: {count}")
        
        # è¿›è¡Œè¿‡æ»¤æ“ä½œ
        filtered_df = df.filter("id > 900")
        filtered_count = filtered_df.count()
        print(f"è¿‡æ»¤åçš„è®¡æ•°: {filtered_count}")
        
        # æµ‹è¯•ç®€å•çš„èšåˆæ“ä½œ
        from pyspark.sql.functions import sum
        sum_result = df.agg(sum("id")).collect()
        print(f"IDæ€»å’Œ: {sum_result[0][0]}")
        
        spark.stop()
        print("\nâœ… æˆåŠŸè¿æ¥åˆ°Dockeré›†ç¾¤å¹¶å®Œæˆæµ‹è¯•!")
        return True
        
    except Exception as e:
        print(f"âŒ è¿æ¥å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def main():
    """
    ä¸»å‡½æ•°
    """
    print("å¼€å§‹æµ‹è¯•è¿æ¥åˆ°Dockerä¸­çš„Sparké›†ç¾¤ï¼ˆä¼˜åŒ–é…ç½®ï¼‰...")
    success = test_docker_cluster_connection()
    
    if success:
        print("\nğŸ‰ é›†ç¾¤è¿æ¥æµ‹è¯•æˆåŠŸ!")
        print("æ‚¨ç°åœ¨å¯ä»¥ä½¿ç”¨æœ¬åœ°PySparkè¿æ¥åˆ°Dockerä¸­çš„Sparké›†ç¾¤äº†ã€‚")
    else:
        print("\nâŒ é›†ç¾¤è¿æ¥æµ‹è¯•å¤±è´¥ã€‚")
        print("è¯·æ£€æŸ¥ä»¥ä¸‹å‡ ç‚¹:")
        print("1. Dockerä¸­çš„Sparké›†ç¾¤æ˜¯å¦æ­£åœ¨è¿è¡Œ")
        print("2. ç«¯å£7077æ˜¯å¦æ­£ç¡®æ˜ å°„")
        print("3. ç½‘ç»œè¿æ¥æ˜¯å¦æ­£å¸¸")
        print("4. é˜²ç«å¢™æ˜¯å¦é˜»æ­¢äº†è¿æ¥")

if __name__ == "__main__":
    main()