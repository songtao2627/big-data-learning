#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Spark é—®é¢˜è¯Šæ–­è„šæœ¬
å¸®åŠ©ç†è§£ df.show() å¡ä½çš„åŸå› 
"""

from pyspark.sql import SparkSession
import time
import sys

def diagnose_spark_connection():
    """é€æ­¥è¯Šæ–­ Spark è¿æ¥é—®é¢˜"""
    
    print("=== Spark è¿æ¥è¯Šæ–­å¼€å§‹ ===\n")
    
    # æ­¥éª¤1: åˆ›å»º SparkSessionï¼ˆè¿™ä¸€æ­¥é€šå¸¸å¾ˆå¿«ï¼‰
    print("æ­¥éª¤1: åˆ›å»º SparkSession...")
    start_time = time.time()
    
    try:
        spark = SparkSession.builder \
            .appName("SparkDiagnosis") \
            .master("spark://spark-master:7077") \
            .config("spark.driver.memory", "512m") \
            .config("spark.executor.memory", "1g") \
            .config("spark.executor.cores", "1") \
            .config("spark.sql.adaptive.enabled", "true") \
            .getOrCreate()
        
        session_time = time.time() - start_time
        print(f"âœ… SparkSession åˆ›å»ºæˆåŠŸ (è€—æ—¶: {session_time:.2f}ç§’)")
        print(f"   Sparkç‰ˆæœ¬: {spark.version}")
        print(f"   åº”ç”¨ID: {spark.sparkContext.applicationId}")
        
    except Exception as e:
        print(f"âŒ SparkSession åˆ›å»ºå¤±è´¥: {e}")
        return False
    
    # æ­¥éª¤2: æ£€æŸ¥ SparkContext çŠ¶æ€
    print(f"\næ­¥éª¤2: æ£€æŸ¥ SparkContext çŠ¶æ€...")
    sc = spark.sparkContext
    print(f"   çŠ¶æ€: {sc.statusTracker().getExecutorInfos()}")
    print(f"   é»˜è®¤å¹¶è¡Œåº¦: {sc.defaultParallelism}")
    
    # æ­¥éª¤3: åˆ›å»ºç®€å•çš„ DataFrameï¼ˆè¿™ä¸€æ­¥ä¹Ÿå¾ˆå¿«ï¼Œå› ä¸ºæ˜¯æ‡’æ‰§è¡Œï¼‰
    print(f"\næ­¥éª¤3: åˆ›å»º DataFrame...")
    start_time = time.time()
    
    test_data = [("æµ‹è¯•", 1), ("æ•°æ®", 2)]
    columns = ["æ–‡æœ¬", "æ•°å­—"]
    df = spark.createDataFrame(test_data, columns)
    
    create_time = time.time() - start_time
    print(f"âœ… DataFrame åˆ›å»ºæˆåŠŸ (è€—æ—¶: {create_time:.2f}ç§’)")
    print("   æ³¨æ„: è¿™æ˜¯æ‡’æ‰§è¡Œï¼Œè¿˜æ²¡æœ‰å®é™…è®¡ç®—")
    
    # æ­¥éª¤4: æ‰§è¡Œç¬¬ä¸€ä¸ª Actionï¼ˆè¿™é‡Œå¯èƒ½ä¼šå¡ä½ï¼‰
    print(f"\næ­¥éª¤4: æ‰§è¡Œç¬¬ä¸€ä¸ª Action (df.count())...")
    print("   è¿™ä¸€æ­¥ä¼šè§¦å‘ executor çš„å®é™…å¯åŠ¨å’Œä»»åŠ¡åˆ†é…")
    start_time = time.time()
    
    try:
        # è®¾ç½®è¾ƒçŸ­çš„è¶…æ—¶æ¥å¿«é€Ÿå‘ç°é—®é¢˜
        spark.conf.set("spark.network.timeout", "60s")
        spark.conf.set("spark.executor.heartbeatInterval", "10s")
        
        count = df.count()
        action_time = time.time() - start_time
        print(f"âœ… ç¬¬ä¸€ä¸ª Action æ‰§è¡ŒæˆåŠŸ (è€—æ—¶: {action_time:.2f}ç§’)")
        print(f"   æ•°æ®è¡Œæ•°: {count}")
        
        # å¦‚æœ count() æˆåŠŸï¼Œshow() åº”è¯¥ä¹Ÿä¼šæˆåŠŸ
        print(f"\næ­¥éª¤5: æ‰§è¡Œ df.show()...")
        start_time = time.time()
        df.show()
        show_time = time.time() - start_time
        print(f"âœ… df.show() æ‰§è¡ŒæˆåŠŸ (è€—æ—¶: {show_time:.2f}ç§’)")
        
    except Exception as e:
        action_time = time.time() - start_time
        print(f"âŒ Action æ‰§è¡Œå¤±è´¥ (è€—æ—¶: {action_time:.2f}ç§’)")
        print(f"   é”™è¯¯ä¿¡æ¯: {e}")
        print(f"   è¿™å°±æ˜¯ä½ çš„è„šæœ¬å¡ä½çš„åŸå› ï¼")
        
        # æä¾›è§£å†³å»ºè®®
        print(f"\nğŸ”§ å¯èƒ½çš„è§£å†³æ–¹æ¡ˆ:")
        print(f"   1. æ£€æŸ¥ executor æ˜¯å¦æˆåŠŸå¯åŠ¨")
        print(f"   2. å¢åŠ ç½‘ç»œè¶…æ—¶æ—¶é—´")
        print(f"   3. å‡å°‘èµ„æºéœ€æ±‚")
        print(f"   4. æ£€æŸ¥ Docker å®¹å™¨èµ„æºé™åˆ¶")
        
        return False
    
    finally:
        print(f"\næ¸…ç†èµ„æº...")
        spark.stop()
    
    print(f"\n=== è¯Šæ–­å®Œæˆ ===")
    return True

if __name__ == "__main__":
    diagnose_spark_connection()