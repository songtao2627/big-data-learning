# Sparkæ€§èƒ½ç›‘æ§å’Œè°ƒä¼˜å®è·µ

æœ¬æ–‡æ¡£æä¾›Sparkåº”ç”¨ç¨‹åºæ€§èƒ½ç›‘æ§å’Œè°ƒä¼˜çš„å®è·µæŒ‡å—ï¼Œå¸®åŠ©æ‚¨è¯†åˆ«æ€§èƒ½ç“¶é¢ˆå¹¶ä¼˜åŒ–åº”ç”¨ç¨‹åºã€‚

## 1. Spark UIè¯¦è§£

### 1.1 è®¿é—®Spark UI

Spark UIæ˜¯ç›‘æ§Sparkåº”ç”¨ç¨‹åºçš„ä¸»è¦å·¥å…·ï¼Œé€šå¸¸å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼è®¿é—®ï¼š

- **Driver UI**: http://localhost:4040 (é»˜è®¤ç«¯å£)
- **Spark Master UI**: http://localhost:8080
- **Spark Worker UI**: http://localhost:8081

### 1.2 Spark UIä¸»è¦é¡µé¢

#### Jobsé¡µé¢
- æ˜¾ç¤ºæ‰€æœ‰ä½œä¸šçš„æ‰§è¡ŒçŠ¶æ€
- æä¾›ä½œä¸šæ‰§è¡Œæ—¶é—´å’Œé˜¶æ®µä¿¡æ¯
- å¸®åŠ©è¯†åˆ«æ…¢ä½œä¸šå’Œå¤±è´¥ä½œä¸š

#### Stagesé¡µé¢
- æ˜¾ç¤ºæ¯ä¸ªé˜¶æ®µçš„è¯¦ç»†ä¿¡æ¯
- åŒ…å«ä»»åŠ¡åˆ†å¸ƒå’Œæ‰§è¡Œæ—¶é—´
- å¸®åŠ©è¯†åˆ«æ•°æ®å€¾æ–œé—®é¢˜

#### Storageé¡µé¢
- æ˜¾ç¤ºç¼“å­˜çš„RDDå’ŒDataFrame
- æä¾›å†…å­˜ä½¿ç”¨æƒ…å†µ
- å¸®åŠ©ä¼˜åŒ–ç¼“å­˜ç­–ç•¥

#### Environmenté¡µé¢
- æ˜¾ç¤ºSparké…ç½®å‚æ•°
- åŒ…å«ç³»ç»Ÿå±æ€§å’Œç±»è·¯å¾„ä¿¡æ¯
- ç”¨äºéªŒè¯é…ç½®æ˜¯å¦æ­£ç¡®

#### Executorsé¡µé¢
- æ˜¾ç¤ºæ‰€æœ‰æ‰§è¡Œå™¨çš„çŠ¶æ€
- åŒ…å«å†…å­˜ä½¿ç”¨ã€GCæ—¶é—´ç­‰æŒ‡æ ‡
- å¸®åŠ©è¯†åˆ«èµ„æºåˆ†é…é—®é¢˜

#### SQLé¡µé¢
- æ˜¾ç¤ºSQLæŸ¥è¯¢çš„æ‰§è¡Œè®¡åˆ’
- æä¾›æŸ¥è¯¢æ€§èƒ½æŒ‡æ ‡
- å¸®åŠ©ä¼˜åŒ–SQLæŸ¥è¯¢

## 2. æ€§èƒ½æŒ‡æ ‡ç›‘æ§

### 2.1 å…³é”®æ€§èƒ½æŒ‡æ ‡

```python
# è·å–Sparkåº”ç”¨ç¨‹åºæŒ‡æ ‡
def get_spark_metrics(spark):
    sc = spark.sparkContext
    
    # åº”ç”¨ç¨‹åºä¿¡æ¯
    app_info = {
        'app_id': sc.applicationId,
        'app_name': sc.appName,
        'master': sc.master,
        'default_parallelism': sc.defaultParallelism
    }
    
    # æ‰§è¡Œå™¨ä¿¡æ¯
    executor_infos = sc.statusTracker().getExecutorInfos()
    
    # æ´»è·ƒä½œä¸š
    active_jobs = sc.statusTracker().getActiveJobIds()
    
    return {
        'app_info': app_info,
        'executor_count': len(executor_infos),
        'active_jobs': len(active_jobs)
    }

# ä½¿ç”¨ç¤ºä¾‹
metrics = get_spark_metrics(spark)
print(f"åº”ç”¨ç¨‹åºID: {metrics['app_info']['app_id']}")
print(f"æ‰§è¡Œå™¨æ•°é‡: {metrics['executor_count']}")
print(f"æ´»è·ƒä½œä¸šæ•°: {metrics['active_jobs']}")
```

### 2.2 è‡ªå®šä¹‰æŒ‡æ ‡æ”¶é›†

```python
from pyspark.sql.functions import *
import time

def monitor_dataframe_operation(df, operation_name):
    """ç›‘æ§DataFrameæ“ä½œçš„æ€§èƒ½"""
    start_time = time.time()
    
    # æ‰§è¡Œæ“ä½œå‰çš„åˆ†åŒºä¿¡æ¯
    partition_count = df.rdd.getNumPartitions()
    
    # æ‰§è¡Œæ“ä½œ
    result = df.count()  # æˆ–å…¶ä»–æ“ä½œ
    
    end_time = time.time()
    execution_time = end_time - start_time
    
    print(f"=== {operation_name} æ€§èƒ½æŠ¥å‘Š ===")
    print(f"æ‰§è¡Œæ—¶é—´: {execution_time:.2f} ç§’")
    print(f"åˆ†åŒºæ•°é‡: {partition_count}")
    print(f"è®°å½•æ•°é‡: {result}")
    print(f"æ¯ç§’å¤„ç†è®°å½•æ•°: {result/execution_time:.2f}")
    print("=" * 40)
    
    return result

# ä½¿ç”¨ç¤ºä¾‹
df = spark.range(1000000).toDF("id")
monitor_dataframe_operation(df, "Count Operation")
```

## 3. å†…å­˜ç›‘æ§

### 3.1 å†…å­˜ä½¿ç”¨åˆ†æ

```python
def analyze_memory_usage(spark):
    """åˆ†æSparkåº”ç”¨ç¨‹åºçš„å†…å­˜ä½¿ç”¨æƒ…å†µ"""
    sc = spark.sparkContext
    
    # è·å–æ‰§è¡Œå™¨ä¿¡æ¯
    executor_infos = sc.statusTracker().getExecutorInfos()
    
    total_memory = 0
    total_used = 0
    
    print("=== æ‰§è¡Œå™¨å†…å­˜ä½¿ç”¨æƒ…å†µ ===")
    for executor in executor_infos:
        memory_used = executor.memoryUsed
        max_memory = executor.maxMemory
        
        total_memory += max_memory
        total_used += memory_used
        
        usage_percent = (memory_used / max_memory) * 100 if max_memory > 0 else 0
        
        print(f"æ‰§è¡Œå™¨ {executor.executorId}:")
        print(f"  å·²ä½¿ç”¨å†…å­˜: {memory_used / (1024**3):.2f} GB")
        print(f"  æœ€å¤§å†…å­˜: {max_memory / (1024**3):.2f} GB")
        print(f"  ä½¿ç”¨ç‡: {usage_percent:.1f}%")
        print()
    
    overall_usage = (total_used / total_memory) * 100 if total_memory > 0 else 0
    print(f"æ€»ä½“å†…å­˜ä½¿ç”¨ç‡: {overall_usage:.1f}%")
    
    return {
        'total_memory_gb': total_memory / (1024**3),
        'total_used_gb': total_used / (1024**3),
        'usage_percent': overall_usage
    }

# ä½¿ç”¨ç¤ºä¾‹
memory_stats = analyze_memory_usage(spark)
```

### 3.2 ç¼“å­˜ç›‘æ§

```python
def monitor_cache_usage(spark):
    """ç›‘æ§RDDå’ŒDataFrameçš„ç¼“å­˜ä½¿ç”¨æƒ…å†µ"""
    sc = spark.sparkContext
    
    # è·å–å­˜å‚¨çŠ¶æ€
    storage_status = sc.statusTracker().getStorageStatus()
    
    print("=== ç¼“å­˜å­˜å‚¨çŠ¶æ€ ===")
    for storage in storage_status:
        print(f"å—ç®¡ç†å™¨ {storage.blockManagerId}:")
        print(f"  æœ€å¤§å†…å­˜: {storage.maxMem / (1024**2):.2f} MB")
        print(f"  å·²ä½¿ç”¨å†…å­˜: {storage.memUsed / (1024**2):.2f} MB")
        print(f"  å‰©ä½™å†…å­˜: {storage.memRemaining / (1024**2):.2f} MB")
        print(f"  ç£ç›˜ä½¿ç”¨: {storage.diskUsed / (1024**2):.2f} MB")
        print()

# ä½¿ç”¨ç¤ºä¾‹
monitor_cache_usage(spark)
```

## 4. ä½œä¸šå’Œé˜¶æ®µç›‘æ§

### 4.1 ä½œä¸šæ‰§è¡Œç›‘æ§

```python
def monitor_job_execution(spark, job_function, job_name):
    """ç›‘æ§ä½œä¸šæ‰§è¡Œæƒ…å†µ"""
    sc = spark.sparkContext
    
    # è·å–æ‰§è¡Œå‰çš„ä½œä¸šæ•°é‡
    initial_jobs = len(sc.statusTracker().getActiveJobIds())
    
    start_time = time.time()
    
    # æ‰§è¡Œä½œä¸š
    result = job_function()
    
    end_time = time.time()
    execution_time = end_time - start_time
    
    # è·å–ä½œä¸šä¿¡æ¯
    job_ids = sc.statusTracker().getJobIdsForGroup(None)
    
    print(f"=== {job_name} æ‰§è¡ŒæŠ¥å‘Š ===")
    print(f"æ‰§è¡Œæ—¶é—´: {execution_time:.2f} ç§’")
    print(f"ä½œä¸šæ•°é‡: {len(job_ids) - initial_jobs}")
    
    # è·å–æœ€è¿‘ä½œä¸šçš„è¯¦ç»†ä¿¡æ¯
    if job_ids:
        latest_job_id = max(job_ids)
        job_info = sc.statusTracker().getJobInfo(latest_job_id)
        
        if job_info:
            print(f"æœ€æ–°ä½œä¸šID: {latest_job_id}")
            print(f"ä½œä¸šçŠ¶æ€: {job_info.status}")
            print(f"é˜¶æ®µæ•°é‡: {len(job_info.stageIds)}")
    
    print("=" * 40)
    
    return result

# ä½¿ç”¨ç¤ºä¾‹
def sample_job():
    df = spark.range(1000000)
    return df.filter(col("id") % 2 == 0).count()

result = monitor_job_execution(spark, sample_job, "Filter and Count")
```

### 4.2 é˜¶æ®µæ€§èƒ½åˆ†æ

```python
def analyze_stage_performance(spark):
    """åˆ†æé˜¶æ®µæ€§èƒ½"""
    sc = spark.sparkContext
    
    # è·å–æ‰€æœ‰é˜¶æ®µä¿¡æ¯
    stage_infos = sc.statusTracker().getActiveStageInfos()
    
    print("=== æ´»è·ƒé˜¶æ®µæ€§èƒ½åˆ†æ ===")
    for stage in stage_infos:
        print(f"é˜¶æ®µ {stage.stageId} (å°è¯• {stage.attemptId}):")
        print(f"  ä»»åŠ¡æ€»æ•°: {stage.numTasks}")
        print(f"  æ´»è·ƒä»»åŠ¡: {stage.numActiveTasks}")
        print(f"  å®Œæˆä»»åŠ¡: {stage.numCompleteTasks}")
        print(f"  å¤±è´¥ä»»åŠ¡: {stage.numFailedTasks}")
        print(f"  æäº¤æ—¶é—´: {stage.submissionTime}")
        print()

# ä½¿ç”¨ç¤ºä¾‹
analyze_stage_performance(spark)
```

## 5. æ•°æ®å€¾æ–œæ£€æµ‹

### 5.1 åˆ†åŒºå¤§å°åˆ†æ

```python
def analyze_partition_skew(df, sample_fraction=0.1):
    """åˆ†æDataFrameçš„åˆ†åŒºå€¾æ–œæƒ…å†µ"""
    
    # è·å–æ¯ä¸ªåˆ†åŒºçš„è®°å½•æ•°
    def count_partition_records(iterator):
        count = sum(1 for _ in iterator)
        yield count
    
    partition_counts = df.sample(sample_fraction).rdd.mapPartitionsWithIndex(
        lambda idx, iterator: [(idx, sum(1 for _ in iterator))]
    ).collect()
    
    if not partition_counts:
        print("æ— æ³•è·å–åˆ†åŒºä¿¡æ¯")
        return
    
    counts = [count for _, count in partition_counts]
    
    print("=== åˆ†åŒºå€¾æ–œåˆ†æ ===")
    print(f"åˆ†åŒºæ•°é‡: {len(counts)}")
    print(f"æ€»è®°å½•æ•°: {sum(counts)}")
    print(f"å¹³å‡æ¯åˆ†åŒº: {sum(counts) / len(counts):.2f}")
    print(f"æœ€å¤§åˆ†åŒº: {max(counts)}")
    print(f"æœ€å°åˆ†åŒº: {min(counts)}")
    
    # è®¡ç®—å€¾æ–œåº¦
    avg_count = sum(counts) / len(counts)
    max_skew = max(counts) / avg_count if avg_count > 0 else 0
    
    print(f"æœ€å¤§å€¾æ–œæ¯”: {max_skew:.2f}")
    
    if max_skew > 2.0:
        print("âš ï¸  æ£€æµ‹åˆ°ä¸¥é‡æ•°æ®å€¾æ–œï¼")
    elif max_skew > 1.5:
        print("âš ï¸  æ£€æµ‹åˆ°è½»å¾®æ•°æ®å€¾æ–œ")
    else:
        print("âœ… æ•°æ®åˆ†å¸ƒç›¸å¯¹å‡åŒ€")
    
    print("=" * 30)
    
    return {
        'partition_count': len(counts),
        'max_skew_ratio': max_skew,
        'partition_sizes': counts
    }

# ä½¿ç”¨ç¤ºä¾‹
df = spark.range(100000).repartition(10)
skew_analysis = analyze_partition_skew(df)
```

### 5.2 é”®å€¼å€¾æ–œæ£€æµ‹

```python
def detect_key_skew(df, key_column, threshold=0.1):
    """æ£€æµ‹é”®å€¼åˆ†å¸ƒå€¾æ–œ"""
    
    # è®¡ç®—æ¯ä¸ªé”®çš„é¢‘ç‡
    key_counts = df.groupBy(key_column).count().orderBy(desc("count"))
    
    total_records = df.count()
    
    # è·å–å‰10ä¸ªæœ€é¢‘ç¹çš„é”®
    top_keys = key_counts.limit(10).collect()
    
    print(f"=== é”®å€¼å€¾æ–œæ£€æµ‹ ({key_column}) ===")
    print(f"æ€»è®°å½•æ•°: {total_records}")
    print(f"å”¯ä¸€é”®æ•°: {key_counts.count()}")
    print()
    
    print("å‰10ä¸ªæœ€é¢‘ç¹çš„é”®:")
    skewed_keys = []
    
    for row in top_keys:
        key_value = row[key_column]
        count = row['count']
        percentage = (count / total_records) * 100
        
        print(f"  {key_value}: {count} ({percentage:.2f}%)")
        
        if percentage > threshold * 100:
            skewed_keys.append((key_value, count, percentage))
    
    if skewed_keys:
        print(f"\nâš ï¸  æ£€æµ‹åˆ° {len(skewed_keys)} ä¸ªå€¾æ–œé”® (>{threshold*100}%):")
        for key, count, pct in skewed_keys:
            print(f"    {key}: {pct:.2f}%")
    else:
        print("\nâœ… æœªæ£€æµ‹åˆ°æ˜¾è‘—çš„é”®å€¼å€¾æ–œ")
    
    print("=" * 40)
    
    return skewed_keys

# ä½¿ç”¨ç¤ºä¾‹
# åˆ›å»ºå€¾æ–œæ•°æ®
from pyspark.sql.functions import when, rand

skewed_df = spark.range(100000).select(
    when(rand() < 0.7, "common_key")
    .when(rand() < 0.9, "medium_key")
    .otherwise(col("id").cast("string"))
    .alias("key"),
    col("id")
)

skewed_keys = detect_key_skew(skewed_df, "key", threshold=0.05)
```

## 6. æ€§èƒ½ä¼˜åŒ–å»ºè®®

### 6.1 è‡ªåŠ¨ä¼˜åŒ–å»ºè®®

```python
def generate_optimization_suggestions(spark, df=None):
    """ç”Ÿæˆæ€§èƒ½ä¼˜åŒ–å»ºè®®"""
    
    suggestions = []
    
    # æ£€æŸ¥Sparké…ç½®
    conf = spark.conf
    
    # æ£€æŸ¥shuffleåˆ†åŒºæ•°
    shuffle_partitions = int(conf.get("spark.sql.shuffle.partitions", "200"))
    if shuffle_partitions == 200:
        suggestions.append({
            'type': 'configuration',
            'priority': 'medium',
            'suggestion': f'è€ƒè™‘è°ƒæ•´spark.sql.shuffle.partitions (å½“å‰: {shuffle_partitions})',
            'reason': 'é»˜è®¤å€¼å¯èƒ½ä¸é€‚åˆæ‚¨çš„æ•°æ®å¤§å°'
        })
    
    # æ£€æŸ¥å¹¿æ’­é˜ˆå€¼
    broadcast_threshold = conf.get("spark.sql.autoBroadcastJoinThreshold", "10485760")
    suggestions.append({
        'type': 'configuration',
        'priority': 'low',
        'suggestion': f'æ£€æŸ¥å¹¿æ’­è¿æ¥é˜ˆå€¼ (å½“å‰: {int(broadcast_threshold)/(1024*1024):.1f}MB)',
        'reason': 'å¯èƒ½éœ€è¦æ ¹æ®å°è¡¨å¤§å°è°ƒæ•´'
    })
    
    # æ£€æŸ¥è‡ªé€‚åº”æŸ¥è¯¢æ‰§è¡Œ
    adaptive_enabled = conf.get("spark.sql.adaptive.enabled", "false")
    if adaptive_enabled.lower() == "false":
        suggestions.append({
            'type': 'configuration',
            'priority': 'high',
            'suggestion': 'å¯ç”¨è‡ªé€‚åº”æŸ¥è¯¢æ‰§è¡Œ (spark.sql.adaptive.enabled=true)',
            'reason': 'å¯ä»¥è‡ªåŠ¨ä¼˜åŒ–æŸ¥è¯¢æ€§èƒ½'
        })
    
    # å¦‚æœæä¾›äº†DataFrameï¼Œè¿›è¡Œæ•°æ®ç›¸å…³çš„å»ºè®®
    if df is not None:
        partition_count = df.rdd.getNumPartitions()
        
        if partition_count < 2:
            suggestions.append({
                'type': 'data',
                'priority': 'high',
                'suggestion': f'å¢åŠ åˆ†åŒºæ•° (å½“å‰: {partition_count})',
                'reason': 'åˆ†åŒºæ•°è¿‡å°‘å¯èƒ½å½±å“å¹¶è¡Œåº¦'
            })
        elif partition_count > 1000:
            suggestions.append({
                'type': 'data',
                'priority': 'medium',
                'suggestion': f'è€ƒè™‘å‡å°‘åˆ†åŒºæ•° (å½“å‰: {partition_count})',
                'reason': 'åˆ†åŒºæ•°è¿‡å¤šå¯èƒ½å¢åŠ è°ƒåº¦å¼€é”€'
            })
    
    # è¾“å‡ºå»ºè®®
    print("=== æ€§èƒ½ä¼˜åŒ–å»ºè®® ===")
    for i, suggestion in enumerate(suggestions, 1):
        priority_icon = {
            'high': 'ğŸ”´',
            'medium': 'ğŸŸ¡',
            'low': 'ğŸŸ¢'
        }.get(suggestion['priority'], 'âšª')
        
        print(f"{i}. {priority_icon} [{suggestion['type'].upper()}] {suggestion['suggestion']}")
        print(f"   åŸå› : {suggestion['reason']}")
        print()
    
    return suggestions

# ä½¿ç”¨ç¤ºä¾‹
suggestions = generate_optimization_suggestions(spark, df)
```

### 6.2 æ€§èƒ½åŸºå‡†æµ‹è¯•

```python
def benchmark_operations(spark, data_size=1000000):
    """å¯¹å¸¸è§æ“ä½œè¿›è¡ŒåŸºå‡†æµ‹è¯•"""
    
    print(f"=== æ€§èƒ½åŸºå‡†æµ‹è¯• (æ•°æ®å¤§å°: {data_size:,}) ===")
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    df = spark.range(data_size).toDF("id")
    df = df.withColumn("value", col("id") * 2)
    df = df.withColumn("category", col("id") % 10)
    
    benchmarks = {}
    
    # æµ‹è¯•1: ç®€å•è¿‡æ»¤
    start_time = time.time()
    filtered_count = df.filter(col("id") > data_size // 2).count()
    filter_time = time.time() - start_time
    benchmarks['filter'] = filter_time
    print(f"è¿‡æ»¤æ“ä½œ: {filter_time:.2f}s (ç»“æœ: {filtered_count:,})")
    
    # æµ‹è¯•2: åˆ†ç»„èšåˆ
    start_time = time.time()
    grouped_count = df.groupBy("category").count().count()
    group_time = time.time() - start_time
    benchmarks['groupby'] = group_time
    print(f"åˆ†ç»„èšåˆ: {group_time:.2f}s (ç»“æœ: {grouped_count})")
    
    # æµ‹è¯•3: æ’åº
    start_time = time.time()
    sorted_count = df.orderBy("value").limit(1000).count()
    sort_time = time.time() - start_time
    benchmarks['sort'] = sort_time
    print(f"æ’åºæ“ä½œ: {sort_time:.2f}s (ç»“æœ: {sorted_count})")
    
    # æµ‹è¯•4: è¿æ¥æ“ä½œ
    df2 = spark.range(data_size // 10).toDF("id").withColumn("info", lit("test"))
    start_time = time.time()
    joined_count = df.join(df2, "id").count()
    join_time = time.time() - start_time
    benchmarks['join'] = join_time
    print(f"è¿æ¥æ“ä½œ: {join_time:.2f}s (ç»“æœ: {joined_count:,})")
    
    print("=" * 50)
    
    return benchmarks

# ä½¿ç”¨ç¤ºä¾‹
benchmark_results = benchmark_operations(spark, 100000)
```

## 7. æ€»ç»“

æœ¬æ–‡æ¡£æä¾›äº†Sparkæ€§èƒ½ç›‘æ§å’Œè°ƒä¼˜çš„å…¨é¢æŒ‡å—ï¼ŒåŒ…æ‹¬ï¼š

1. **Spark UIä½¿ç”¨**ï¼šäº†è§£å„ä¸ªé¡µé¢çš„åŠŸèƒ½å’Œç”¨é€”
2. **æ€§èƒ½æŒ‡æ ‡ç›‘æ§**ï¼šæ”¶é›†å’Œåˆ†æå…³é”®æ€§èƒ½æŒ‡æ ‡
3. **å†…å­˜ç›‘æ§**ï¼šåˆ†æå†…å­˜ä½¿ç”¨æƒ…å†µå’Œç¼“å­˜æ•ˆæœ
4. **ä½œä¸šç›‘æ§**ï¼šè·Ÿè¸ªä½œä¸šå’Œé˜¶æ®µçš„æ‰§è¡Œæƒ…å†µ
5. **æ•°æ®å€¾æ–œæ£€æµ‹**ï¼šè¯†åˆ«å’Œåˆ†ææ•°æ®åˆ†å¸ƒé—®é¢˜
6. **ä¼˜åŒ–å»ºè®®**ï¼šè‡ªåŠ¨ç”Ÿæˆæ€§èƒ½ä¼˜åŒ–å»ºè®®
7. **åŸºå‡†æµ‹è¯•**ï¼šè¯„ä¼°ä¸åŒæ“ä½œçš„æ€§èƒ½è¡¨ç°

é€šè¿‡ç³»ç»Ÿæ€§åœ°åº”ç”¨è¿™äº›ç›‘æ§å’Œè°ƒä¼˜æŠ€æœ¯ï¼Œæ‚¨å¯ä»¥æ˜¾è‘—æé«˜Sparkåº”ç”¨ç¨‹åºçš„æ€§èƒ½å’Œç¨³å®šæ€§ã€‚

## æœ€ä½³å®è·µ

1. **å®šæœŸç›‘æ§**ï¼šå»ºç«‹å®šæœŸçš„æ€§èƒ½ç›‘æ§æœºåˆ¶
2. **åŸºçº¿å»ºç«‹**ï¼šä¸ºå…³é”®æ“ä½œå»ºç«‹æ€§èƒ½åŸºçº¿
3. **æ¸è¿›ä¼˜åŒ–**ï¼šé€æ­¥åº”ç”¨ä¼˜åŒ–æªæ–½ï¼Œé¿å…ä¸€æ¬¡æ€§å¤§å¹…ä¿®æ”¹
4. **æµ‹è¯•éªŒè¯**ï¼šåœ¨ç”Ÿäº§ç¯å¢ƒåº”ç”¨å‰å……åˆ†æµ‹è¯•ä¼˜åŒ–æ•ˆæœ
5. **æ–‡æ¡£è®°å½•**ï¼šè®°å½•ä¼˜åŒ–è¿‡ç¨‹å’Œæ•ˆæœï¼Œä¾¿äºåç»­å‚è€ƒ