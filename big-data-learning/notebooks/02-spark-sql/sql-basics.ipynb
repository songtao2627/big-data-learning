{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark SQL基础\n",
    "\n",
    "本笔记本介绍Spark SQL，这是Spark用于处理结构化数据的模块。Spark SQL允许您使用SQL语句查询数据，同时还提供了与其他Spark组件的无缝集成。\n",
    "\n",
    "## 什么是Spark SQL？\n",
    "\n",
    "Spark SQL是Spark的一个模块，用于处理结构化数据。它提供了一个编程抽象，称为DataFrame，并且可以作为分布式SQL查询引擎。\n",
    "\n",
    "Spark SQL的主要特点：\n",
    "- **集成**：可以在SQL和编程API之间无缝切换\n",
    "- **统一数据访问**：可以连接到各种数据源\n",
    "- **兼容Hive**：可以运行未修改的Hive查询\n",
    "- **标准连接**：支持JDBC/ODBC连接"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 创建SparkSession\n",
    "\n",
    "SparkSession是Spark SQL的入口点，它允许您创建DataFrame和执行SQL查询。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# 创建SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Spark SQL基础\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# 显示Spark版本\n",
    "print(f\"Spark版本: {spark.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 加载数据\n",
    "\n",
    "Spark SQL可以从各种数据源加载数据，包括CSV、JSON、Parquet等。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载CSV数据\n",
    "sales_df = spark.read.option(\"header\", \"true\") \\\n",
    "                    .option(\"inferSchema\", \"true\") \\\n",
    "                    .csv(\"/home/jovyan/data/sample/sales_data.csv\")\n",
    "\n",
    "# 显示数据\n",
    "print(\"销售数据:\")\n",
    "sales_df.show()\n",
    "\n",
    "# 显示Schema\n",
    "print(\"\\n销售数据Schema:\")\n",
    "sales_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载JSON数据\n",
    "user_df = spark.read.json(\"/home/jovyan/data/sample/user_behavior.json\")\n",
    "\n",
    "print(\"用户行为数据:\")\n",
    "user_df.show()\n",
    "\n",
    "# 显示Schema\n",
    "print(\"\\n用户行为数据Schema:\")\n",
    "user_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 创建临时视图\n",
    "\n",
    "要使用SQL查询DataFrame，首先需要将其注册为临时视图。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 注册临时视图\n",
    "sales_df.createOrReplaceTempView(\"sales\")\n",
    "user_df.createOrReplaceTempView(\"user_behavior\")\n",
    "\n",
    "# 创建全局临时视图（跨SparkSession可用）\n",
    "sales_df.createGlobalTempView(\"global_sales\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 基本SQL查询\n",
    "\n",
    "现在，我们可以使用SQL语句查询数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 简单的SELECT查询\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT * FROM sales LIMIT 5\n",
    "\"\"\")\n",
    "\n",
    "print(\"简单查询结果:\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 选择特定列\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT product_id, category, price FROM sales\n",
    "\"\"\")\n",
    "\n",
    "print(\"选择特定列:\")\n",
    "result.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用WHERE子句过滤数据\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT * FROM sales\n",
    "    WHERE price > 100\n",
    "\"\"\")\n",
    "\n",
    "print(\"价格大于100的商品:\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 聚合和分组\n",
    "\n",
    "SQL的强大之处在于其聚合和分组功能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 按类别分组并计算总销售额\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT category, SUM(price * quantity) AS total_sales\n",
    "    FROM sales\n",
    "    GROUP BY category\n",
    "    ORDER BY total_sales DESC\n",
    "\"\"\")\n",
    "\n",
    "print(\"按类别统计销售额:\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算多个聚合\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        category,\n",
    "        COUNT(*) AS count,\n",
    "        SUM(price) AS total_price,\n",
    "        AVG(price) AS avg_price,\n",
    "        MAX(price) AS max_price,\n",
    "        MIN(price) AS min_price\n",
    "    FROM sales\n",
    "    GROUP BY category\n",
    "\"\"\")\n",
    "\n",
    "print(\"按类别的详细统计:\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用HAVING子句过滤分组结果\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT category, COUNT(*) AS count\n",
    "    FROM sales\n",
    "    GROUP BY category\n",
    "    HAVING COUNT(*) > 2\n",
    "\"\"\")\n",
    "\n",
    "print(\"商品数量大于2的类别:\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 连接操作\n",
    "\n",
    "SQL连接操作允许您组合多个表的数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建客户数据\n",
    "customers_data = [\n",
    "    (\"C1001\", \"张三\", \"北京\"),\n",
    "    (\"C1002\", \"李四\", \"上海\"),\n",
    "    (\"C1003\", \"王五\", \"广州\"),\n",
    "    (\"C1004\", \"赵六\", \"深圳\"),\n",
    "    (\"C1005\", \"钱七\", \"杭州\"),\n",
    "    (\"C1006\", \"孙八\", \"成都\"),\n",
    "    (\"C1007\", \"周九\", \"武汉\")\n",
    "]\n",
    "customers_df = spark.createDataFrame(customers_data, [\"customer_id\", \"name\", \"city\"])\n",
    "customers_df.createOrReplaceTempView(\"customers\")\n",
    "\n",
    "# 显示客户数据\n",
    "print(\"客户数据:\")\n",
    "customers_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 内连接\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT s.date, c.name, s.product_id, s.price, s.quantity\n",
    "    FROM sales s\n",
    "    JOIN customers c ON s.customer_id = c.customer_id\n",
    "\"\"\")\n",
    "\n",
    "print(\"销售和客户数据内连接:\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 左外连接\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT c.name, c.city, s.product_id, s.price\n",
    "    FROM customers c\n",
    "    LEFT OUTER JOIN sales s ON c.customer_id = s.customer_id\n",
    "\"\"\")\n",
    "\n",
    "print(\"左外连接结果:\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 右外连接\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT c.name, c.city, s.product_id, s.price\n",
    "    FROM customers c\n",
    "    RIGHT OUTER JOIN sales s ON c.customer_id = s.customer_id\n",
    "\"\"\")\n",
    "\n",
    "print(\"右外连接结果:\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 全外连接\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT c.name, c.city, s.product_id, s.price\n",
    "    FROM customers c\n",
    "    FULL OUTER JOIN sales s ON c.customer_id = s.customer_id\n",
    "\"\"\")\n",
    "\n",
    "print(\"全外连接结果:\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 子查询和公共表表达式(CTE)\n",
    "\n",
    "子查询和CTE可以帮助您构建复杂的查询。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用子查询\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT * FROM (\n",
    "        SELECT category, SUM(price * quantity) AS total_sales\n",
    "        FROM sales\n",
    "        GROUP BY category\n",
    "    ) AS category_sales\n",
    "    WHERE total_sales > 1000\n",
    "\"\"\")\n",
    "\n",
    "print(\"使用子查询:\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用公共表表达式(CTE)\n",
    "result = spark.sql(\"\"\"\n",
    "    WITH category_sales AS (\n",
    "        SELECT category, SUM(price * quantity) AS total_sales\n",
    "        FROM sales\n",
    "        GROUP BY category\n",
    "    ),\n",
    "    region_sales AS (\n",
    "        SELECT region, SUM(price * quantity) AS total_sales\n",
    "        FROM sales\n",
    "        GROUP BY region\n",
    "    )\n",
    "    SELECT c.category, c.total_sales AS category_sales, r.region, r.total_sales AS region_sales\n",
    "    FROM category_sales c\n",
    "    CROSS JOIN region_sales r\n",
    "    ORDER BY c.total_sales DESC, r.total_sales DESC\n",
    "\"\"\")\n",
    "\n",
    "print(\"使用公共表表达式:\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 窗口函数\n",
    "\n",
    "窗口函数允许您在分组数据上执行计算，而不会将结果合并为单个输出行。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用窗口函数计算每个类别中的排名\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        category,\n",
    "        product_id,\n",
    "        price,\n",
    "        RANK() OVER (PARTITION BY category ORDER BY price DESC) AS price_rank\n",
    "    FROM sales\n",
    "\"\"\")\n",
    "\n",
    "print(\"使用窗口函数计算排名:\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算累计总和\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        date,\n",
    "        product_id,\n",
    "        price,\n",
    "        SUM(price) OVER (ORDER BY date) AS cumulative_price\n",
    "    FROM sales\n",
    "\"\"\")\n",
    "\n",
    "print(\"计算累计总和:\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算移动平均\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        date,\n",
    "        product_id,\n",
    "        price,\n",
    "        AVG(price) OVER (\n",
    "            ORDER BY date \n",
    "            ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING\n",
    "        ) AS moving_avg_price\n",
    "    FROM sales\n",
    "\"\"\")\n",
    "\n",
    "print(\"计算移动平均:\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 用户自定义函数(UDF)\n",
    "\n",
    "UDF允许您在SQL查询中使用自定义函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义UDF\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# 定义一个函数，将价格分类为\"低\"、\"中\"或\"高\"\n",
    "def price_category(price):\n",
    "    if price < 50:\n",
    "        return \"低\"\n",
    "    elif price < 200:\n",
    "        return \"中\"\n",
    "    else:\n",
    "        return \"高\"\n",
    "\n",
    "# 注册UDF\n",
    "spark.udf.register(\"price_category\", price_category, StringType())\n",
    "\n",
    "# 在SQL查询中使用UDF\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        product_id,\n",
    "        price,\n",
    "        price_category(price) AS price_category\n",
    "    FROM sales\n",
    "    ORDER BY price\n",
    "\"\"\")\n",
    "\n",
    "print(\"使用UDF分类价格:\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. 保存查询结果\n",
    "\n",
    "您可以将SQL查询结果保存为各种格式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 执行查询\n",
    "category_sales = spark.sql(\"\"\"\n",
    "    SELECT category, SUM(price * quantity) AS total_sales\n",
    "    FROM sales\n",
    "    GROUP BY category\n",
    "    ORDER BY total_sales DESC\n",
    "\"\"\")\n",
    "\n",
    "# 保存为CSV\n",
    "category_sales.write.mode(\"overwrite\").option(\"header\", \"true\").csv(\"/home/jovyan/data/output/category_sales_csv\")\n",
    "\n",
    "# 保存为Parquet\n",
    "category_sales.write.mode(\"overwrite\").parquet(\"/home/jovyan/data/output/category_sales_parquet\")\n",
    "\n",
    "# 保存为JSON\n",
    "category_sales.write.mode(\"overwrite\").json(\"/home/jovyan/data/output/category_sales_json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. 实际案例：销售数据分析\n",
    "\n",
    "让我们使用SQL进行一些更复杂的销售数据分析。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 按地区和类别的销售额分析\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        region,\n",
    "        category,\n",
    "        SUM(price * quantity) AS total_sales,\n",
    "        COUNT(DISTINCT customer_id) AS customer_count,\n",
    "        SUM(price * quantity) / COUNT(DISTINCT customer_id) AS sales_per_customer\n",
    "    FROM sales\n",
    "    GROUP BY region, category\n",
    "    ORDER BY region, total_sales DESC\n",
    "\"\"\")\n",
    "\n",
    "print(\"按地区和类别的销售分析:\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 找出每个类别中销售额最高的产品\n",
    "result = spark.sql(\"\"\"\n",
    "    WITH product_sales AS (\n",
    "        SELECT \n",
    "            category,\n",
    "            product_id,\n",
    "            SUM(price * quantity) AS total_sales,\n",
    "            RANK() OVER (PARTITION BY category ORDER BY SUM(price * quantity) DESC) AS sales_rank\n",
    "        FROM sales\n",
    "        GROUP BY category, product_id\n",
    "    )\n",
    "    SELECT \n",
    "        category,\n",
    "        product_id,\n",
    "        total_sales\n",
    "    FROM product_sales\n",
    "    WHERE sales_rank = 1\n",
    "    ORDER BY total_sales DESC\n",
    "\"\"\")\n",
    "\n",
    "print(\"每个类别中销售额最高的产品:\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 客户购买行为分析\n",
    "result = spark.sql(\"\"\"\n",
    "    WITH customer_purchases AS (\n",
    "        SELECT \n",
    "            c.name,\n",
    "            c.city,\n",
    "            COUNT(*) AS purchase_count,\n",
    "            SUM(s.price * s.quantity) AS total_spent,\n",
    "            AVG(s.price) AS avg_price,\n",
    "            MAX(s.price) AS max_price\n",
    "        FROM sales s\n",
    "        JOIN customers c ON s.customer_id = c.customer_id\n",
    "        GROUP BY c.name, c.city\n",
    "    )\n",
    "    SELECT \n",
    "        name,\n",
    "        city,\n",
    "        purchase_count,\n",
    "        total_spent,\n",
    "        avg_price,\n",
    "        max_price,\n",
    "        CASE \n",
    "            WHEN total_spent > 1000 THEN '高价值'\n",
    "            WHEN total_spent > 500 THEN '中价值'\n",
    "            ELSE '低价值'\n",
    "        END AS customer_value\n",
    "    FROM customer_purchases\n",
    "    ORDER BY total_spent DESC\n",
    "\"\"\")\n",
    "\n",
    "print(\"客户购买行为分析:\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. 练习\n",
    "\n",
    "现在，让我们通过一些练习来巩固所学知识。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习1：用户行为分析\n",
    "\n",
    "使用用户行为数据，编写SQL查询完成以下任务：\n",
    "1. 统计每种行为（view、add_to_cart、purchase）的次数\n",
    "2. 找出浏览次数最多的商品\n",
    "3. 计算每个用户的购买转化率（购买次数/浏览次数）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 统计每种行为的次数\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT action, COUNT(*) AS count\n",
    "    FROM user_behavior\n",
    "    GROUP BY action\n",
    "    ORDER BY count DESC\n",
    "\"\"\")\n",
    "\n",
    "print(\"各类行为次数:\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 找出浏览次数最多的商品\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT item_id, COUNT(*) AS view_count\n",
    "    FROM user_behavior\n",
    "    WHERE action = 'view'\n",
    "    GROUP BY item_id\n",
    "    ORDER BY view_count DESC\n",
    "    LIMIT 5\n",
    "\"\"\")\n",
    "\n",
    "print(\"浏览次数最多的商品:\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 计算每个用户的购买转化率\n",
    "result = spark.sql(\"\"\"\n",
    "    WITH user_actions AS (\n",
    "        SELECT \n",
    "            user_id,\n",
    "            SUM(CASE WHEN action = 'view' THEN 1 ELSE 0 END) AS view_count,\n",
    "            SUM(CASE WHEN action = 'purchase' THEN 1 ELSE 0 END) AS purchase_count\n",
    "        FROM user_behavior\n",
    "        GROUP BY user_id\n",
    "    )\n",
    "    SELECT \n",
    "        user_id,\n",
    "        view_count,\n",
    "        purchase_count,\n",
    "        CASE \n",
    "            WHEN view_count > 0 THEN ROUND(purchase_count / view_count, 2)\n",
    "            ELSE 0\n",
    "        END AS conversion_rate\n",
    "    FROM user_actions\n",
    "    WHERE view_count > 0\n",
    "    ORDER BY conversion_rate DESC\n",
    "\"\"\")\n",
    "\n",
    "print(\"用户购买转化率:\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习2：销售数据高级分析\n",
    "\n",
    "使用销售数据，编写SQL查询完成以下任务：\n",
    "1. 按月份统计销售趋势\n",
    "2. 计算每个类别的销售占比\n",
    "3. 找出购买频率最高的客户"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 按月份统计销售趋势\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        SUBSTRING(date, 1, 7) AS month,\n",
    "        SUM(price * quantity) AS total_sales\n",
    "    FROM sales\n",
    "    GROUP BY SUBSTRING(date, 1, 7)\n",
    "    ORDER BY month\n",
    "\"\"\")\n",
    "\n",
    "print(\"月度销售趋势:\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 计算每个类别的销售占比\n",
    "result = spark.sql(\"\"\"\n",
    "    WITH category_sales AS (\n",
    "        SELECT \n",
    "            category,\n",
    "            SUM(price * quantity) AS category_sales\n",
    "        FROM sales\n",
    "        GROUP BY category\n",
    "    ),\n",
    "    total AS (\n",
    "        SELECT SUM(price * quantity) AS total_sales\n",
    "        FROM sales\n",
    "    )\n",
    "    SELECT \n",
    "        c.category,\n",
    "        c.category_sales,\n",
    "        t.total_sales,\n",
    "        ROUND(c.category_sales / t.total_sales * 100, 2) AS sales_percentage\n",
    "    FROM category_sales c, total t\n",
    "    ORDER BY sales_percentage DESC\n",
    "\"\"\")\n",
    "\n",
    "print(\"类别销售占比:\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 找出购买频率最高的客户\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        s.customer_id,\n",
    "        c.name,\n",
    "        COUNT(*) AS purchase_count,\n",
    "        SUM(s.price * s.quantity) AS total_spent\n",
    "    FROM sales s\n",
    "    JOIN customers c ON s.customer_id = c.customer_id\n",
    "    GROUP BY s.customer_id, c.name\n",
    "    ORDER BY purchase_count DESC\n",
    "    LIMIT 5\n",
    "\"\"\")\n",
    "\n",
    "print(\"客户购买频率排行:\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. 总结\n",
    "\n",
    "在本笔记本中，我们学习了：\n",
    "\n",
    "1. Spark SQL的基本概念和功能\n",
    "2. 如何加载数据并创建临时视图\n",
    "3. 基本SQL查询（SELECT、WHERE、GROUP BY等）\n",
    "4. 高级SQL功能（连接、子查询、CTE、窗口函数）\n",
    "5. 如何使用用户自定义函数(UDF)\n",
    "6. 如何保存查询结果\n",
    "7. 如何应用SQL解决实际问题\n",
    "\n",
    "Spark SQL是一个强大的工具，它结合了SQL的易用性和Spark的分布式计算能力，使得大规模数据分析变得简单高效。\n",
    "\n",
    "## 下一步\n",
    "\n",
    "接下来，我们将学习更高级的SQL查询技术和优化策略。请继续学习 `advanced-queries.ipynb` 笔记本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 停止SparkSession\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}