# ğŸ”§ Spark ç¯å¢ƒæ‰‹åŠ¨é…ç½®å’Œè°ƒè¯•æ•™ç¨‹

## å­¦ä¹ ç›®æ ‡
é€šè¿‡æ‰‹åŠ¨é…ç½® PySpark ç¯å¢ƒï¼Œæ·±å…¥ç†è§£ï¼š
- ç‰ˆæœ¬å…¼å®¹æ€§é—®é¢˜
- ç¯å¢ƒå˜é‡é…ç½®
- ä¾èµ–ç®¡ç†
- åˆ†å¸ƒå¼ç³»ç»Ÿè°ƒè¯•æ–¹æ³•

## å‰ç½®çŸ¥è¯†å›é¡¾

### æˆ‘ä»¬å‘ç°çš„é—®é¢˜
1. **è¡¨é¢ç°è±¡**ï¼š`df.show()` å¡ä½
2. **æ ¹æœ¬åŸå› **ï¼šJupyter å®¹å™¨ä¸­çš„ Spark ç‰ˆæœ¬ï¼ˆ3.5.0ï¼‰ä¸é›†ç¾¤ç‰ˆæœ¬ï¼ˆ3.4.xï¼‰ä¸åŒ¹é…
3. **è§£å†³æ–¹æ¡ˆ**ï¼šå®‰è£…åŒ¹é…çš„ PySpark ç‰ˆæœ¬

## ğŸš€ æ‰‹åŠ¨é…ç½®æ­¥éª¤

### æ­¥éª¤1ï¼šè¿›å…¥ Jupyter å®¹å™¨

é¦–å…ˆå¯åŠ¨ Jupyter å®¹å™¨ï¼ˆä¸å¸¦è‡ªåŠ¨å®‰è£…ï¼‰ï¼š

```bash
# åœ¨å®¿ä¸»æœºæ‰§è¡Œ
docker-compose up -d jupyter
```

ç­‰å¾…å®¹å™¨å¯åŠ¨åï¼Œè¿›å…¥å®¹å™¨ï¼š

```bash
# è¿›å…¥å®¹å™¨çš„ bash ç¯å¢ƒ
docker-compose exec jupyter bash
```

### æ­¥éª¤2ï¼šæ£€æŸ¥å½“å‰ç¯å¢ƒ

åœ¨å®¹å™¨å†…æ‰§è¡Œä»¥ä¸‹å‘½ä»¤ï¼Œäº†è§£å½“å‰çŠ¶æ€ï¼š

```bash
# æ£€æŸ¥ Python ç‰ˆæœ¬
python --version

# æ£€æŸ¥å½“å‰å®‰è£…çš„åŒ…
pip list | grep -i spark

# æ£€æŸ¥ç¯å¢ƒå˜é‡
echo "SPARK_HOME: $SPARK_HOME"
echo "PYTHONPATH: $PYTHONPATH"

# æ£€æŸ¥ Spark å®‰è£…ä½ç½®
find /usr/local -name "spark*" -type d 2>/dev/null
```

### æ­¥éª¤3ï¼šå¸è½½ç°æœ‰çš„ PySparkï¼ˆå¦‚æœå­˜åœ¨ï¼‰

```bash
# å¸è½½å¯èƒ½å­˜åœ¨çš„ PySpark
pip uninstall pyspark -y

# éªŒè¯å¸è½½
python -c "import pyspark" 2>&1 || echo "PySpark å·²æˆåŠŸå¸è½½"
```

### æ­¥éª¤4ï¼šå®‰è£…åŒ¹é…ç‰ˆæœ¬çš„ PySpark

```bash
# å®‰è£…ä¸é›†ç¾¤åŒ¹é…çš„ PySpark ç‰ˆæœ¬
pip install pyspark==3.4.3

# éªŒè¯å®‰è£…
python -c "import pyspark; print('PySpark version:', pyspark.__version__)"
```

### æ­¥éª¤5ï¼šå®‰è£…è¾…åŠ©å·¥å…·

```bash
# å®‰è£…æœ‰ç”¨çš„è¾…åŠ©åº“
pip install findspark pyarrow

# findspark å¯ä»¥å¸®åŠ©è‡ªåŠ¨æ‰¾åˆ° Spark å®‰è£…ä½ç½®
```

### æ­¥éª¤6ï¼šé…ç½®ç¯å¢ƒå˜é‡

```bash
# è®¾ç½®ç¯å¢ƒå˜é‡ï¼ˆä¸´æ—¶ï¼‰
export SPARK_HOME=/usr/local/spark
export PYTHONPATH=$SPARK_HOME/python:$PYTHONPATH

# éªŒè¯é…ç½®
echo "SPARK_HOME: $SPARK_HOME"
echo "PYTHONPATH: $PYTHONPATH"
```

### æ­¥éª¤7ï¼šæµ‹è¯•æœ¬åœ°æ¨¡å¼

åˆ›å»ºä¸€ä¸ªæµ‹è¯•è„šæœ¬ï¼š

```bash
# åˆ›å»ºæµ‹è¯•æ–‡ä»¶
cat > test_local_spark.py << 'EOF'
import sys
print("Python path:", sys.path)

try:
    from pyspark.sql import SparkSession
    print("âœ… PySpark å¯¼å…¥æˆåŠŸ")
    
    # æµ‹è¯•æœ¬åœ°æ¨¡å¼
    spark = SparkSession.builder \
        .appName("LocalTest") \
        .master("local[2]") \
        .getOrCreate()
    
    print(f"âœ… Spark ç‰ˆæœ¬: {spark.version}")
    
    # ç®€å•æµ‹è¯•
    df = spark.createDataFrame([(1, "test")], ["id", "name"])
    df.show()
    
    spark.stop()
    print("âœ… æœ¬åœ°æ¨¡å¼æµ‹è¯•æˆåŠŸ")
    
except Exception as e:
    print(f"âŒ é”™è¯¯: {e}")
EOF

# è¿è¡Œæµ‹è¯•
python test_local_spark.py
```

### æ­¥éª¤8ï¼šæµ‹è¯•é›†ç¾¤è¿æ¥

```bash
# åˆ›å»ºé›†ç¾¤è¿æ¥æµ‹è¯•
cat > test_cluster_spark.py << 'EOF'
from pyspark.sql import SparkSession
import time

try:
    print("è¿æ¥åˆ° Spark é›†ç¾¤...")
    spark = SparkSession.builder \
        .appName("ClusterTest") \
        .master("spark://spark-master:7077") \
        .config("spark.driver.memory", "512m") \
        .config("spark.executor.memory", "1g") \
        .config("spark.executor.cores", "1") \
        .getOrCreate()
    
    print(f"âœ… é›†ç¾¤è¿æ¥æˆåŠŸï¼ŒSpark ç‰ˆæœ¬: {spark.version}")
    print(f"âœ… åº”ç”¨ ID: {spark.sparkContext.applicationId}")
    
    # æµ‹è¯•ç®€å•æ“ä½œ
    rdd = spark.sparkContext.parallelize([1, 2, 3, 4, 5])
    result = rdd.sum()
    print(f"âœ… RDD æµ‹è¯•æˆåŠŸï¼Œæ±‚å’Œç»“æœ: {result}")
    
    # æµ‹è¯• DataFrame
    df = spark.createDataFrame([(1, "å¼ ä¸‰"), (2, "æå››")], ["id", "name"])
    print("âœ… DataFrame åˆ›å»ºæˆåŠŸ")
    
    # è¿™é‡Œæ˜¯ä¹‹å‰å¡ä½çš„åœ°æ–¹
    print("æ‰§è¡Œ df.show()...")
    df.show()
    print("âœ… df.show() æ‰§è¡ŒæˆåŠŸï¼")
    
    spark.stop()
    print("ğŸ‰ é›†ç¾¤æµ‹è¯•å®Œå…¨æˆåŠŸï¼")
    
except Exception as e:
    print(f"âŒ é›†ç¾¤æµ‹è¯•å¤±è´¥: {e}")
    import traceback
    traceback.print_exc()
EOF

# è¿è¡Œé›†ç¾¤æµ‹è¯•
python test_cluster_spark.py
```

## ğŸ” è°ƒè¯•æŠ€å·§

### å¦‚æœé‡åˆ°é—®é¢˜ï¼ŒæŒ‰ä»¥ä¸‹é¡ºåºæ£€æŸ¥ï¼š

#### 1. ç‰ˆæœ¬æ£€æŸ¥
```bash
# æ£€æŸ¥ PySpark ç‰ˆæœ¬
python -c "import pyspark; print(pyspark.__version__)"

# æ£€æŸ¥é›†ç¾¤ç‰ˆæœ¬ï¼ˆåœ¨å®¿ä¸»æœºæ‰§è¡Œï¼‰
docker-compose logs spark-master | grep -i version
```

#### 2. ç½‘ç»œè¿æ¥æ£€æŸ¥
```bash
# åœ¨å®¹å™¨å†…æµ‹è¯•ç½‘ç»œè¿æ¥
ping spark-master
telnet spark-master 7077
```

#### 3. èµ„æºæ£€æŸ¥
```bash
# æ£€æŸ¥å®¹å™¨èµ„æº
docker stats --no-stream
```

#### 4. æ—¥å¿—æ£€æŸ¥
```bash
# æŸ¥çœ‹ Spark Master æ—¥å¿—
docker-compose logs spark-master --tail=20

# æŸ¥çœ‹ Worker æ—¥å¿—
docker-compose logs spark-worker-1 --tail=20
```

## ğŸ“ å­¦ä¹ è¦ç‚¹æ€»ç»“

### ç‰ˆæœ¬å…¼å®¹æ€§
- **å®¢æˆ·ç«¯ç‰ˆæœ¬** = **æœåŠ¡ç«¯ç‰ˆæœ¬** æ˜¯åˆ†å¸ƒå¼ç³»ç»Ÿçš„åŸºæœ¬è¦æ±‚
- ç‰ˆæœ¬ä¸åŒ¹é…ä¼šåœ¨åºåˆ—åŒ–/ååºåˆ—åŒ–æ—¶æš´éœ²é—®é¢˜
- ä½¿ç”¨ `pip install pyspark==å…·ä½“ç‰ˆæœ¬` ç¡®ä¿ç‰ˆæœ¬ä¸€è‡´

### ç¯å¢ƒå˜é‡é‡è¦æ€§
- `SPARK_HOME`: Spark å®‰è£…ç›®å½•
- `PYTHONPATH`: Python æ¨¡å—æœç´¢è·¯å¾„
- ç¯å¢ƒå˜é‡å¿…é¡»åœ¨ Python è¿›ç¨‹å¯åŠ¨å‰è®¾ç½®

### è°ƒè¯•æ–¹æ³•è®º
1. **åˆ†å±‚è°ƒè¯•**: ä»ç®€å•åˆ°å¤æ‚ï¼ˆæœ¬åœ°æ¨¡å¼ â†’ é›†ç¾¤æ¨¡å¼ï¼‰
2. **é€æ­¥éªŒè¯**: å¯¼å…¥ â†’ è¿æ¥ â†’ ç®€å•æ“ä½œ â†’ å¤æ‚æ“ä½œ
3. **æ—¥å¿—åˆ†æ**: æŸ¥çœ‹å„ç»„ä»¶æ—¥å¿—æ‰¾åˆ°æ ¹æœ¬åŸå› 

### Spark æ‰§è¡Œæ¨¡å‹
- **Transformation**: æ‡’æ‰§è¡Œï¼Œä¸ä¼šç«‹å³æš´éœ²é—®é¢˜
- **Action**: è§¦å‘å®é™…è®¡ç®—ï¼Œé—®é¢˜åœ¨è¿™é‡Œæš´éœ²
- ç†è§£è¿™ä¸ªåŒºåˆ«æœ‰åŠ©äºå®šä½é—®é¢˜

## ğŸš€ ä¸‹ä¸€æ­¥

å®Œæˆæ‰‹åŠ¨é…ç½®åï¼Œä½ å¯ä»¥ï¼š
1. åœ¨ Jupyter Notebook ä¸­åˆ›å»ºæ–°çš„ notebook
2. ä½¿ç”¨é…ç½®å¥½çš„ç¯å¢ƒå­¦ä¹  Spark åŸºç¡€æ¦‚å¿µ
3. é€æ­¥å­¦ä¹ æ›´å¤æ‚çš„ Spark åŠŸèƒ½

## ğŸ’¡ ä¸“ä¸šæç¤º

- æ¯æ¬¡é‡å¯å®¹å™¨åéœ€è¦é‡æ–°è®¾ç½®ç¯å¢ƒå˜é‡
- å¯ä»¥å°†ç¯å¢ƒå˜é‡è®¾ç½®å†™å…¥ `~/.bashrc` ä½¿å…¶æŒä¹…åŒ–
- ä½¿ç”¨ `findspark.init()` å¯ä»¥è‡ªåŠ¨é…ç½® PySpark è·¯å¾„
- åœ¨ç”Ÿäº§ç¯å¢ƒä¸­ï¼Œç‰ˆæœ¬ç®¡ç†æ›´åŠ é‡è¦