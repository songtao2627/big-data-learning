#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¿«é€Ÿ Spark è¿æ¥æµ‹è¯• - å…¼å®¹ç‰ˆæœ¬å·®å¼‚
è¿™ä¸ªè„šæœ¬å±•ç¤ºäº†å¦‚ä½•å¤„ç†ç‰ˆæœ¬å…¼å®¹æ€§é—®é¢˜
"""

import sys
import os

def setup_pyspark_path():
    """è®¾ç½® PySpark è·¯å¾„"""
    # æ–¹æ³•1: ä½¿ç”¨ findsparkï¼ˆå¦‚æœå¯ç”¨ï¼‰
    try:
        import findspark
        findspark.init()
        print("âœ… ä½¿ç”¨ findspark åˆå§‹åŒ– PySpark")
        return True
    except ImportError:
        pass
    
    # æ–¹æ³•2: æ‰‹åŠ¨è®¾ç½®è·¯å¾„
    spark_home = "/usr/local/spark"
    if os.path.exists(spark_home):
        sys.path.insert(0, os.path.join(spark_home, "python"))
        sys.path.insert(0, os.path.join(spark_home, "python", "lib", "py4j-0.10.9.7-src.zip"))
        print("âœ… æ‰‹åŠ¨è®¾ç½® PySpark è·¯å¾„")
        return True
    
    print("âŒ æ— æ³•æ‰¾åˆ° PySpark")
    return False

def test_spark_with_local_mode():
    """ä½¿ç”¨æœ¬åœ°æ¨¡å¼æµ‹è¯• Sparkï¼ˆé¿å…ç‰ˆæœ¬å…¼å®¹æ€§é—®é¢˜ï¼‰"""
    try:
        from pyspark.sql import SparkSession
        
        print("åˆ›å»ºæœ¬åœ°æ¨¡å¼çš„ SparkSession...")
        spark = SparkSession.builder \
            .appName("LocalSparkTest") \
            .master("local[2]") \
            .config("spark.sql.adaptive.enabled", "true") \
            .getOrCreate()
        
        print(f"âœ… Sparkç‰ˆæœ¬: {spark.version}")
        print(f"âœ… è¿è¡Œæ¨¡å¼: æœ¬åœ°æ¨¡å¼")
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        test_data = [("å¼ ä¸‰", 25, "åŒ—äº¬"), ("æå››", 30, "ä¸Šæµ·"), ("ç‹äº”", 35, "å¹¿å·")]
        columns = ["å§“å", "å¹´é¾„", "åŸå¸‚"]
        df = spark.createDataFrame(test_data, columns)
        
        print("\næµ‹è¯• DataFrame æ“ä½œ:")
        df.show()
        
        print(f"æ•°æ®è¡Œæ•°: {df.count()}")
        avg_age = df.agg({"å¹´é¾„": "avg"}).collect()[0][0]
        print(f"å¹³å‡å¹´é¾„: {avg_age:.1f}")
        
        spark.stop()
        print("\nğŸ‰ æœ¬åœ°æ¨¡å¼æµ‹è¯•æˆåŠŸï¼")
        return True
        
    except Exception as e:
        print(f"âŒ æœ¬åœ°æ¨¡å¼æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_spark_cluster_connection():
    """æµ‹è¯•é›†ç¾¤è¿æ¥ï¼ˆå¤„ç†ç‰ˆæœ¬å…¼å®¹æ€§ï¼‰"""
    try:
        from pyspark.sql import SparkSession
        
        print("\nå°è¯•è¿æ¥ Spark é›†ç¾¤...")
        spark = SparkSession.builder \
            .appName("ClusterCompatibilityTest") \
            .master("spark://spark-master:7077") \
            .config("spark.driver.memory", "512m") \
            .config("spark.executor.memory", "1g") \
            .config("spark.executor.cores", "1") \
            .config("spark.network.timeout", "30s") \
            .config("spark.executor.heartbeatInterval", "10s") \
            .getOrCreate()
        
        print(f"âœ… é›†ç¾¤è¿æ¥æˆåŠŸ")
        print(f"âœ… Sparkç‰ˆæœ¬: {spark.version}")
        
        # åˆ›å»ºç®€å•æ•°æ®è¿›è¡Œæµ‹è¯•
        print("\næµ‹è¯•ç®€å•æ“ä½œ...")
        rdd = spark.sparkContext.parallelize([1, 2, 3, 4, 5])
        result = rdd.sum()
        print(f"âœ… RDD æ±‚å’Œç»“æœ: {result}")
        
        # æµ‹è¯• DataFrameï¼ˆå¯èƒ½åœ¨è¿™é‡Œé‡åˆ°ç‰ˆæœ¬å…¼å®¹æ€§é—®é¢˜ï¼‰
        try:
            df = spark.createDataFrame([(1, "test")], ["id", "name"])
            count = df.count()
            print(f"âœ… DataFrame æµ‹è¯•æˆåŠŸï¼Œè¡Œæ•°: {count}")
        except Exception as df_error:
            print(f"âš ï¸  DataFrame æ“ä½œé‡åˆ°ç‰ˆæœ¬å…¼å®¹æ€§é—®é¢˜: {df_error}")
            print("è¿™é€šå¸¸æ˜¯ç”±äºå®¢æˆ·ç«¯å’ŒæœåŠ¡ç«¯ Spark ç‰ˆæœ¬ä¸åŒ¹é…é€ æˆçš„")
        
        spark.stop()
        return True
        
    except Exception as e:
        print(f"âŒ é›†ç¾¤è¿æ¥å¤±è´¥: {e}")
        return False

def main():
    """ä¸»å‡½æ•°"""
    print("=== Spark å¿«é€Ÿå…¼å®¹æ€§æµ‹è¯• ===\n")
    
    # æ­¥éª¤1: è®¾ç½® PySpark è·¯å¾„
    if not setup_pyspark_path():
        print("æ— æ³•è®¾ç½® PySpark ç¯å¢ƒï¼Œé€€å‡ºæµ‹è¯•")
        return
    
    # æ­¥éª¤2: æµ‹è¯•æœ¬åœ°æ¨¡å¼ï¼ˆæ€»æ˜¯åº”è¯¥å·¥ä½œï¼‰
    print("\n--- æµ‹è¯•æœ¬åœ°æ¨¡å¼ ---")
    local_success = test_spark_with_local_mode()
    
    # æ­¥éª¤3: æµ‹è¯•é›†ç¾¤è¿æ¥
    print("\n--- æµ‹è¯•é›†ç¾¤è¿æ¥ ---")
    cluster_success = test_spark_cluster_connection()
    
    # æ€»ç»“
    print("\n=== æµ‹è¯•æ€»ç»“ ===")
    print(f"æœ¬åœ°æ¨¡å¼: {'âœ… æˆåŠŸ' if local_success else 'âŒ å¤±è´¥'}")
    print(f"é›†ç¾¤æ¨¡å¼: {'âœ… æˆåŠŸ' if cluster_success else 'âŒ å¤±è´¥'}")
    
    if local_success and not cluster_success:
        print("\nğŸ’¡ å»ºè®®:")
        print("1. æœ¬åœ°æ¨¡å¼å·¥ä½œæ­£å¸¸ï¼Œè¯´æ˜ PySpark å®‰è£…æ­£ç¡®")
        print("2. é›†ç¾¤è¿æ¥é—®é¢˜å¯èƒ½æ˜¯ç‰ˆæœ¬å…¼å®¹æ€§å¯¼è‡´")
        print("3. å¯ä»¥å…ˆä½¿ç”¨æœ¬åœ°æ¨¡å¼å­¦ä¹  Spark åŸºç¡€æ¦‚å¿µ")
        print("4. ç­‰å¾… Jupyter å®¹å™¨å®Œæˆ PySpark 3.4.3 å®‰è£…åå†æµ‹è¯•é›†ç¾¤æ¨¡å¼")

if __name__ == "__main__":
    main()