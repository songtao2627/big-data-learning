    
    api_match = re.search(r'API request: (\\S+)', message)
        api_url = api_match.group(1) if api_match else None
        
        return {
            'timestamp': timestamp_str,
            'level': level,
            'message': message,
            'user_id': user_id,
            'page_url': page_url,
            'api_url': api_url
        }
    return None

# 解析日志并转换为DataFrame
parsed_logs = logs_rdd.map(parse_log).filter(lambda x: x is not None)
log_df = spark.createDataFrame(parsed_logs)

# 显示解析后的数据结构
log_df.printSchema()
log_df.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 基本统计分析\n",
    "\n",
    "现在我们对日志数据进行基本的统计分析。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 日志级别分布\nprint(\"=== 日志级别分布 ===\")\nlevel_counts = log_df.groupBy(\"level\").count().orderBy(desc(\"count\"))\nlevel_counts.show()\n\n# 转换为Pandas进行可视化\nlevel_counts_pd = level_counts.toPandas()\n\n# 创建饼图\nplt.figure(figsize=(10, 6))\nplt.subplot(1, 2, 1)\nplt.pie(level_counts_pd['count'], labels=level_counts_pd['level'], autopct='%1.1f%%')\nplt.title('日志级别分布')\n\n# 创建柱状图\nplt.subplot(1, 2, 2)\nsns.barplot(data=level_counts_pd, x='level', y='count')\nplt.title('日志级别计数')\nplt.xticks(rotation=45)\n\nplt.tight_layout()\nplt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 时间分布分析\nfrom pyspark.sql.functions import to_timestamp, hour, date_format\n\n# 将时间戳转换为时间类型\nlog_df_with_time = log_df.withColumn(\n    \"timestamp_parsed\", \n    to_timestamp(col(\"timestamp\"), \"yyyy-MM-dd HH:mm:ss\")\n).withColumn(\n    \"hour\", hour(col(\"timestamp_parsed\"))\n)\n\n# 按小时统计日志数量\nprint(\"=== 按小时统计日志数量 ===\")\nhourly_counts = log_df_with_time.groupBy(\"hour\").count().orderBy(\"hour\")\nhourly_counts.show()\n\n# 可视化时间分布\nhourly_counts_pd = hourly_counts.toPandas()\n\nplt.figure(figsize=(12, 6))\nplt.plot(hourly_counts_pd['hour'], hourly_counts_pd['count'], marker='o')\nplt.title('每小时日志数量分布')\nplt.xlabel('小时')\nplt.ylabel('日志数量')\nplt.grid(True)\nplt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 错误分析\n",
    "\n",
    "重点分析ERROR和WARN级别的日志，识别系统问题。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 错误日志分析\nprint(\"=== 错误日志分析 ===\")\nerror_logs = log_df.filter(col(\"level\") == \"ERROR\")\nprint(f\"错误日志总数: {error_logs.count()}\")\n\n# 显示所有错误日志\nprint(\"\\n错误日志详情:\")\nerror_logs.select(\"timestamp\", \"message\").show(truncate=False)\n\n# 警告日志分析\nprint(\"\\n=== 警告日志分析 ===\")\nwarn_logs = log_df.filter(col(\"level\") == \"WARN\")\nprint(f\"警告日志总数: {warn_logs.count()}\")\n\n# 显示所有警告日志\nprint(\"\\n警告日志详情:\")\nwarn_logs.select(\"timestamp\", \"message\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 错误模式分析\nfrom pyspark.sql.functions import regexp_extract\n\n# 提取错误类型\nerror_patterns = error_logs.withColumn(\n    \"error_type\",\n    when(col(\"message\").contains(\"Database\"), \"Database Error\")\n    .when(col(\"message\").contains(\"404\"), \"404 Not Found\")\n    .when(col(\"message\").contains(\"500\"), \"500 Internal Error\")\n    .when(col(\"message\").contains(\"Service unavailable\"), \"Service Unavailable\")\n    .otherwise(\"Other Error\")\n)\n\n# 错误类型统计\nprint(\"=== 错误类型分布 ===\")\nerror_type_counts = error_patterns.groupBy(\"error_type\").count().orderBy(desc(\"count\"))\nerror_type_counts.show()\n\n# 可视化错误类型分布\nerror_type_counts_pd = error_type_counts.toPandas()\n\nplt.figure(figsize=(10, 6))\nsns.barplot(data=error_type_counts_pd, x='error_type', y='count')\nplt.title('错误类型分布')\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 用户行为分析\n",
    "\n",
    "分析用户登录、页面访问等行为模式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用户登录分析\nprint(\"=== 用户登录分析 ===\")\nlogin_logs = log_df.filter(col(\"user_id\").isNotNull())\n\n# 用户登录次数统计\nuser_login_counts = login_logs.groupBy(\"user_id\").count().orderBy(desc(\"count\"))\nprint(\"用户登录次数排行:\")\nuser_login_counts.show()\n\n# 页面访问分析\nprint(\"\\n=== 页面访问分析 ===\")\npage_logs = log_df.filter(col(\"page_url\").isNotNull())\n\n# 页面访问次数统计\npage_access_counts = page_logs.groupBy(\"page_url\").count().orderBy(desc(\"count\"))\nprint(\"页面访问次数排行:\")\npage_access_counts.show()\n\n# API访问分析\nprint(\"\\n=== API访问分析 ===\")\napi_logs = log_df.filter(col(\"api_url\").isNotNull())\n\n# API访问次数统计\napi_access_counts = api_logs.groupBy(\"api_url\").count().orderBy(desc(\"count\"))\nprint(\"API访问次数排行:\")\napi_access_counts.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 系统性能分析\n",
    "\n",
    "分析系统性能相关的日志信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 系统性能相关日志\nprint(\"=== 系统性能分析 ===\")\n\n# 内存使用警告\nmemory_warnings = log_df.filter(col(\"message\").contains(\"memory\"))\nprint(f\"内存相关警告数量: {memory_warnings.count()}\")\nif memory_warnings.count() > 0:\n    memory_warnings.select(\"timestamp\", \"message\").show(truncate=False)\n\n# CPU使用警告\ncpu_warnings = log_df.filter(col(\"message\").contains(\"CPU\"))\nprint(f\"\\nCPU相关警告数量: {cpu_warnings.count()}\")\nif cpu_warnings.count() > 0:\n    cpu_warnings.select(\"timestamp\", \"message\").show(truncate=False)\n\n# 慢查询检测\nslow_queries = log_df.filter(col(\"message\").contains(\"Slow query\"))\nprint(f\"\\n慢查询数量: {slow_queries.count()}\")\nif slow_queries.count() > 0:\n    slow_queries.select(\"timestamp\", \"message\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 综合报告生成\n",
    "\n",
    "生成日志分析的综合报告。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成综合报告\nprint(\"=== 日志分析综合报告 ===\")\nprint(\"=\" * 50)\n\n# 基本统计\ntotal_logs = log_df.count()\nprint(f\"总日志数量: {total_logs}\")\n\n# 日志级别统计\nlevel_stats = log_df.groupBy(\"level\").count().collect()\nfor row in level_stats:\n    percentage = (row['count'] / total_logs) * 100\n    print(f\"{row['level']} 级别: {row['count']} 条 ({percentage:.1f}%)\")\n\nprint(\"\\n\" + \"=\" * 50)\n\n# 错误和警告汇总\nerror_count = log_df.filter(col(\"level\") == \"ERROR\").count()\nwarn_count = log_df.filter(col(\"level\") == \"WARN\").count()\nprint(f\"错误日志: {error_count} 条\")\nprint(f\"警告日志: {warn_count} 条\")\nprint(f\"错误率: {(error_count / total_logs) * 100:.2f}%\")\nprint(f\"警告率: {(warn_count / total_logs) * 100:.2f}%\")\n\nprint(\"\\n\" + \"=\" * 50)\n\n# 用户活动汇总\nunique_users = log_df.filter(col(\"user_id\").isNotNull()).select(\"user_id\").distinct().count()\nlogin_events = log_df.filter(col(\"message\").contains(\"login\")).count()\nlogout_events = log_df.filter(col(\"message\").contains(\"logout\")).count()\n\nprint(f\"活跃用户数: {unique_users}\")\nprint(f\"登录事件: {login_events} 次\")\nprint(f\"登出事件: {logout_events} 次\")\n\nprint(\"\\n\" + \"=\" * 50)\n\n# 系统健康状况\nif error_count == 0 and warn_count <= 2:\n    health_status = \"良好\"\nelif error_count <= 2 and warn_count <= 5:\n    health_status = \"一般\"\nelse:\n    health_status = \"需要关注\"\n\nprint(f\"系统健康状况: {health_status}\")\n\n# 建议\nprint(\"\\n建议:\")\nif error_count > 0:\n    print(\"- 需要调查和修复错误日志中的问题\")\nif warn_count > 3:\n    print(\"- 建议关注警告日志，预防潜在问题\")\nif slow_queries.count() > 0:\n    print(\"- 优化慢查询以提高系统性能\")\nif memory_warnings.count() > 0 or cpu_warnings.count() > 0:\n    print(\"- 监控系统资源使用情况，考虑扩容\")\n\nprint(\"\\n\" + \"=\" * 50)\nprint(\"报告生成完成\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 数据导出\n",
    "\n",
    "将分析结果导出为文件，便于后续使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导出分析结果\nprint(\"=== 导出分析结果 ===\")\n\n# 创建输出目录\noutput_dir = \"/home/jovyan/output/log_analysis\"\nos.makedirs(output_dir, exist_ok=True)\n\n# 导出日志级别统计\nlevel_counts.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(f\"{output_dir}/level_counts\")\nprint(\"日志级别统计已导出\")\n\n# 导出错误日志\nerror_logs.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(f\"{output_dir}/error_logs\")\nprint(\"错误日志已导出\")\n\n# 导出用户活动统计\nuser_login_counts.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(f\"{output_dir}/user_activity\")\nprint(\"用户活动统计已导出\")\n\n# 导出页面访问统计\npage_access_counts.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(f\"{output_dir}/page_access\")\nprint(\"页面访问统计已导出\")\n\nprint(f\"\\n所有结果已导出到: {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 总结\n",
    "\n",
    "本项目演示了如何使用Apache Spark进行日志分析，包括：\n",
    "\n",
    "1. **数据加载和解析**: 使用正则表达式解析非结构化日志数据\n",
    "2. **基本统计分析**: 分析日志级别分布和时间模式\n",
    "3. **错误分析**: 识别和分类系统错误\n",
    "4. **用户行为分析**: 分析用户登录和页面访问模式\n",
    "5. **性能分析**: 检测系统性能问题\n",
    "6. **报告生成**: 生成综合分析报告\n",
    "7. **数据导出**: 将结果保存为文件\n",
    "\n",
    "### 学习要点\n",
    "\n",
    "- 使用RDD和DataFrame API处理非结构化数据\n",
    "- 正则表达式在数据解析中的应用\n",
    "- Spark SQL在数据分析中的使用\n",
    "- 数据可视化技术\n",
    "- 大数据处理的最佳实践\n",
    "\n",
    "### 扩展建议\n",
    "\n",
    "- 实现实时日志分析\n",
    "- 添加机器学习算法进行异常检测\n",
    "- 集成告警系统\n",
    "- 构建交互式仪表板\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}