{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark结构化流处理 (Structured Streaming)\n",
    "\n",
    "本笔记本介绍Spark的结构化流处理，这是Spark 2.0引入的新的流处理引擎，建立在Spark SQL引擎之上。\n",
    "\n",
    "## 什么是结构化流处理？\n",
    "\n",
    "结构化流处理是一个可扩展且容错的流处理引擎，建立在Spark SQL引擎之上。您可以像表达静态数据上的批处理计算一样表达流计算，Spark SQL引擎将负责增量地、连续地运行它并更新最终结果。\n",
    "\n",
    "### 核心概念\n",
    "\n",
    "- **无界表**：将数据流视为一个不断增长的表\n",
    "- **增量查询**：对流数据的查询被转换为增量执行计划\n",
    "- **输出模式**：控制如何将结果写入输出接收器\n",
    "- **触发器**：控制何时检查新数据并更新结果\n",
    "- **检查点和WAL**：用于容错的机制"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 创建SparkSession\n",
    "\n",
    "结构化流处理使用SparkSession作为入口点。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import time\n",
    "import threading\n",
    "import random\n",
    "import os\n",
    "\n",
    "# 创建SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"结构化流处理\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# 设置日志级别\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "print(f\"Spark版本: {spark.version}\")\n",
    "print(\"结构化流处理环境准备完成\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 创建流数据源\n",
    "\n",
    "结构化流处理支持多种数据源，包括文件、套接字、Kafka等。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 从文件创建流"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建监控目录\n",
    "streaming_dir = \"/home/jovyan/data/streaming_input\"\n",
    "os.makedirs(streaming_dir, exist_ok=True)\n",
    "\n",
    "# 定义schema\n",
    "schema = StructType([\n",
    "    StructField(\"timestamp\", LongType(), True),\n",
    "    StructField(\"product\", StringType(), True),\n",
    "    StructField(\"category\", StringType(), True),\n",
    "    StructField(\"price\", DoubleType(), True),\n",
    "    StructField(\"quantity\", IntegerType(), True),\n",
    "    StructField(\"customer_id\", StringType(), True),\n",
    "    StructField(\"region\", StringType(), True)\n",
    "])\n",
    "\n",
    "# 从CSV文件创建流\n",
    "file_stream = spark.readStream \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(schema) \\\n",
    "    .csv(streaming_dir)\n",
    "\n",
    "print(f\"监控目录: {streaming_dir}\")\n",
    "print(\"文件流创建成功\")\n",
    "print(\"Schema:\")\n",
    "file_stream.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 从Rate Source创建流（用于测试）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rate source生成测试数据\n",
    "rate_stream = spark.readStream \\\n",
    "    .format(\"rate\") \\\n",
    "    .option(\"rowsPerSecond\", 5) \\\n",
    "    .load()\n",
    "\n",
    "print(\"Rate流创建成功\")\n",
    "print(\"Schema:\")\n",
    "rate_stream.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 创建模拟数据生成器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建模拟销售数据生成器\n",
    "def generate_sales_data(num_records=10):\n",
    "    \"\"\"生成模拟销售数据\"\"\"\n",
    "    products = ['laptop', 'phone', 'tablet', 'watch', 'headphones', 'camera', 'speaker']\n",
    "    categories = ['Electronics', 'Accessories', 'Computing']\n",
    "    regions = ['North', 'South', 'East', 'West', 'Central']\n",
    "    \n",
    "    data = []\n",
    "    for i in range(num_records):\n",
    "        timestamp = int(time.time()) + i\n",
    "        product = random.choice(products)\n",
    "        category = random.choice(categories)\n",
    "        price = round(random.uniform(50, 1000), 2)\n",
    "        quantity = random.randint(1, 5)\n",
    "        customer_id = f\"C{random.randint(1000, 9999)}\"\n",
    "        region = random.choice(regions)\n",
    "        \n",
    "        data.append(f\"{timestamp},{product},{category},{price},{quantity},{customer_id},{region}\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "# 测试数据生成器\n",
    "sample_data = generate_sales_data(5)\n",
    "print(\"示例销售数据:\")\n",
    "for item in sample_data:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 基本流处理操作\n",
    "\n",
    "结构化流处理支持大部分DataFrame操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用rate stream进行基本操作演示\n",
    "# 添加计算列\n",
    "enhanced_stream = rate_stream.select(\n",
    "    col(\"timestamp\"),\n",
    "    col(\"value\"),\n",
    "    (col(\"value\") * 2).alias(\"double_value\"),\n",
    "    when(col(\"value\") % 2 == 0, \"even\").otherwise(\"odd\").alias(\"parity\")\n",
    ")\n",
    "\n",
    "print(\"增强后的流Schema:\")\n",
    "enhanced_stream.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 过滤操作\n",
    "filtered_stream = enhanced_stream.filter(col(\"value\") > 10)\n",
    "\n",
    "# 聚合操作\n",
    "aggregated_stream = enhanced_stream.groupBy(\"parity\").count()\n",
    "\n",
    "print(\"流处理操作配置完成\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 输出操作和输出模式\n",
    "\n",
    "结构化流处理支持三种输出模式：\n",
    "- **Append**：只有新行被添加到结果表\n",
    "- **Complete**：整个结果表被写入外部存储\n",
    "- **Update**：只有结果表中被更新的行被写入外部存储"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输出到控制台 - Append模式\n",
    "console_query = enhanced_stream \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\", \"false\") \\\n",
    "    .trigger(processingTime='5 seconds') \\\n",
    "    .start()\n",
    "\n",
    "print(\"控制台输出查询已启动\")\n",
    "\n",
    "# 等待一段时间后停止\n",
    "time.sleep(15)\n",
    "console_query.stop()\n",
    "print(\"控制台输出查询已停止\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 聚合查询 - Complete模式\n",
    "aggregation_query = aggregated_stream \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"console\") \\\n",
    "    .trigger(processingTime='5 seconds') \\\n",
    "    .start()\n",
    "\n",
    "print(\"聚合查询已启动\")\n",
    "\n",
    "# 等待一段时间后停止\n",
    "time.sleep(15)\n",
    "aggregation_query.stop()\n",
    "print(\"聚合查询已停止\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 窗口操作\n",
    "\n",
    "结构化流处理支持基于时间的窗口操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建带时间戳的流\n",
    "timestamped_stream = rate_stream.select(\n",
    "    col(\"value\"),\n",
    "    col(\"timestamp\").cast(\"timestamp\").alias(\"event_time\")\n",
    ")\n",
    "\n",
    "# 窗口聚合：每10秒窗口，每5秒滑动\n",
    "windowed_counts = timestamped_stream \\\n",
    "    .groupBy(\n",
    "        window(col(\"event_time\"), \"10 seconds\", \"5 seconds\")\n",
    "    ) \\\n",
    "    .count()\n",
    "\n",
    "# 输出窗口结果\n",
    "window_query = windowed_counts \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\", \"false\") \\\n",
    "    .trigger(processingTime='5 seconds') \\\n",
    "    .start()\n",
    "\n",
    "print(\"窗口查询已启动\")\n",
    "\n",
    "# 等待一段时间后停止\n",
    "time.sleep(20)\n",
    "window_query.stop()\n",
    "print(\"窗口查询已停止\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 水印和延迟数据处理\n",
    "\n",
    "水印用于处理延迟到达的数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用水印处理延迟数据\n",
    "watermarked_stream = timestamped_stream \\\n",
    "    .withWatermark(\"event_time\", \"10 seconds\")\n",
    "\n",
    "# 带水印的窗口聚合\n",
    "watermarked_counts = watermarked_stream \\\n",
    "    .groupBy(\n",
    "        window(col(\"event_time\"), \"10 seconds\")\n",
    "    ) \\\n",
    "    .count()\n",
    "\n",
    "# 输出带水印的结果\n",
    "watermark_query = watermarked_counts \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\", \"false\") \\\n",
    "    .trigger(processingTime='5 seconds') \\\n",
    "    .start()\n",
    "\n",
    "print(\"带水印的查询已启动\")\n",
    "\n",
    "# 等待一段时间后停止\n",
    "time.sleep(15)\n",
    "watermark_query.stop()\n",
    "print(\"带水印的查询已停止\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 流-流连接\n",
    "\n",
    "结构化流处理支持流与流之间的连接操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建两个流用于连接演示\n",
    "stream1 = spark.readStream \\\n",
    "    .format(\"rate\") \\\n",
    "    .option(\"rowsPerSecond\", 2) \\\n",
    "    .load() \\\n",
    "    .select(\n",
    "        col(\"value\").alias(\"id\"),\n",
    "        col(\"timestamp\").cast(\"timestamp\").alias(\"timestamp1\")\n",
    "    )\n",
    "\n",
    "stream2 = spark.readStream \\\n",
    "    .format(\"rate\") \\\n",
    "    .option(\"rowsPerSecond\", 2) \\\n",
    "    .option(\"numPartitions\", 1) \\\n",
    "    .load() \\\n",
    "    .select(\n",
    "        col(\"value\").alias(\"id\"),\n",
    "        col(\"timestamp\").cast(\"timestamp\").alias(\"timestamp2\")\n",
    "    )\n",
    "\n",
    "# 添加水印\n",
    "stream1_watermarked = stream1.withWatermark(\"timestamp1\", \"10 seconds\")\n",
    "stream2_watermarked = stream2.withWatermark(\"timestamp2\", \"10 seconds\")\n",
    "\n",
    "# 流-流内连接\n",
    "joined_stream = stream1_watermarked.join(\n",
    "    stream2_watermarked,\n",
    "    expr(\"\"\"\n",
    "        id = id AND\n",
    "        timestamp1 >= timestamp2 - interval 5 seconds AND\n",
    "        timestamp1 <= timestamp2 + interval 5 seconds\n",
    "    \"\"\")\n",
    ")\n",
    "\n",
    "# 输出连接结果\n",
    "join_query = joined_stream \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\", \"false\") \\\n",
    "    .trigger(processingTime='5 seconds') \\\n",
    "    .start()\n",
    "\n",
    "print(\"流-流连接查询已启动\")\n",
    "\n",
    "# 等待一段时间后停止\n",
    "time.sleep(20)\n",
    "join_query.stop()\n",
    "print(\"流-流连接查询已停止\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 实际案例：实时销售分析系统\n",
    "\n",
    "让我们创建一个完整的实时销售分析系统。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建销售数据文件用于演示\n",
    "def create_sales_files():\n",
    "    \"\"\"创建销售数据文件\"\"\"\n",
    "    for i in range(3):\n",
    "        filename = f\"{streaming_dir}/sales_{int(time.time())}_{i}.csv\"\n",
    "        \n",
    "        # 创建CSV内容\n",
    "        header = \"timestamp,product,category,price,quantity,customer_id,region\\n\"\n",
    "        data_lines = generate_sales_data(20)\n",
    "        \n",
    "        with open(filename, 'w') as f:\n",
    "            f.write(header)\n",
    "            for line in data_lines:\n",
    "                f.write(line + \"\\n\")\n",
    "        \n",
    "        print(f\"创建文件: {filename}\")\n",
    "        time.sleep(2)  # 间隔2秒创建下一个文件\n",
    "\n",
    "# 在后台线程中创建文件\n",
    "file_thread = threading.Thread(target=create_sales_files)\n",
    "file_thread.start()\n",
    "\n",
    "print(\"开始创建销售数据文件...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 等待文件创建完成\n",
    "file_thread.join()\n",
    "\n",
    "# 从文件创建销售数据流\n",
    "sales_stream = spark.readStream \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(schema) \\\n",
    "    .csv(streaming_dir)\n",
    "\n",
    "# 添加计算列\n",
    "enhanced_sales = sales_stream.select(\n",
    "    col(\"*\"),\n",
    "    (col(\"price\") * col(\"quantity\")).alias(\"total_amount\"),\n",
    "    from_unixtime(col(\"timestamp\")).cast(\"timestamp\").alias(\"event_time\")\n",
    ")\n",
    "\n",
    "print(\"销售数据流创建完成\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 实时销售总额统计\n",
    "sales_summary = enhanced_sales \\\n",
    "    .groupBy() \\\n",
    "    .agg(\n",
    "        sum(\"total_amount\").alias(\"total_sales\"),\n",
    "        count(\"*\").alias(\"total_orders\"),\n",
    "        avg(\"total_amount\").alias(\"avg_order_value\")\n",
    "    )\n",
    "\n",
    "summary_query = sales_summary \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\", \"false\") \\\n",
    "    .queryName(\"sales_summary\") \\\n",
    "    .trigger(processingTime='5 seconds') \\\n",
    "    .start()\n",
    "\n",
    "print(\"销售总额统计查询已启动\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 按地区统计销售额\n",
    "region_sales = enhanced_sales \\\n",
    "    .groupBy(\"region\") \\\n",
    "    .agg(\n",
    "        sum(\"total_amount\").alias(\"region_sales\"),\n",
    "        count(\"*\").alias(\"region_orders\")\n",
    "    ) \\\n",
    "    .orderBy(desc(\"region_sales\"))\n",
    "\n",
    "region_query = region_sales \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\", \"false\") \\\n",
    "    .queryName(\"region_sales\") \\\n",
    "    .trigger(processingTime='5 seconds') \\\n",
    "    .start()\n",
    "\n",
    "print(\"地区销售统计查询已启动\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 时间窗口销售趋势\n",
    "windowed_sales = enhanced_sales \\\n",
    "    .withWatermark(\"event_time\", \"10 seconds\") \\\n",
    "    .groupBy(\n",
    "        window(col(\"event_time\"), \"30 seconds\", \"15 seconds\")\n",
    "    ) \\\n",
    "    .agg(\n",
    "        sum(\"total_amount\").alias(\"window_sales\"),\n",
    "        count(\"*\").alias(\"window_orders\")\n",
    "    )\n",
    "\n",
    "window_sales_query = windowed_sales \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\", \"false\") \\\n",
    "    .queryName(\"windowed_sales\") \\\n",
    "    .trigger(processingTime='5 seconds') \\\n",
    "    .start()\n",
    "\n",
    "print(\"时间窗口销售趋势查询已启动\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. 异常检测：检测大额订单\n",
    "large_orders = enhanced_sales.filter(col(\"total_amount\") > 2000)\n",
    "\n",
    "alert_query = large_orders \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\", \"false\") \\\n",
    "    .queryName(\"large_orders_alert\") \\\n",
    "    .trigger(processingTime='3 seconds') \\\n",
    "    .start()\n",
    "\n",
    "print(\"大额订单告警查询已启动\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 让所有查询运行一段时间\n",
    "print(\"所有查询正在运行，等待30秒...\")\n",
    "time.sleep(30)\n",
    "\n",
    "# 停止所有查询\n",
    "queries = [summary_query, region_query, window_sales_query, alert_query]\n",
    "for query in queries:\n",
    "    if query.isActive:\n",
    "        query.stop()\n",
    "        print(f\"查询 {query.name} 已停止\")\n",
    "\n",
    "print(\"所有查询已停止\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 检查点和容错\n",
    "\n",
    "检查点是结构化流处理容错机制的核心。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置检查点目录\n",
    "checkpoint_dir = \"/home/jovyan/data/checkpoint_structured\"\n",
    "\n",
    "# 创建带检查点的查询\n",
    "checkpointed_query = enhanced_sales \\\n",
    "    .groupBy(\"category\") \\\n",
    "    .agg(sum(\"total_amount\").alias(\"category_sales\")) \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"checkpointLocation\", checkpoint_dir) \\\n",
    "    .trigger(processingTime='5 seconds') \\\n",
    "    .start()\n",
    "\n",
    "print(f\"带检查点的查询已启动，检查点目录: {checkpoint_dir}\")\n",
    "\n",
    "# 运行一段时间\n",
    "time.sleep(15)\n",
    "\n",
    "# 停止查询\n",
    "checkpointed_query.stop()\n",
    "print(\"带检查点的查询已停止\")\n",
    "\n",
    "# 检查检查点目录\n",
    "if os.path.exists(checkpoint_dir):\n",
    "    print(f\"检查点目录内容: {os.listdir(checkpoint_dir)}\")\n",
    "else:\n",
    "    print(\"检查点目录不存在\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. 监控和调试\n",
    "\n",
    "结构化流处理提供了丰富的监控和调试功能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建一个查询用于监控演示\n",
    "monitoring_stream = spark.readStream \\\n",
    "    .format(\"rate\") \\\n",
    "    .option(\"rowsPerSecond\", 10) \\\n",
    "    .load()\n",
    "\n",
    "monitoring_query = monitoring_stream \\\n",
    "    .groupBy() \\\n",
    "    .count() \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"console\") \\\n",
    "    .trigger(processingTime='3 seconds') \\\n",
    "    .start()\n",
    "\n",
    "print(\"监控查询已启动\")\n",
    "\n",
    "# 等待几个批次\n",
    "time.sleep(10)\n",
    "\n",
    "# 获取查询状态\n",
    "print(\"\\n=== 查询状态 ===\")\n",
    "print(f\"查询ID: {monitoring_query.id}\")\n",
    "print(f\"查询名称: {monitoring_query.name}\")\n",
    "print(f\"是否活跃: {monitoring_query.isActive}\")\n",
    "\n",
    "# 获取最近的进度信息\n",
    "progress = monitoring_query.lastProgress\n",
    "if progress:\n",
    "    print(f\"\\n=== 最近进度 ===\")\n",
    "    print(f\"批次ID: {progress.get('batchId', 'N/A')}\")\n",
    "    print(f\"批次持续时间: {progress.get('batchDuration', 'N/A')} ms\")\n",
    "    print(f\"输入行数: {progress.get('inputRowsPerSecond', 'N/A')}\")\n",
    "    print(f\"处理行数: {progress.get('processedRowsPerSecond', 'N/A')}\")\n",
    "\n",
    "# 停止查询\n",
    "monitoring_query.stop()\n",
    "print(\"\\n监控查询已停止\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. 练习\n",
    "\n",
    "现在，让我们通过一些练习来巩固所学知识。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习1：实时用户活动分析\n",
    "\n",
    "创建一个实时用户活动分析系统，分析用户的点击行为。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建用户活动数据生成器\n",
    "def generate_user_activity():\n",
    "    \"\"\"生成用户活动数据\"\"\"\n",
    "    users = [f\"user_{i}\" for i in range(1, 21)]\n",
    "    pages = ['home', 'product', 'cart', 'checkout', 'profile', 'search']\n",
    "    actions = ['view', 'click', 'scroll', 'purchase']\n",
    "    \n",
    "    data = []\n",
    "    for _ in range(50):\n",
    "        timestamp = int(time.time())\n",
    "        user_id = random.choice(users)\n",
    "        page = random.choice(pages)\n",
    "        action = random.choice(actions)\n",
    "        session_id = f\"session_{random.randint(1000, 9999)}\"\n",
    "        \n",
    "        data.append(f\"{timestamp},{user_id},{page},{action},{session_id}\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "# 创建用户活动文件\n",
    "activity_dir = \"/home/jovyan/data/user_activity\"\n",
    "os.makedirs(activity_dir, exist_ok=True)\n",
    "\n",
    "def create_activity_files():\n",
    "    for i in range(2):\n",
    "        filename = f\"{activity_dir}/activity_{int(time.time())}_{i}.csv\"\n",
    "        \n",
    "        header = \"timestamp,user_id,page,action,session_id\\n\"\n",
    "        data_lines = generate_user_activity()\n",
    "        \n",
    "        with open(filename, 'w') as f:\n",
    "            f.write(header)\n",
    "            for line in data_lines:\n",
    "                f.write(line + \"\\n\")\n",
    "        \n",
    "        print(f\"创建活动文件: {filename}\")\n",
    "        time.sleep(3)\n",
    "\n",
    "# 在后台创建文件\n",
    "activity_thread = threading.Thread(target=create_activity_files)\n",
    "activity_thread.start()\n",
    "\n",
    "print(\"开始创建用户活动文件...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 等待文件创建完成\n",
    "activity_thread.join()\n",
    "\n",
    "# 定义用户活动schema\n",
    "activity_schema = StructType([\n",
    "    StructField(\"timestamp\", LongType(), True),\n",
    "    StructField(\"user_id\", StringType(), True),\n",
    "    StructField(\"page\", StringType(), True),\n",
    "    StructField(\"action\", StringType(), True),\n",
    "    StructField(\"session_id\", StringType(), True)\n",
    "])\n",
    "\n",
    "# 创建用户活动流\n",
    "activity_stream = spark.readStream \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(activity_schema) \\\n",
    "    .csv(activity_dir)\n",
    "\n",
    "# 添加事件时间\n",
    "activity_with_time = activity_stream.select(\n",
    "    col(\"*\"),\n",
    "    from_unixtime(col(\"timestamp\")).cast(\"timestamp\").alias(\"event_time\")\n",
    ")\n",
    "\n",
    "print(\"用户活动流创建完成\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 实时页面访问统计\n",
    "page_stats = activity_with_time \\\n",
    "    .groupBy(\"page\") \\\n",
    "    .count() \\\n",
    "    .orderBy(desc(\"count\"))\n",
    "\n",
    "page_query = page_stats \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"console\") \\\n",
    "    .queryName(\"page_stats\") \\\n",
    "    .trigger(processingTime='5 seconds') \\\n",
    "    .start()\n",
    "\n",
    "# 2. 用户行为分析\n",
    "user_behavior = activity_with_time \\\n",
    "    .groupBy(\"user_id\", \"action\") \\\n",
    "    .count() \\\n",
    "    .orderBy(\"user_id\", desc(\"count\"))\n",
    "\n",
    "behavior_query = user_behavior \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"console\") \\\n",
    "    .queryName(\"user_behavior\") \\\n",
    "    .trigger(processingTime='5 seconds') \\\n",
    "    .start()\n",
    "\n",
    "print(\"用户活动分析查询已启动\")\n",
    "\n",
    "# 运行查询\n",
    "time.sleep(20)\n",
    "\n",
    "# 停止查询\n",
    "page_query.stop()\n",
    "behavior_query.stop()\n",
    "print(\"用户活动分析查询已停止\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习2：实时异常检测\n",
    "\n",
    "创建一个实时异常检测系统，检测异常的用户行为模式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 异常检测：检测短时间内大量点击的用户\n",
    "suspicious_activity = activity_with_time \\\n",
    "    .withWatermark(\"event_time\", \"10 seconds\") \\\n",
    "    .groupBy(\n",
    "        \"user_id\",\n",
    "        window(col(\"event_time\"), \"30 seconds\")\n",
    "    ) \\\n",
    "    .count() \\\n",
    "    .filter(col(\"count\") > 10)  # 30秒内超过10次活动视为异常\n",
    "\n",
    "anomaly_query = suspicious_activity \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\", \"false\") \\\n",
    "    .queryName(\"anomaly_detection\") \\\n",
    "    .trigger(processingTime='5 seconds') \\\n",
    "    .start()\n",
    "\n",
    "print(\"异常检测查询已启动\")\n",
    "\n",
    "# 运行查询\n",
    "time.sleep(15)\n",
    "\n",
    "# 停止查询\n",
    "anomaly_query.stop()\n",
    "print(\"异常检测查询已停止\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. 总结\n",
    "\n",
    "在本笔记本中，我们学习了：\n",
    "\n",
    "1. 结构化流处理的基本概念和优势\n",
    "2. 如何创建流数据源（文件、Rate source等）\n",
    "3. 基本流处理操作（选择、过滤、聚合）\n",
    "4. 输出操作和输出模式（Append、Complete、Update）\n",
    "5. 窗口操作和时间处理\n",
    "6. 水印和延迟数据处理\n",
    "7. 流-流连接操作\n",
    "8. 实际案例：实时销售分析系统\n",
    "9. 检查点和容错机制\n",
    "10. 监控和调试技术\n",
    "11. 实践练习：用户活动分析和异常检测\n",
    "\n",
    "结构化流处理是Spark中推荐的流处理方式，它提供了更高级的API、更好的性能和更强的容错能力。它建立在Spark SQL引擎之上，可以利用Catalyst优化器的所有优化功能。\n",
    "\n",
    "## 下一步\n",
    "\n",
    "接下来，我们将学习如何将Spark Streaming与Kafka集成，处理真实的流数据。请继续学习 `kafka-integration.ipynb` 笔记本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 清理资源\n",
    "spark.stop()\n",
    "print(\"SparkSession已停止\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}