{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark RDD基础\n",
    "\n",
    "本笔记本介绍Spark的核心抽象：弹性分布式数据集（Resilient Distributed Dataset，简称RDD）。\n",
    "\n",
    "## 什么是RDD？\n",
    "\n",
    "RDD是Spark的基础数据结构，具有以下特点：\n",
    "- **弹性**：可以从失败中恢复\n",
    "- **分布式**：数据分布在集群的多个节点上\n",
    "- **数据集**：是一个不可变的分布式对象集合\n",
    "\n",
    "RDD支持两种类型的操作：\n",
    "- **转换（Transformations）**：从现有RDD创建新RDD的操作（如map、filter）\n",
    "- **动作（Actions）**：返回值或将结果写入存储系统的操作（如count、collect）\n",
    "\n",
    "让我们开始学习RDD的基本操作。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 创建SparkSession\n",
    "\n",
    "首先，我们需要创建一个SparkSession，这是与Spark交互的入口点。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# 导入PySpark模块\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# 创建SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"RDD基础\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# 获取SparkContext\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# 显示Spark版本\n",
    "print(f\"Spark版本: {spark.version}\")\n",
    "print(f\"Python版本: {sys.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 创建RDD\n",
    "\n",
    "有多种方法可以创建RDD：\n",
    "1. 从集合（列表、集合等）创建\n",
    "2. 从外部数据源（文件、数据库等）创建\n",
    "3. 从现有RDD转换得到"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 从集合创建RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从列表创建RDD\n",
    "data = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "rdd1 = sc.parallelize(data)\n",
    "\n",
    "# 查看RDD内容\n",
    "print(\"RDD1内容:\")\n",
    "print(rdd1.collect())\n",
    "\n",
    "# 创建带分区的RDD\n",
    "rdd2 = sc.parallelize(data, 4)  # 4个分区\n",
    "print(f\"\\nRDD2分区数: {rdd2.getNumPartitions()}\")\n",
    "\n",
    "# 创建键值对RDD\n",
    "pairs = [(\"a\", 1), (\"b\", 2), (\"c\", 3)]\n",
    "pairRDD = sc.parallelize(pairs)\n",
    "print(\"\\n键值对RDD内容:\")\n",
    "print(pairRDD.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 从文件创建RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从文本文件创建RDD\n",
    "# 使用示例日志文件\n",
    "logFile = \"/home/jovyan/data/sample/server_logs.txt\"\n",
    "logRDD = sc.textFile(logFile)\n",
    "\n",
    "# 查看前几行\n",
    "print(\"日志文件前5行:\")\n",
    "for line in logRDD.take(5):\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. RDD转换操作\n",
    "\n",
    "转换操作会从现有RDD创建新的RDD。这些操作是**惰性**的，只有在执行动作操作时才会计算。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 基本转换操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map: 对每个元素应用函数\n",
    "squared = rdd1.map(lambda x: x * x)\n",
    "print(\"平方后的RDD:\")\n",
    "print(squared.collect())\n",
    "\n",
    "# filter: 过滤元素\n",
    "even = rdd1.filter(lambda x: x % 2 == 0)\n",
    "print(\"\\n偶数RDD:\")\n",
    "print(even.collect())\n",
    "\n",
    "# flatMap: 将每个元素展平为多个元素\n",
    "words = sc.parallelize([\"Hello Spark\", \"Learning RDD\", \"Big Data\"])\n",
    "flatWords = words.flatMap(lambda line: line.split(\" \"))\n",
    "print(\"\\n单词RDD:\")\n",
    "print(flatWords.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 键值对RDD转换操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建一个更大的键值对RDD\n",
    "kvData = [(\"apple\", 3), (\"banana\", 2), (\"orange\", 5), (\"apple\", 1), (\"banana\", 4)]\n",
    "kvRDD = sc.parallelize(kvData)\n",
    "\n",
    "# reduceByKey: 按键聚合值\n",
    "sumByKey = kvRDD.reduceByKey(lambda a, b: a + b)\n",
    "print(\"按键求和:\")\n",
    "print(sumByKey.collect())\n",
    "\n",
    "# groupByKey: 按键分组\n",
    "groupedByKey = kvRDD.groupByKey().mapValues(list)\n",
    "print(\"\\n按键分组:\")\n",
    "print(groupedByKey.collect())\n",
    "\n",
    "# keys: 获取所有键\n",
    "keys = kvRDD.keys()\n",
    "print(\"\\n所有键:\")\n",
    "print(keys.collect())\n",
    "\n",
    "# values: 获取所有值\n",
    "values = kvRDD.values()\n",
    "print(\"\\n所有值:\")\n",
    "print(values.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. RDD动作操作\n",
    "\n",
    "动作操作会触发计算并返回结果或将结果写入外部存储系统。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用之前创建的数字RDD\n",
    "numbersRDD = sc.parallelize(range(1, 101))\n",
    "\n",
    "# count: 计数\n",
    "count = numbersRDD.count()\n",
    "print(f\"元素数量: {count}\")\n",
    "\n",
    "# first: 获取第一个元素\n",
    "first = numbersRDD.first()\n",
    "print(f\"第一个元素: {first}\")\n",
    "\n",
    "# take: 获取前n个元素\n",
    "taken = numbersRDD.take(5)\n",
    "print(f\"前5个元素: {taken}\")\n",
    "\n",
    "# reduce: 使用函数聚合元素\n",
    "sum_result = numbersRDD.reduce(lambda a, b: a + b)\n",
    "print(f\"总和: {sum_result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 实际案例：单词计数\n",
    "\n",
    "让我们使用RDD实现经典的单词计数示例。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建一个包含文本的RDD\n",
    "text = [\n",
    "    \"Apache Spark是一个开源的分布式计算系统\",\n",
    "    \"Spark提供了一个接口用于编程整个集群\",\n",
    "    \"Spark的RDD是分布式数据集合\",\n",
    "    \"RDD是Spark的核心抽象\",\n",
    "    \"Spark支持Python、Java、Scala和R语言\"\n",
    "]\n",
    "textRDD = sc.parallelize(text)\n",
    "\n",
    "# 实现单词计数\n",
    "# 1. 将每行文本拆分为单词\n",
    "words = textRDD.flatMap(lambda line: line.split(\" \"))\n",
    "\n",
    "# 2. 将每个单词映射为(word, 1)的键值对\n",
    "word_pairs = words.map(lambda word: (word, 1))\n",
    "\n",
    "# 3. 按单词聚合计数\n",
    "word_counts = word_pairs.reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "# 4. 按计数降序排序\n",
    "sorted_counts = word_counts.sortBy(lambda x: x[1], ascending=False)\n",
    "\n",
    "# 显示结果\n",
    "print(\"单词计数结果:\")\n",
    "for word, count in sorted_counts.collect():\n",
    "    print(f\"{word}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 练习\n",
    "\n",
    "现在，让我们通过一些练习来巩固所学知识。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习1：分析日志文件\n",
    "\n",
    "使用之前加载的日志文件，完成以下任务：\n",
    "1. 统计不同日志级别（INFO、ERROR、WARN）的消息数量\n",
    "2. 找出包含特定关键字（如\"error\"、\"failed\"）的日志行\n",
    "3. 按小时统计日志数量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在这里编写代码\n",
    "# 提示：使用正则表达式提取日志级别和时间\n",
    "import re\n",
    "\n",
    "# 重新加载日志文件\n",
    "logFile = \"/home/jovyan/data/sample/server_logs.txt\"\n",
    "logRDD = sc.textFile(logFile)\n",
    "\n",
    "# 1. 统计不同日志级别的消息数量\n",
    "# 提取日志级别\n",
    "log_levels = logRDD.map(lambda line: re.search(r'\\[(.*?)\\]\\s+(\\w+)', line).group(2) if re.search(r'\\[(.*?)\\]\\s+(\\w+)', line) else \"UNKNOWN\")\n",
    "\n",
    "# 计算每个级别的数量\n",
    "level_counts = log_levels.map(lambda level: (level, 1)).reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "print(\"日志级别统计:\")\n",
    "for level, count in level_counts.collect():\n",
    "    print(f\"{level}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 找出包含特定关键字的日志行\n",
    "error_logs = logRDD.filter(lambda line: \"error\" in line.lower() or \"failed\" in line.lower())\n",
    "\n",
    "print(\"包含'error'或'failed'的日志:\")\n",
    "for log in error_logs.collect():\n",
    "    print(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 按小时统计日志数量\n",
    "# 提取小时\n",
    "hour_logs = logRDD.map(lambda line: (re.search(r'\\[(\\d{4}-\\d{2}-\\d{2})\\s+(\\d{2}):', line).group(2) if re.search(r'\\[(\\d{4}-\\d{2}-\\d{2})\\s+(\\d{2}):', line) else \"UNKNOWN\", 1))\n",
    "\n",
    "# 按小时聚合\n",
    "hour_counts = hour_logs.reduceByKey(lambda a, b: a + b).sortByKey()\n",
    "\n",
    "print(\"按小时统计日志数量:\")\n",
    "for hour, count in hour_counts.collect():\n",
    "    print(f\"{hour}时: {count}条\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习2：销售数据分析\n",
    "\n",
    "使用示例销售数据，完成以下任务：\n",
    "1. 计算每个产品类别的总销售额\n",
    "2. 找出销售量最高的产品\n",
    "3. 按地区统计销售情况"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载销售数据\n",
    "salesFile = \"/home/jovyan/data/sample/sales_data.csv\"\n",
    "salesLines = sc.textFile(salesFile)\n",
    "\n",
    "# 提取标题行\n",
    "header = salesLines.first()\n",
    "salesData = salesLines.filter(lambda line: line != header)\n",
    "\n",
    "# 解析CSV数据\n",
    "def parse_sales(line):\n",
    "    fields = line.split(',')\n",
    "    return {\n",
    "        'date': fields[0],\n",
    "        'product_id': fields[1],\n",
    "        'category': fields[2],\n",
    "        'price': float(fields[3]),\n",
    "        'quantity': int(fields[4]),\n",
    "        'customer_id': fields[5],\n",
    "        'region': fields[6]\n",
    "    }\n",
    "\n",
    "salesRDD = salesData.map(parse_sales)\n",
    "\n",
    "# 1. 计算每个产品类别的总销售额\n",
    "category_sales = salesRDD.map(lambda sale: (sale['category'], sale['price'] * sale['quantity'])) \\\n",
    "                        .reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "print(\"产品类别销售额:\")\n",
    "for category, total in category_sales.collect():\n",
    "    print(f\"{category}: ${total:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 找出销售量最高的产品\n",
    "product_quantity = salesRDD.map(lambda sale: (sale['product_id'], sale['quantity'])) \\\n",
    "                          .reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "top_product = product_quantity.sortBy(lambda x: x[1], ascending=False).first()\n",
    "\n",
    "print(f\"销售量最高的产品: {top_product[0]}，销售量: {top_product[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 按地区统计销售情况\n",
    "region_sales = salesRDD.map(lambda sale: (sale['region'], sale['price'] * sale['quantity'])) \\\n",
    "                      .reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "print(\"按地区统计销售额:\")\n",
    "for region, total in region_sales.collect():\n",
    "    print(f\"{region}: ${total:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 总结\n",
    "\n",
    "在本笔记本中，我们学习了：\n",
    "\n",
    "1. RDD的基本概念和特点\n",
    "2. 如何创建RDD（从集合、文件等）\n",
    "3. RDD的转换操作（map、filter、flatMap等）\n",
    "4. RDD的动作操作（collect、count、reduce等）\n",
    "5. 键值对RDD的特殊操作（reduceByKey、groupByKey等）\n",
    "6. 如何应用RDD解决实际问题\n",
    "\n",
    "RDD是Spark的基础，虽然在较新的Spark版本中DataFrame和Dataset API更为常用，但理解RDD对于深入掌握Spark非常重要。\n",
    "\n",
    "## 下一步\n",
    "\n",
    "接下来，我们将学习Spark的DataFrame API，它提供了更高级的抽象和更好的性能优化。请继续学习 `dataframe-operations.ipynb` 笔记本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 停止SparkSession\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}