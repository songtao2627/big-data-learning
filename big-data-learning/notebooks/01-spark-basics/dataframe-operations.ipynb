{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark DataFrame操作\n",
    "\n",
    "本笔记本介绍Spark的DataFrame API，这是一个更高级的数据处理抽象，类似于关系型数据库中的表或R/Python中的DataFrame。\n",
    "\n",
    "## 什么是DataFrame？\n",
    "\n",
    "DataFrame是一个分布式的数据集合，组织成命名的列。它概念上等同于关系型数据库中的表或R/Python中的DataFrame，但具有更丰富的优化功能。DataFrame可以从多种数据源构建，如结构化数据文件、Hive表、外部数据库或现有RDD。\n",
    "\n",
    "DataFrame的主要优点：\n",
    "- **结构化数据处理**：提供类似SQL的操作\n",
    "- **优化执行**：通过Catalyst优化器自动优化查询\n",
    "- **更好的性能**：比RDD更高效的内存使用和执行计划\n",
    "- **易用性**：提供高级API，减少代码量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 创建SparkSession\n",
    "\n",
    "与RDD一样，我们首先需要创建一个SparkSession："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark版本: 3.4.3\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# 创建SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DataFrame操作\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# 显示Spark版本\n",
    "print(f\"Spark版本: {spark.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 创建DataFrame\n",
    "\n",
    "有多种方法可以创建DataFrame："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 从结构化数据文件创建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从CSV文件创建DataFrame\n",
    "sales_df = spark.read.option(\"header\", \"true\") \\\n",
    "                    .option(\"inferSchema\", \"true\") \\\n",
    "                    .csv(\"/home/jovyan/data/sample/sales_data.csv\")\n",
    "\n",
    "# 显示DataFrame内容\n",
    "print(\"销售数据:\")\n",
    "sales_df.show()\n",
    "\n",
    "# 显示Schema\n",
    "print(\"销售数据Schema:\")\n",
    "sales_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从JSON文件创建DataFrame\n",
    "user_df = spark.read.json(\"/home/jovyan/data/sample/user_behavior.json\")\n",
    "\n",
    "print(\"用户行为数据:\")\n",
    "user_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 从Python对象创建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从列表创建DataFrame\n",
    "data = [(\"张三\", 25), (\"李四\", 30), (\"王五\", 35), (\"赵六\", 40)]\n",
    "columns = [\"name\", \"age\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "print(\"从列表创建的DataFrame:\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用Row对象创建DataFrame\n",
    "from pyspark.sql import Row\n",
    "Person = Row(\"name\", \"age\", \"city\")\n",
    "people_data = [\n",
    "    Person(\"张三\", 25, \"北京\"),\n",
    "    Person(\"李四\", 30, \"上海\"),\n",
    "    Person(\"王五\", 35, \"广州\")\n",
    "]\n",
    "people_df = spark.createDataFrame(people_data)\n",
    "\n",
    "print(\"使用Row创建的DataFrame:\")\n",
    "people_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 从RDD转换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从RDD创建DataFrame\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "# 创建RDD\n",
    "rdd = spark.sparkContext.parallelize([\n",
    "    (\"张三\", 25, \"北京\"),\n",
    "    (\"李四\", 30, \"上海\"),\n",
    "    (\"王五\", 35, \"广州\")\n",
    "])\n",
    "\n",
    "# 定义schema\n",
    "schema = StructType([\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"city\", StringType(), True)\n",
    "])\n",
    "\n",
    "# 应用schema创建DataFrame\n",
    "df_from_rdd = spark.createDataFrame(rdd, schema)\n",
    "\n",
    "print(\"从RDD创建的DataFrame:\")\n",
    "df_from_rdd.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. DataFrame基本操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 选择和过滤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 选择特定列\n",
    "names = df.select(\"name\")\n",
    "print(\"仅选择姓名列:\")\n",
    "names.show()\n",
    "\n",
    "# 选择多列\n",
    "name_age = df.select(\"name\", \"age\")\n",
    "print(\"选择姓名和年龄列:\")\n",
    "name_age.show()\n",
    "\n",
    "# 过滤行\n",
    "young_people = df.filter(df.age < 30)\n",
    "print(\"年龄小于30的人:\")\n",
    "young_people.show()\n",
    "\n",
    "# 组合选择和过滤\n",
    "young_names = df.filter(df.age < 30).select(\"name\")\n",
    "print(\"年龄小于30的人的姓名:\")\n",
    "young_names.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 列操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, expr, lit\n",
    "\n",
    "# 添加新列\n",
    "df_with_city = df.withColumn(\"city\", lit(\"未知\"))\n",
    "print(\"添加城市列:\")\n",
    "df_with_city.show()\n",
    "\n",
    "# 修改列\n",
    "df_age_plus_one = df.withColumn(\"age\", df.age + 1)\n",
    "print(\"年龄加1:\")\n",
    "df_age_plus_one.show()\n",
    "\n",
    "# 重命名列\n",
    "df_renamed = df.withColumnRenamed(\"age\", \"年龄\")\n",
    "print(\"重命名年龄列:\")\n",
    "df_renamed.show()\n",
    "\n",
    "# 删除列\n",
    "df_no_age = df.drop(\"age\")\n",
    "print(\"删除年龄列:\")\n",
    "df_no_age.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 排序和限制"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 按年龄排序\n",
    "df_sorted = df.sort(\"age\")\n",
    "print(\"按年龄升序排序:\")\n",
    "df_sorted.show()\n",
    "\n",
    "# 按年龄降序排序\n",
    "df_sorted_desc = df.sort(df.age.desc())\n",
    "print(\"按年龄降序排序:\")\n",
    "df_sorted_desc.show()\n",
    "\n",
    "# 限制结果数量\n",
    "df_limited = df.limit(2)\n",
    "print(\"仅显示前2行:\")\n",
    "df_limited.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 聚合和分组"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用销售数据\n",
    "sales_df = spark.read.option(\"header\", \"true\") \\\n",
    "                    .option(\"inferSchema\", \"true\") \\\n",
    "                    .csv(\"/home/jovyan/data/sample/sales_data.csv\")\n",
    "\n",
    "# 按类别分组并计算总销售额\n",
    "from pyspark.sql.functions import sum as sum_func\n",
    "\n",
    "category_sales = sales_df.groupBy(\"category\") \\\n",
    "                        .agg(sum_func(sales_df.price * sales_df.quantity).alias(\"total_sales\"))\n",
    "\n",
    "print(\"按类别统计销售额:\")\n",
    "category_sales.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算多个聚合\n",
    "from pyspark.sql.functions import avg, count, max as max_func, min as min_func\n",
    "\n",
    "category_stats = sales_df.groupBy(\"category\").agg(\n",
    "    count(\"*\").alias(\"count\"),\n",
    "    sum_func(\"price\").alias(\"total_price\"),\n",
    "    avg(\"price\").alias(\"avg_price\"),\n",
    "    max_func(\"price\").alias(\"max_price\"),\n",
    "    min_func(\"price\").alias(\"min_price\")\n",
    ")\n",
    "\n",
    "print(\"按类别的详细统计:\")\n",
    "category_stats.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 连接操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建客户数据\n",
    "customers_data = [\n",
    "    (1, \"张三\", \"北京\"),\n",
    "    (2, \"李四\", \"上海\"),\n",
    "    (3, \"王五\", \"广州\"),\n",
    "    (4, \"赵六\", \"深圳\")\n",
    "]\n",
    "customers_df = spark.createDataFrame(customers_data, [\"id\", \"name\", \"city\"])\n",
    "\n",
    "# 创建订单数据\n",
    "orders_data = [\n",
    "    (101, 1, 100.0),\n",
    "    (102, 2, 150.0),\n",
    "    (103, 3, 200.0),\n",
    "    (104, 1, 120.0),\n",
    "    (105, 4, 80.0),\n",
    "    (106, 5, 90.0)  # 注意：客户ID 5不存在\n",
    "]\n",
    "orders_df = spark.createDataFrame(orders_data, [\"order_id\", \"customer_id\", \"amount\"])\n",
    "\n",
    "# 内连接\n",
    "inner_join = customers_df.join(orders_df, customers_df.id == orders_df.customer_id)\n",
    "print(\"内连接结果:\")\n",
    "inner_join.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 左外连接\n",
    "left_join = customers_df.join(orders_df, customers_df.id == orders_df.customer_id, \"left\")\n",
    "print(\"左外连接结果:\")\n",
    "left_join.show()\n",
    "\n",
    "# 右外连接\n",
    "right_join = customers_df.join(orders_df, customers_df.id == orders_df.customer_id, \"right\")\n",
    "print(\"右外连接结果:\")\n",
    "right_join.show()\n",
    "\n",
    "# 全外连接\n",
    "full_join = customers_df.join(orders_df, customers_df.id == orders_df.customer_id, \"full\")\n",
    "print(\"全外连接结果:\")\n",
    "full_join.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 使用SQL查询\n",
    "\n",
    "Spark SQL允许您使用SQL语句查询DataFrame："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 注册临时视图\n",
    "sales_df.createOrReplaceTempView(\"sales\")\n",
    "customers_df.createOrReplaceTempView(\"customers\")\n",
    "orders_df.createOrReplaceTempView(\"orders\")\n",
    "\n",
    "# 执行SQL查询\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT category, SUM(price * quantity) as total_sales\n",
    "    FROM sales\n",
    "    GROUP BY category\n",
    "    ORDER BY total_sales DESC\n",
    "\"\"\")\n",
    "\n",
    "print(\"SQL查询结果:\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 连接查询\n",
    "join_result = spark.sql(\"\"\"\n",
    "    SELECT c.name, c.city, o.order_id, o.amount\n",
    "    FROM customers c\n",
    "    JOIN orders o ON c.id = o.customer_id\n",
    "    ORDER BY o.amount DESC\n",
    "\"\"\")\n",
    "\n",
    "print(\"SQL连接查询结果:\")\n",
    "join_result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 保存DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存为CSV\n",
    "sales_df.write.mode(\"overwrite\").option(\"header\", \"true\").csv(\"/home/jovyan/data/output/sales_csv\")\n",
    "\n",
    "# 保存为Parquet（Spark的默认格式，更高效）\n",
    "sales_df.write.mode(\"overwrite\").parquet(\"/home/jovyan/data/output/sales_parquet\")\n",
    "\n",
    "# 保存为JSON\n",
    "sales_df.write.mode(\"overwrite\").json(\"/home/jovyan/data/output/sales_json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. DataFrame性能优化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 缓存DataFrame\n",
    "sales_df.cache()\n",
    "\n",
    "# 或者使用特定的存储级别\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "sales_df.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "\n",
    "# 查看执行计划\n",
    "sales_df.groupBy(\"category\").count().explain()\n",
    "\n",
    "# 释放缓存\n",
    "sales_df.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 实际案例：销售数据分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载销售数据\n",
    "sales_df = spark.read.option(\"header\", \"true\") \\\n",
    "                    .option(\"inferSchema\", \"true\") \\\n",
    "                    .csv(\"/home/jovyan/data/sample/sales_data.csv\")\n",
    "\n",
    "# 1. 计算每个地区的总销售额\n",
    "from pyspark.sql.functions import sum as sum_func, round\n",
    "\n",
    "region_sales = sales_df.groupBy(\"region\") \\\n",
    "                      .agg(round(sum_func(sales_df.price * sales_df.quantity), 2).alias(\"total_sales\")) \\\n",
    "                      .orderBy(\"total_sales\", ascending=False)\n",
    "\n",
    "print(\"各地区销售额:\")\n",
    "region_sales.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 找出每个类别中价格最高的产品\n",
    "from pyspark.sql.functions import max as max_func\n",
    "from pyspark.sql.window import Window\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "window_spec = Window.partitionBy(\"category\")\n",
    "max_price_products = sales_df.withColumn(\"max_price\", max_func(\"price\").over(window_spec)) \\\n",
    "                           .filter(sales_df.price == F.col(\"max_price\")) \\\n",
    "                           .select(\"category\", \"product_id\", \"price\") \\\n",
    "                           .orderBy(\"price\", ascending=False)\n",
    "\n",
    "print(\"各类别最贵产品:\")\n",
    "max_price_products.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 计算每个客户的购买总额\n",
    "customer_spending = sales_df.groupBy(\"customer_id\") \\\n",
    "                          .agg(round(sum_func(sales_df.price * sales_df.quantity), 2).alias(\"total_spending\")) \\\n",
    "                          .orderBy(\"total_spending\", ascending=False)\n",
    "\n",
    "print(\"客户消费排行:\")\n",
    "customer_spending.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. 练习\n",
    "\n",
    "### 练习1：用户行为分析\n",
    "\n",
    "使用用户行为数据，完成以下任务：\n",
    "1. 统计每种行为（view、add_to_cart、purchase）的次数\n",
    "2. 找出浏览次数最多的商品\n",
    "3. 计算每个用户的购买转化率（购买次数/浏览次数）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载用户行为数据\n",
    "user_df = spark.read.json(\"/home/jovyan/data/sample/user_behavior.json\")\n",
    "user_df.show(5)\n",
    "\n",
    "# 1. 统计每种行为的次数\n",
    "action_counts = user_df.groupBy(\"action\").count().orderBy(\"count\", ascending=False)\n",
    "print(\"各类行为次数:\")\n",
    "action_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 找出浏览次数最多的商品\n",
    "view_counts = user_df.filter(user_df.action == \"view\") \\\n",
    "                    .groupBy(\"item_id\") \\\n",
    "                    .count() \\\n",
    "                    .orderBy(\"count\", ascending=False)\n",
    "\n",
    "print(\"浏览次数最多的商品:\")\n",
    "view_counts.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 计算每个用户的购买转化率\n",
    "# 先计算每个用户的浏览次数和购买次数\n",
    "from pyspark.sql.functions import when, count\n",
    "\n",
    "user_actions = user_df.groupBy(\"user_id\").agg(\n",
    "    count(when(user_df.action == \"view\", 1)).alias(\"view_count\"),\n",
    "    count(when(user_df.action == \"purchase\", 1)).alias(\"purchase_count\")\n",
    ")\n",
    "\n",
    "# 计算转化率\n",
    "from pyspark.sql.functions import col, round\n",
    "\n",
    "conversion_rates = user_actions.withColumn(\n",
    "    \"conversion_rate\", \n",
    "    round(col(\"purchase_count\") / col(\"view_count\"), 2)\n",
    ").filter(col(\"view_count\") > 0).orderBy(\"conversion_rate\", ascending=False)\n",
    "\n",
    "print(\"用户购买转化率:\")\n",
    "conversion_rates.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习2：销售数据高级分析\n",
    "\n",
    "使用销售数据，完成以下任务：\n",
    "1. 按月份统计销售趋势\n",
    "2. 计算每个类别的销售占比\n",
    "3. 找出购买频率最高的客户"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 按月份统计销售趋势\n",
    "from pyspark.sql.functions import substring, to_date\n",
    "\n",
    "# 提取月份\n",
    "sales_with_date = sales_df.withColumn(\"date\", to_date(\"date\", \"yyyy-MM-dd\"))\n",
    "sales_with_month = sales_with_date.withColumn(\"month\", substring(\"date\", 1, 7))\n",
    "\n",
    "# 按月份统计销售额\n",
    "monthly_sales = sales_with_month.groupBy(\"month\") \\\n",
    "                              .agg(round(sum_func(sales_df.price * sales_df.quantity), 2).alias(\"total_sales\")) \\\n",
    "                              .orderBy(\"month\")\n",
    "\n",
    "print(\"月度销售趋势:\")\n",
    "monthly_sales.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 计算每个类别的销售占比\n",
    "# 先计算总销售额\n",
    "total_sales = sales_df.agg(sum_func(sales_df.price * sales_df.quantity)).collect()[0][0]\n",
    "\n",
    "# 计算每个类别的销售额和占比\n",
    "category_sales_ratio = sales_df.groupBy(\"category\") \\\n",
    "                              .agg(sum_func(sales_df.price * sales_df.quantity).alias(\"category_sales\")) \\\n",
    "                              .withColumn(\"total_sales\", lit(total_sales)) \\\n",
    "                              .withColumn(\"sales_ratio\", round(col(\"category_sales\") / col(\"total_sales\") * 100, 2)) \\\n",
    "                              .select(\"category\", \"category_sales\", \"sales_ratio\") \\\n",
    "                              .orderBy(\"sales_ratio\", ascending=False)\n",
    "\n",
    "print(\"类别销售占比:\")\n",
    "category_sales_ratio.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 找出购买频率最高的客户\n",
    "customer_frequency = sales_df.groupBy(\"customer_id\") \\\n",
    "                            .agg(count(\"*\").alias(\"purchase_count\")) \\\n",
    "                            .orderBy(\"purchase_count\", ascending=False)\n",
    "\n",
    "print(\"客户购买频率排行:\")\n",
    "customer_frequency.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. 总结\n",
    "\n",
    "在本笔记本中，我们学习了：\n",
    "\n",
    "1. DataFrame的基本概念和优势\n",
    "2. 如何创建DataFrame（从文件、Python对象、RDD）\n",
    "3. DataFrame的基本操作（选择、过滤、列操作）\n",
    "4. 聚合和分组操作\n",
    "5. 连接操作\n",
    "6. 使用SQL查询DataFrame\n",
    "7. 保存DataFrame到不同格式\n",
    "8. DataFrame性能优化技巧\n",
    "9. 如何应用DataFrame解决实际问题\n",
    "\n",
    "DataFrame API是Spark中最常用的数据处理接口，它结合了SQL的易用性和Spark的分布式计算能力，是大数据分析的强大工具。\n",
    "\n",
    "## 下一步\n",
    "\n",
    "接下来，我们将学习Dataset API，它结合了DataFrame的优势和RDD的类型安全特性。请继续学习 `dataset-api.ipynb` 笔记本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 停止SparkSession\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (.venv)",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
