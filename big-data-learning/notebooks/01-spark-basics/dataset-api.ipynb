{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Dataset API 教程\n",
    "\n",
    "本教程将详细介绍Apache Spark Dataset API，它结合了RDD的类型安全和DataFrame的性能优化。\n",
    "\n",
    "## 学习目标\n",
    "\n",
    "通过本教程，你将学会：\n",
    "1. Dataset的基本概念和优势\n",
    "2. Dataset与DataFrame和RDD的区别\n",
    "3. 创建和操作Dataset\n",
    "4. 类型安全的数据处理\n",
    "5. Dataset的性能优化\n",
    "6. 实际应用案例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset基础概念\n",
    "\n",
    "Dataset是Spark 1.6引入的新抽象，它结合了RDD和DataFrame的优点：\n",
    "- **类型安全**：编译时类型检查\n",
    "- **性能优化**：Catalyst优化器和Tungsten执行引擎\n",
    "- **面向对象编程**：支持lambda函数和复杂数据类型\n",
    "\n",
    "### Dataset vs DataFrame vs RDD\n",
    "\n",
    "| 特性 | RDD | DataFrame | Dataset |\n",
    "|------|-----|-----------|----------|\n",
    "| 类型安全 | 编译时 | 运行时 | 编译时 |\n",
    "| 性能优化 | 无 | Catalyst | Catalyst |\n",
    "| API风格 | 函数式 | SQL风格 | 混合 |\n",
    "| 序列化 | Java/Kryo | Tungsten | Tungsten |\n",
    "| GC开销 | 高 | 低 | 低 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 环境准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import pandas as pd\n",
    "\n",
    "# 创建SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Dataset API Tutorial\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# 设置日志级别\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "print(f\"Spark版本: {spark.version}\")\n",
    "print(\"Dataset API教程环境准备完成\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 创建Dataset\n",
    "\n",
    "在PySpark中，Dataset实际上就是DataFrame，因为Python是动态类型语言。但我们可以通过类型提示和验证来模拟类型安全。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义数据类型\n",
    "from typing import NamedTuple\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# 使用NamedTuple定义Person类型\n",
    "class Person(NamedTuple):\n",
    "    name: str\n",
    "    age: int\n",
    "    city: str\n",
    "    salary: float\n",
    "\n",
    "# 使用dataclass定义Product类型\n",
    "@dataclass\n",
    "class Product:\n",
    "    id: str\n",
    "    name: str\n",
    "    category: str\n",
    "    price: float\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        # 类型验证\n",
    "        if not isinstance(self.price, (int, float)) or self.price < 0:\n",
    "            raise ValueError(\"Price must be a non-negative number\")\n",
    "\n",
    "print(\"数据类型定义完成\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建Person Dataset\n",
    "person_data = [\n",
    "    Person(\"Alice\", 25, \"New York\", 75000.0),\n",
    "    Person(\"Bob\", 30, \"San Francisco\", 85000.0),\n",
    "    Person(\"Charlie\", 35, \"Chicago\", 95000.0),\n",
    "    Person(\"Diana\", 28, \"Boston\", 65000.0),\n",
    "    Person(\"Eve\", 32, \"Seattle\", 78000.0)\n",
    "]\n",
    "\n",
    "# 定义Schema\n",
    "person_schema = StructType([\n",
    "    StructField(\"name\", StringType(), False),\n",
    "    StructField(\"age\", IntegerType(), False),\n",
    "    StructField(\"city\", StringType(), False),\n",
    "    StructField(\"salary\", DoubleType(), False)\n",
    "])\n",
    "\n",
    "# 创建Dataset（在PySpark中实际是DataFrame）\n",
    "person_ds = spark.createDataFrame(person_data, person_schema)\n",
    "\n",
    "print(\"Person Dataset:\")\n",
    "person_ds.show()\n",
    "person_ds.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建Product Dataset\n",
    "product_data = [\n",
    "    Product(\"P001\", \"Laptop\", \"Electronics\", 1200.0),\n",
    "    Product(\"P002\", \"Mouse\", \"Electronics\", 25.0),\n",
    "    Product(\"P003\", \"Keyboard\", \"Electronics\", 75.0),\n",
    "    Product(\"P004\", \"Monitor\", \"Electronics\", 300.0),\n",
    "    Product(\"P005\", \"Chair\", \"Furniture\", 150.0)\n",
    "]\n",
    "\n",
    "# 转换为字典列表（因为dataclass不能直接用于createDataFrame）\n",
    "product_dict_data = [\n",
    "    {\"id\": p.id, \"name\": p.name, \"category\": p.category, \"price\": p.price}\n",
    "    for p in product_data\n",
    "]\n",
    "\n",
    "product_ds = spark.createDataFrame(product_dict_data)\n",
    "\n",
    "print(\"Product Dataset:\")\n",
    "product_ds.show()\n",
    "product_ds.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Dataset基本操作\n",
    "\n",
    "Dataset支持两种类型的操作：\n",
    "- **Transformation**：返回新的Dataset\n",
    "- **Action**：触发计算并返回结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 类型安全的转换操作\n",
    "print(\"=== 基本转换操作 ===\")\n",
    "\n",
    "# 过滤操作\n",
    "high_salary_persons = person_ds.filter(col(\"salary\") > 70000)\n",
    "print(\"高薪人员:\")\n",
    "high_salary_persons.show()\n",
    "\n",
    "# 映射操作\n",
    "person_with_bonus = person_ds.withColumn(\"bonus\", col(\"salary\") * 0.1)\n",
    "print(\"\\n添加奖金列:\")\n",
    "person_with_bonus.show()\n",
    "\n",
    "# 选择操作\n",
    "name_salary = person_ds.select(\"name\", \"salary\")\n",
    "print(\"\\n姓名和薪资:\")\n",
    "name_salary.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 复杂转换操作\n",
    "print(\"=== 复杂转换操作 ===\")\n",
    "\n",
    "# 添加计算列\n",
    "person_enhanced = person_ds.withColumn(\n",
    "    \"salary_level\",\n",
    "    when(col(\"salary\") > 80000, \"High\")\n",
    "    .when(col(\"salary\") > 60000, \"Medium\")\n",
    "    .otherwise(\"Low\")\n",
    ").withColumn(\n",
    "    \"age_group\",\n",
    "    when(col(\"age\") < 30, \"Young\")\n",
    "    .when(col(\"age\") < 35, \"Middle\")\n",
    "    .otherwise(\"Senior\")\n",
    ")\n",
    "\n",
    "print(\"增强的Person Dataset:\")\n",
    "person_enhanced.show()\n",
    "\n",
    "# 排序操作\n",
    "sorted_by_salary = person_ds.orderBy(col(\"salary\").desc())\n",
    "print(\"\\n按薪资排序:\")\n",
    "sorted_by_salary.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 聚合和分组操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基本聚合操作\n",
    "print(\"=== 基本聚合操作 ===\")\n",
    "\n",
    "# 统计信息\n",
    "person_stats = person_ds.agg(\n",
    "    count(\"*\").alias(\"total_count\"),\n",
    "    avg(\"salary\").alias(\"avg_salary\"),\n",
    "    max(\"salary\").alias(\"max_salary\"),\n",
    "    min(\"salary\").alias(\"min_salary\"),\n",
    "    stddev(\"salary\").alias(\"salary_stddev\")\n",
    ")\n",
    "\n",
    "print(\"人员统计信息:\")\n",
    "person_stats.show()\n",
    "\n",
    "# 按城市分组\n",
    "city_stats = person_ds.groupBy(\"city\").agg(\n",
    "    count(\"*\").alias(\"person_count\"),\n",
    "    avg(\"salary\").alias(\"avg_salary\"),\n",
    "    avg(\"age\").alias(\"avg_age\")\n",
    ").orderBy(col(\"avg_salary\").desc())\n",
    "\n",
    "print(\"\\n按城市统计:\")\n",
    "city_stats.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 产品数据聚合\n",
    "print(\"=== 产品数据聚合 ===\")\n",
    "\n",
    "# 按类别分组\n",
    "category_stats = product_ds.groupBy(\"category\").agg(\n",
    "    count(\"*\").alias(\"product_count\"),\n",
    "    avg(\"price\").alias(\"avg_price\"),\n",
    "    max(\"price\").alias(\"max_price\"),\n",
    "    min(\"price\").alias(\"min_price\"),\n",
    "    sum(\"price\").alias(\"total_value\")\n",
    ")\n",
    "\n",
    "print(\"按类别统计:\")\n",
    "category_stats.show()\n",
    "\n",
    "# 价格区间分析\n",
    "price_ranges = product_ds.withColumn(\n",
    "    \"price_range\",\n",
    "    when(col(\"price\") < 50, \"Low\")\n",
    "    .when(col(\"price\") < 200, \"Medium\")\n",
    "    .otherwise(\"High\")\n",
    ").groupBy(\"price_range\").agg(\n",
    "    count(\"*\").alias(\"count\"),\n",
    "    avg(\"price\").alias(\"avg_price\")\n",
    ")\n",
    "\n",
    "print(\"\\n价格区间分析:\")\n",
    "price_ranges.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 连接操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建订单数据\n",
    "from typing import NamedTuple\n",
    "\n",
    "class Order(NamedTuple):\n",
    "    order_id: str\n",
    "    customer_name: str\n",
    "    product_id: str\n",
    "    quantity: int\n",
    "    order_date: str\n",
    "\n",
    "order_data = [\n",
    "    Order(\"O001\", \"Alice\", \"P001\", 1, \"2023-01-01\"),\n",
    "    Order(\"O002\", \"Bob\", \"P002\", 2, \"2023-01-02\"),\n",
    "    Order(\"O003\", \"Charlie\", \"P003\", 1, \"2023-01-03\"),\n",
    "    Order(\"O004\", \"Alice\", \"P004\", 1, \"2023-01-04\"),\n",
    "    Order(\"O005\", \"Diana\", \"P005\", 2, \"2023-01-05\")\n",
    "]\n",
    "\n",
    "order_ds = spark.createDataFrame(order_data)\n",
    "\n",
    "print(\"Order Dataset:\")\n",
    "order_ds.show()\n",
    "order_ds.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 连接操作\n",
    "print(\"=== 连接操作 ===\")\n",
    "\n",
    "# 订单与产品信息连接\n",
    "order_with_product = order_ds.join(\n",
    "    product_ds,\n",
    "    order_ds.product_id == product_ds.id,\n",
    "    \"inner\"\n",
    ").select(\n",
    "    order_ds.order_id,\n",
    "    order_ds.customer_name,\n",
    "    product_ds.name.alias(\"product_name\"),\n",
    "    product_ds.category,\n",
    "    product_ds.price,\n",
    "    order_ds.quantity,\n",
    "    (product_ds.price * order_ds.quantity).alias(\"total_amount\")\n",
    ")\n",
    "\n",
    "print(\"订单产品信息:\")\n",
    "order_with_product.show()\n",
    "\n",
    "# 订单与客户信息连接\n",
    "order_with_customer = order_ds.join(\n",
    "    person_ds,\n",
    "    order_ds.customer_name == person_ds.name,\n",
    "    \"inner\"\n",
    ").select(\n",
    "    order_ds.order_id,\n",
    "    person_ds.name,\n",
    "    person_ds.city,\n",
    "    person_ds.salary,\n",
    "    order_ds.product_id,\n",
    "    order_ds.quantity\n",
    ")\n",
    "\n",
    "print(\"\\n订单客户信息:\")\n",
    "order_with_customer.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 窗口函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "\n",
    "print(\"=== 窗口函数 ===\")\n",
    "\n",
    "# 薪资排名\n",
    "salary_window = Window.orderBy(col(\"salary\").desc())\n",
    "city_salary_window = Window.partitionBy(\"city\").orderBy(col(\"salary\").desc())\n",
    "\n",
    "person_with_rank = person_ds.withColumn(\n",
    "    \"global_salary_rank\", row_number().over(salary_window)\n",
    ").withColumn(\n",
    "    \"city_salary_rank\", row_number().over(city_salary_window)\n",
    ").withColumn(\n",
    "    \"salary_percentile\", percent_rank().over(salary_window)\n",
    ")\n",
    "\n",
    "print(\"薪资排名:\")\n",
    "person_with_rank.show()\n",
    "\n",
    "# 移动平均\n",
    "salary_moving_avg = person_ds.withColumn(\n",
    "    \"salary_moving_avg\",\n",
    "    avg(\"salary\").over(\n",
    "        salary_window.rowsBetween(-1, 1)\n",
    "    )\n",
    ").withColumn(\n",
    "    \"salary_cumsum\",\n",
    "    sum(\"salary\").over(\n",
    "        salary_window.rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"\\n移动平均和累计和:\")\n",
    "salary_moving_avg.orderBy(col(\"salary\").desc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 用户定义函数 (UDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType, IntegerType\n",
    "\n",
    "print(\"=== 用户定义函数 ===\")\n",
    "\n",
    "# 定义UDF函数\n",
    "def categorize_salary(salary):\n",
    "    if salary >= 80000:\n",
    "        return \"High\"\n",
    "    elif salary >= 60000:\n",
    "        return \"Medium\"\n",
    "    else:\n",
    "        return \"Low\"\n",
    "\n",
    "def calculate_tax(salary):\n",
    "    if salary <= 50000:\n",
    "        return int(salary * 0.1)\n",
    "    elif salary <= 80000:\n",
    "        return int(salary * 0.15)\n",
    "    else:\n",
    "        return int(salary * 0.2)\n",
    "\n",
    "# 注册UDF\n",
    "categorize_salary_udf = udf(categorize_salary, StringType())\n",
    "calculate_tax_udf = udf(calculate_tax, IntegerType())\n",
    "\n",
    "# 应用UDF\n",
    "person_with_udf = person_ds.withColumn(\n",
    "    \"salary_category\", categorize_salary_udf(col(\"salary\"))\n",
    ").withColumn(\n",
    "    \"estimated_tax\", calculate_tax_udf(col(\"salary\"))\n",
    ").withColumn(\n",
    "    \"net_salary\", col(\"salary\") - col(\"estimated_tax\")\n",
    ")\n",
    "\n",
    "print(\"应用UDF后的结果:\")\n",
    "person_with_udf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 向量化UDF (pandas UDF) - 更高性能\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "import pandas as pd\n",
    "\n",
    "@pandas_udf(returnType=StringType())\n",
    "def vectorized_salary_category(salary_series: pd.Series) -> pd.Series:\n",
    "    return salary_series.apply(lambda x: \"High\" if x >= 80000 else (\"Medium\" if x >= 60000 else \"Low\"))\n",
    "\n",
    "@pandas_udf(returnType=IntegerType())\n",
    "def vectorized_tax_calculation(salary_series: pd.Series) -> pd.Series:\n",
    "    def calc_tax(salary):\n",
    "        if salary <= 50000:\n",
    "            return int(salary * 0.1)\n",
    "        elif salary <= 80000:\n",
    "            return int(salary * 0.15)\n",
    "        else:\n",
    "            return int(salary * 0.2)\n",
    "    \n",
    "    return salary_series.apply(calc_tax)\n",
    "\n",
    "# 应用向量化UDF\n",
    "person_with_pandas_udf = person_ds.withColumn(\n",
    "    \"salary_category_v2\", vectorized_salary_category(col(\"salary\"))\n",
    ").withColumn(\n",
    "    \"estimated_tax_v2\", vectorized_tax_calculation(col(\"salary\"))\n",
    ")\n",
    "\n",
    "print(\"\\n应用向量化UDF后的结果:\")\n",
    "person_with_pandas_udf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 数据验证和质量检查"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== 数据验证和质量检查 ===\")\n",
    "\n",
    "# 创建包含问题数据的Dataset\n",
    "problematic_data = [\n",
    "    (\"Alice\", 25, \"New York\", 75000.0),\n",
    "    (\"Bob\", None, \"San Francisco\", 85000.0),  # 缺失年龄\n",
    "    (\"Charlie\", 35, None, 95000.0),  # 缺失城市\n",
    "    (\"Diana\", 28, \"Boston\", None),  # 缺失薪资\n",
    "    (None, 32, \"Seattle\", 78000.0),  # 缺失姓名\n",
    "    (\"Frank\", -5, \"Miami\", 60000.0),  # 无效年龄\n",
    "    (\"Grace\", 30, \"Denver\", -1000.0)  # 无效薪资\n",
    "]\n",
    "\n",
    "problematic_ds = spark.createDataFrame(problematic_data, person_schema)\n",
    "\n",
    "print(\"原始数据（包含问题）:\")\n",
    "problematic_ds.show()\n",
    "\n",
    "# 数据质量检查\n",
    "print(\"\\n数据质量报告:\")\n",
    "total_rows = problematic_ds.count()\n",
    "print(f\"总行数: {total_rows}\")\n",
    "\n",
    "# 检查空值\n",
    "null_counts = problematic_ds.select([\n",
    "    sum(when(col(c).isNull(), 1).otherwise(0)).alias(f\"{c}_nulls\")\n",
    "    for c in problematic_ds.columns\n",
    "])\n",
    "null_counts.show()\n",
    "\n",
    "# 检查无效值\n",
    "invalid_age_count = problematic_ds.filter(col(\"age\") < 0).count()\n",
    "invalid_salary_count = problematic_ds.filter(col(\"salary\") < 0).count()\n",
    "\n",
    "print(f\"无效年龄记录数: {invalid_age_count}\")\n",
    "print(f\"无效薪资记录数: {invalid_salary_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据清洗\n",
    "print(\"=== 数据清洗 ===\")\n",
    "\n",
    "# 方法1: 删除包含空值的行\n",
    "clean_ds_method1 = problematic_ds.dropna()\n",
    "print(f\"删除空值后剩余行数: {clean_ds_method1.count()}\")\n",
    "clean_ds_method1.show()\n",
    "\n",
    "# 方法2: 填充默认值并过滤无效数据\n",
    "clean_ds_method2 = problematic_ds.fillna({\n",
    "    \"name\": \"Unknown\",\n",
    "    \"age\": 30,\n",
    "    \"city\": \"Unknown\",\n",
    "    \"salary\": 50000.0\n",
    "}).filter(\n",
    "    (col(\"age\") >= 0) & (col(\"salary\") >= 0)\n",
    ")\n",
    "\n",
    "print(f\"\\n填充默认值并过滤后剩余行数: {clean_ds_method2.count()}\")\n",
    "clean_ds_method2.show()\n",
    "\n",
    "# 方法3: 智能填充\n",
    "# 计算平均值用于填充\n",
    "avg_age = problematic_ds.agg(avg(\"age\")).collect()[0][0]\n",
    "avg_salary = problematic_ds.agg(avg(\"salary\")).collect()[0][0]\n",
    "\n",
    "clean_ds_method3 = problematic_ds.fillna({\n",
    "    \"name\": \"Unknown\",\n",
    "    \"age\": int(avg_age) if avg_age else 30,\n",
    "    \"city\": \"Unknown\",\n",
    "    \"salary\": avg_salary if avg_salary else 50000.0\n",
    "}).filter(\n",
    "    (col(\"age\") >= 0) & (col(\"salary\") >= 0)\n",
    ")\n",
    "\n",
    "print(f\"\\n智能填充后剩余行数: {clean_ds_method3.count()}\")\n",
    "clean_ds_method3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. 性能优化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== 性能优化 ===\")\n",
    "\n",
    "# 缓存策略\n",
    "print(\"1. 缓存策略\")\n",
    "person_ds.cache()\n",
    "print(f\"缓存状态: {person_ds.is_cached}\")\n",
    "\n",
    "# 分区优化\n",
    "print(f\"\\n2. 分区信息\")\n",
    "print(f\"当前分区数: {person_ds.rdd.getNumPartitions()}\")\n",
    "\n",
    "# 重新分区\n",
    "repartitioned_ds = person_ds.repartition(2, col(\"city\"))\n",
    "print(f\"按城市重新分区后: {repartitioned_ds.rdd.getNumPartitions()}\")\n",
    "\n",
    "# 查看执行计划\n",
    "print(\"\\n3. 执行计划分析\")\n",
    "complex_query = person_ds.filter(col(\"salary\") > 70000) \\\n",
    "                         .groupBy(\"city\") \\\n",
    "                         .agg(avg(\"salary\").alias(\"avg_salary\")) \\\n",
    "                         .orderBy(\"avg_salary\")\n",
    "\n",
    "print(\"查询执行计划:\")\n",
    "complex_query.explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 广播连接优化\n",
    "print(\"=== 广播连接优化 ===\")\n",
    "\n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "# 小表广播连接\n",
    "# 假设product_ds是小表\n",
    "optimized_join = order_ds.join(\n",
    "    broadcast(product_ds),\n",
    "    order_ds.product_id == product_ds.id,\n",
    "    \"inner\"\n",
    ")\n",
    "\n",
    "print(\"广播连接执行计划:\")\n",
    "optimized_join.explain()\n",
    "\n",
    "print(\"\\n广播连接结果:\")\n",
    "optimized_join.select(\n",
    "    order_ds.order_id,\n",
    "    order_ds.customer_name,\n",
    "    product_ds.name.alias(\"product_name\"),\n",
    "    product_ds.price\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. 实际应用案例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== 实际应用案例：员工薪资分析系统 ===\")\n",
    "\n",
    "# 加载真实数据\n",
    "sales_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"/home/jovyan/data/sample/sales_data.csv\")\n",
    "\n",
    "# 案例1: 销售业绩分析\n",
    "print(\"1. 销售业绩分析\")\n",
    "\n",
    "# 计算每个客户的购买统计\n",
    "customer_analysis = sales_df.groupBy(\"customer_id\").agg(\n",
    "    count(\"*\").alias(\"purchase_count\"),\n",
    "    sum(col(\"price\") * col(\"quantity\")).alias(\"total_spent\"),\n",
    "    avg(col(\"price\") * col(\"quantity\")).alias(\"avg_order_value\"),\n",
    "    countDistinct(\"product_id\").alias(\"unique_products\"),\n",
    "    countDistinct(\"category\").alias(\"unique_categories\")\n",
    ").withColumn(\n",
    "    \"customer_tier\",\n",
    "    when(col(\"total_spent\") > 2000, \"Premium\")\n",
    "    .when(col(\"total_spent\") > 1000, \"Gold\")\n",
    "    .when(col(\"total_spent\") > 500, \"Silver\")\n",
    "    .otherwise(\"Bronze\")\n",
    ")\n",
    "\n",
    "print(\"客户分析结果:\")\n",
    "customer_analysis.orderBy(col(\"total_spent\").desc()).show()\n",
    "\n",
    "# 客户分层统计\n",
    "tier_stats = customer_analysis.groupBy(\"customer_tier\").agg(\n",
    "    count(\"*\").alias(\"customer_count\"),\n",
    "    avg(\"total_spent\").alias(\"avg_spent\"),\n",
    "    avg(\"purchase_count\").alias(\"avg_purchases\")\n",
    ")\n",
    "\n",
    "print(\"\\n客户分层统计:\")\n",
    "tier_stats.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 案例2: 产品推荐系统基础分析\n",
    "print(\"=== 案例2: 产品推荐系统基础分析 ===\")\n",
    "\n",
    "# 产品流行度分析\n",
    "product_popularity = sales_df.groupBy(\"product_id\", \"category\").agg(\n",
    "    count(\"*\").alias(\"purchase_frequency\"),\n",
    "    sum(\"quantity\").alias(\"total_quantity\"),\n",
    "    countDistinct(\"customer_id\").alias(\"unique_customers\"),\n",
    "    avg(\"price\").alias(\"avg_price\")\n",
    ").withColumn(\n",
    "    \"popularity_score\",\n",
    "    col(\"purchase_frequency\") * 0.4 + col(\"unique_customers\") * 0.6\n",
    ")\n",
    "\n",
    "print(\"产品流行度分析:\")\n",
    "product_popularity.orderBy(col(\"popularity_score\").desc()).show()\n",
    "\n",
    "# 类别关联分析\n",
    "category_cooccurrence = sales_df.alias(\"s1\").join(\n",
    "    sales_df.alias(\"s2\"),\n",
    "    (col(\"s1.customer_id\") == col(\"s2.customer_id\")) & \n",
    "    (col(\"s1.category\") != col(\"s2.category\")),\n",
    "    \"inner\"\n",
    ").groupBy(\n",
    "    col(\"s1.category\").alias(\"category1\"),\n",
    "    col(\"s2.category\").alias(\"category2\")\n",
    ").agg(\n",
    "    countDistinct(col(\"s1.customer_id\")).alias(\"common_customers\")\n",
    ").filter(\n",
    "    col(\"common_customers\") > 1\n",
    ").orderBy(col(\"common_customers\").desc())\n",
    "\n",
    "print(\"\\n类别关联分析:\")\n",
    "category_cooccurrence.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. 实践练习"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== 实践练习 ===\")\n",
    "\n",
    "# 练习1: 创建一个员工绩效分析系统\n",
    "print(\"练习1: 员工绩效分析系统\")\n",
    "\n",
    "# 创建员工绩效数据\n",
    "performance_data = [\n",
    "    (\"Alice\", \"Engineering\", 85, 92, 88, 4.2),\n",
    "    (\"Bob\", \"Sales\", 78, 85, 82, 3.8),\n",
    "    (\"Charlie\", \"Marketing\", 92, 88, 90, 4.5),\n",
    "    (\"Diana\", \"Engineering\", 88, 90, 89, 4.3),\n",
    "    (\"Eve\", \"Sales\", 82, 87, 85, 4.0),\n",
    "    (\"Frank\", \"Marketing\", 75, 80, 78, 3.5),\n",
    "    (\"Grace\", \"Engineering\", 90, 93, 92, 4.6)\n",
    "]\n",
    "\n",
    "performance_schema = StructType([\n",
    "    StructField(\"name\", StringType(), False),\n",
    "    StructField(\"department\", StringType(), False),\n",
    "    StructField(\"technical_score\", IntegerType(), False),\n",
    "    StructField(\"communication_score\", IntegerType(), False),\n",
    "    StructField(\"overall_score\", IntegerType(), False),\n",
    "    StructField(\"manager_rating\", DoubleType(), False)\n",
    "])\n",
    "\n",
    "performance_ds = spark.createDataFrame(performance_data, performance_schema)\n",
    "\n",
    "# 任务1: 计算综合绩效分数\n",
    "performance_analysis = performance_ds.withColumn(\n",
    "    \"weighted_score\",\n",
    "    col(\"technical_score\") * 0.4 + \n",
    "    col(\"communication_score\") * 0.3 + \n",
    "    col(\"overall_score\") * 0.2 +\n",
    "    col(\"manager_rating\") * 10 * 0.1\n",
    ").withColumn(\n",
    "    \"performance_grade\",\n",
    "    when(col(\"weighted_score\") >= 90, \"A\")\n",
    "    .when(col(\"weighted_score\") >= 80, \"B\")\n",
    "    .when(col(\"weighted_score\") >= 70, \"C\")\n",
    "    .otherwise(\"D\")\n",
    ")\n",
    "\n",
    "print(\"员工绩效分析:\")\n",
    "performance_analysis.orderBy(col(\"weighted_score\").desc()).show()\n",
    "\n",
    "# 任务2: 部门绩效统计\n",
    "dept_performance = performance_analysis.groupBy(\"department\").agg(\n",
    "    count(\"*\").alias(\"employee_count\"),\n",
    "    avg(\"weighted_score\").alias(\"avg_performance\"),\n",
    "    max(\"weighted_score\").alias(\"best_performance\"),\n",
    "    min(\"weighted_score\").alias(\"worst_performance\")\n",
    ").withColumn(\n",
    "    \"dept_grade\",\n",
    "    when(col(\"avg_performance\") >= 85, \"Excellent\")\n",
    "    .when(col(\"avg_performance\") >= 80, \"Good\")\n",
    "    .otherwise(\"Needs Improvement\")\n",
    ")\n",
    "\n",
    "print(\"\\n部门绩效统计:\")\n",
    "dept_performance.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. 总结"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Dataset API教程总结 ===\")\n",
    "print(\"\\n1. 核心概念:\")\n",
    "print(\"   - Dataset = DataFrame + 类型安全\")\n",
    "print(\"   - 编译时类型检查\")\n",
    "print(\"   - Catalyst优化器支持\")\n",
    "\n",
    "print(\"\\n2. 主要优势:\")\n",
    "print(\"   - 类型安全的数据处理\")\n",
    "print(\"   - 高性能执行\")\n",
    "print(\"   - 丰富的API支持\")\n",
    "print(\"   - 易于调试和维护\")\n",
    "\n",
    "print(\"\\n3. 核心操作:\")\n",
    "print(\"   - 转换: filter, map, flatMap, groupBy\")\n",
    "print(\"   - 行动: collect, count, show, write\")\n",
    "print(\"   - 聚合: agg, groupBy, window functions\")\n",
    "print(\"   - 连接: join, broadcast join\")\n",
    "\n",
    "print(\"\\n4. 性能优化:\")\n",
    "print(\"   - 缓存策略\")\n",
    "print(\"   - 分区优化\")\n",
    "print(\"   - 广播连接\")\n",
    "print(\"   - 向量化UDF\")\n",
    "\n",
    "print(\"\\n5. 最佳实践:\")\n",
    "print(\"   - 明确定义数据类型\")\n",
    "print(\"   - 使用向量化操作\")\n",
    "print(\"   - 合理使用缓存\")\n",
    "print(\"   - 监控执行计划\")\n",
    "print(\"   - 数据质量验证\")\n",
    "\n",
    "print(\"\\n恭喜！你已经掌握了Dataset API的核心概念和操作。\")\n",
    "print(\"Dataset API是Spark中最强大的数据处理抽象，\")\n",
    "print(\"它结合了类型安全和高性能，是现代大数据应用的理想选择。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 清理资源\n",
    "print(\"清理Spark资源...\")\n",
    "\n",
    "# 取消缓存\n",
    "person_ds.unpersist()\n",
    "spark.catalog.clearCache()\n",
    "\n",
    "print(\"资源清理完成\")\n",
    "# spark.stop()  # 保持会话活跃"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}